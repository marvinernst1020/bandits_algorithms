---
title: "Baseline Bandit Simulation"
author: "Marvin Ernst, Oriol Gelabert, Melisa Vadenja"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
packages <- c("ggplot2", "dplyr", "tidyr", "ggthemes")

for (pkg in packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
  library(pkg, character.only = TRUE)
}
```

# Baseline Bandit Simulation

## Static Bandit Setup

We consider a stationary stochastic multi-armed bandit problem with $K = 3$ arms. Each arm returns a reward drawn from a Bernoulli distribution with success probabilities:

- Arm 1: $\mu^1 = 0.70$
- Arm 2: $\mu^2 = 0.60$
- Arm 3: $\mu^3 = 0.40$

The environment is stationary, i.e., the reward distributions do not change over time. We run each algorithm for $T = 1000$ time steps and repeat the simulation for 1000 independent runs to compute average performance metrics.

## Bandit Algorithms

We evaluate the following classical algorithms:

1. **UCB1** - Upper Confidence Bound (Auer et al., 2002)
2. **Tuned UCB** - Variant of UCB with empirical variance adjustment
3. **Thompson Sampling** - Bayesian algorithm sampling from posterior

```{r bandit-functions}
simulate_bandit_full <- function(algorithm, probs, T = 1000, warmup_order = sample(1:length(probs))) {
  K <- length(probs)
  counts <- rep(0, K)
  rewards <- rep(0, K)
  best_arm <- which.max(probs)

  inst_regret <- numeric(T)
  total_regret <- numeric(T)
  chosen_arm <- numeric(T)
  regret <- 0

  for (t in 1:T) {
    if (t <= K) {
      idx <- warmup_order[t]
    } else {
      if (algorithm == "ucb") {
        ucb_values <- rewards / counts + sqrt(2 * log(t) / counts)
        idx <- which.max(ucb_values)
      } else if (algorithm == "ucb-tuned") {
        means <- rewards / counts
        variances <- pmin(1/4, means * (1 - means) + sqrt(2 * log(t) / counts))
        ucb_tuned_values <- means + sqrt(log(t) / counts * variances)
        idx <- which.max(ucb_tuned_values)
      } else if (algorithm == "ts") {
        samples <- rbeta(K, 1 + rewards, 1 + counts - rewards)
        idx <- which.max(samples)
      }
    }

    reward <- rbinom(1, 1, probs[idx])
    counts[idx] <- counts[idx] + 1
    rewards[idx] <- rewards[idx] + reward

    regret <- regret + (max(probs) - probs[idx])
    total_regret[t] <- regret
    inst_regret[t] <- max(probs) - probs[idx]
    chosen_arm[t] <- idx
  }

  last_simple_regret <- if (any(counts == 0)) {
    NA
  } else {
    estimated_means <- rewards / counts
    max(probs) - probs[which.max(estimated_means)]
  }

  list(
    cumulative = total_regret,
    instantaneous = inst_regret,
    optimal_pulls = chosen_arm == best_arm,
    last_simple_regret = last_simple_regret
  )
}
```

## Simulation and Results

```{r simulation}
set.seed(42)
probs <- c(0.7, 0.6, 0.4)
T <- 1000
runs <- 10000
methods <- c("ucb", "ucb-tuned", "ts")
labels <- c("Standard UCB", "Tuned UCB", "Thompson Sampling")

simulate_all <- function(method) {
  replicate(runs, {
    warmup <- sample(1:3)
    simulate_bandit_full(method, probs, T, warmup_order = warmup)
  }, simplify = FALSE)
}

results <- lapply(methods, simulate_all)
names(results) <- labels
```

## Cumulative Regret over Time

```{r}
cumulative_df <- do.call(rbind, lapply(names(results), function(name) {
  do.call(cbind, lapply(results[[name]], function(r) r$cumulative)) |>
    rowMeans() |>
    data.frame(time = 1:T, Regret = _) |>
    mutate(Algorithm = name)
})) |>
  bind_rows()

ggplot(cumulative_df, aes(x = time, y = Regret, color = Algorithm)) +
  geom_line(size = 0.8) +
  scale_color_manual(values = c("Standard UCB" = "#D62728", 
                                "Thompson Sampling" = "#1F77B4", 
                                "Tuned UCB" = "#2CA02C")) +
  labs(x = "Time Step", y = "Average Cumulative Regret") +
  theme_classic(base_size = 12) +
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Histogram of Final Regret

Extract final cumulative regret from results:
```{r}
final_regrets <- do.call(rbind, lapply(names(results), function(name) {
  values <- sapply(results[[name]], function(r) tail(r$cumulative, 1))
  data.frame(FinalRegret = values, Algorithm = name)
}))
```

Plot:
```{r}
ggplot(final_regrets, aes(x = FinalRegret, fill = Algorithm)) +
  geom_histogram(alpha = 0.65, bins = 30, position = "identity", color = "white") +
  scale_fill_manual(values = c("Standard UCB" = "#D62728", 
                               "Thompson Sampling" = "#1F77B4", 
                               "Tuned UCB" = "#2CA02C")) +
  labs(x = "Final Cumulative Regret", y = "Frequency") +
  theme_classic(base_size = 12) +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    axis.text = element_text(color = "black"),
    panel.grid = element_blank()
  )
```

### Summary

The results show clear differences in algorithm performance. Standard UCB accumulates significantly more regret over time, indicating inefficient exploration. In contrast, Tuned UCB and Thompson Sampling perform substantially better, with Tuned UCB showing the lowest cumulative regret and highest stability across runs. This suggests that accounting for reward variance or using Bayesian sampling leads to faster and more reliable convergence in static environments.

---

# Alternative Metrics

## Percentage of Optimal Arm Pulls

```{r}
opt_df <- do.call(rbind, lapply(names(results), function(name) {
  do.call(cbind, lapply(results[[name]], function(r) r$optimal_pulls)) |>
    rowMeans() |>
    data.frame(time = 1:T, OptimalRate = _) |>
    mutate(Algorithm = name)
})) |>
  bind_rows()

ggplot(opt_df, aes(x = time, y = OptimalRate, color = Algorithm)) +
  geom_line(size = 0.7) +
  scale_color_manual(values = c("Standard UCB" = "#D62728", 
                                "Thompson Sampling" = "#1F77B4", 
                                "Tuned UCB" = "#2CA02C")) +
  labs(x = "Time Step", y = "Optimal Arm Selection Rate") +
  theme_classic(base_size = 12) +
  theme(legend.position = "bottom", legend.title = element_blank())
```


## Convergence Rate

We need this utility function:
```{r}
compute_convergence_time <- function(opt_pulls, window = 50, threshold = 0.95) {
  T <- length(opt_pulls)
  for (t in 1:(T - window + 1)) {
    window_avg <- mean(opt_pulls[t:(t + window - 1)])
    if (window_avg >= threshold) {
      return(t)
    }
  }
  return(NA)
}
```

Apply to all simulations:
```{r}
convergence_df <- do.call(rbind, lapply(names(results), function(name) {
  conv_times <- sapply(results[[name]], function(r) compute_convergence_time(r$optimal_pulls))
  data.frame(ConvergenceTime = conv_times, Algorithm = name)
})) %>% 
  filter(!is.na(ConvergenceTime))  # remove failed runs (never converged)
```

Plot convergence time histogram:
```{r}
ggplot(convergence_df, aes(x = ConvergenceTime, fill = Algorithm)) +
  geom_histogram(alpha = 0.7, bins = 30, position = "identity", color = "white") +
  scale_fill_manual(values = c("Standard UCB" = "#D62728",
                               "Thompson Sampling" = "#1F77B4",
                               "Tuned UCB" = "#2CA02C")) +
  labs(x = "Time to Convergence", y = "Number of Runs") +
  theme_classic(base_size = 12) +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()
  )
```

Summary:
```{r}
convergence_df %>%
  group_by(Algorithm) %>%
  summarise(mean_time = mean(ConvergenceTime), sd_time = sd(ConvergenceTime), .groups = "drop")
```


## Simple Regret

```{r}
simple_summary <- simple_df %>%
  group_by(Algorithm) %>%
  summarise(
    mean_simple_regret = mean(SimpleRegret),
    sd_simple_regret = sd(SimpleRegret),
    .groups = "drop"
  )

simple_summary
```


---

## Instantaneous Regret over Time

(Takes 30 minutes to run.)

We need to do another simulation only for the first time steps:
```{r simulation}
set.seed(42)
probs <- c(0.7, 0.6, 0.4)
T <- 100
runs <- 1000000
methods <- c("ucb", "ucb-tuned", "ts")
labels <- c("Standard UCB", "Tuned UCB", "Thompson Sampling")

simulate_all <- function(method) {
  replicate(runs, {
    warmup <- sample(1:3)
    simulate_bandit_full(method, probs, T, warmup_order = warmup)
  }, simplify = FALSE)
}

results <- lapply(methods, simulate_all)
names(results) <- labels
```

Now we return the instantaneous regrets:
```{r}
inst_df <- do.call(rbind, lapply(names(results), function(name) {
  regrets <- sapply(results[[name]], function(r) r$instantaneous)
  means <- rowMeans(regrets)
  data.frame(time = 1:T, InstantRegret = means, Algorithm = name)
}))

ggplot(inst_df, aes(x = time, y = InstantRegret, color = Algorithm)) +
  geom_line(size = 0.7, alpha = 0.85) +
  scale_color_manual(values = c("Standard UCB" = "#D62728", 
                                "Thompson Sampling" = "#1F77B4", 
                                "Tuned UCB" = "#2CA02C")) +
  labs(x = "Time Step", y = "Instantaneous Regret") +
  theme_classic(base_size = 12) +
  theme(legend.position = "bottom", legend.title = element_blank())
```

Table with the first 25 time steps:
```{r}
inst_wide <- inst_df %>%
  filter(time <= 25) %>%
  pivot_wider(
    names_from = Algorithm,
    values_from = InstantRegret
  ) %>%
  arrange(time)

inst_wide
```

Plot for the first 50 time steps:
```{r}
inst_df <- do.call(rbind, lapply(names(results), function(name) {
  regrets <- sapply(results[[name]], function(r) r$instantaneous)
  means <- rowMeans(regrets)
  data.frame(time = 1:T, InstantRegret = means, Algorithm = name)
}))

inst_df_subset <- inst_df %>% filter(time <= 50)

ggplot(inst_df_subset, aes(x = time, y = InstantRegret, color = Algorithm)) +
  geom_line(size = 0.7, alpha = 0.85) +
  scale_color_manual(values = c("Standard UCB" = "#D62728", 
                                "Thompson Sampling" = "#1F77B4", 
                                "Tuned UCB" = "#2CA02C")) +
  labs(x = "Time Step", y = "Instantaneous Regret") +
  theme_classic(base_size = 12) +
  theme(legend.position = "bottom", legend.title = element_blank())
```


