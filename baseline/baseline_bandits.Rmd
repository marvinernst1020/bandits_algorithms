---
title: "Baseline Bandit Simulation"
author: "Marvin Ernst, Oriol Gelabert, Melisa Vadenja"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
packages <- c("ggplot2", "dplyr", "tidyr", "ggthemes")

for (pkg in packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
  library(pkg, character.only = TRUE)
}
```

# Baseline Bandit Simulation

## Static Bandit Setup

We consider a stationary stochastic multi-armed bandit problem with $K = 3$ arms. Each arm returns a reward drawn from a Bernoulli distribution with success probabilities:

- Arm 1: $p_1 = 0.70$
- Arm 2: $p_2 = 0.60$
- Arm 3: $p_3 = 0.40$

The environment is stationary, i.e., the reward distributions do not change over time. We run each algorithm for $T = 1000$ time steps and repeat the simulation for 1000 independent runs to compute average performance metrics.

## Bandit Algorithms

We evaluate the following classical algorithms:

1. **UCB1** - Upper Confidence Bound (Auer et al., 2002)
2. **Tuned UCB** - Variant of UCB with empirical variance adjustment
3. **Thompson Sampling** - Bayesian algorithm sampling from posterior

```{r bandit-functions}
simulate_bandit_full <- function(algorithm, probs, T = 1000) {
  K <- length(probs)
  counts <- rep(0, K)
  rewards <- rep(0, K)
  best_arm <- which.max(probs)
  
  inst_regret <- numeric(T)
  total_regret <- numeric(T)
  chosen_arm <- numeric(T)
  regret <- 0
  
  for (t in 1:T) {
    if (algorithm == "ucb") {
      if (any(counts == 0)) {
        idx <- which(counts == 0)[1]
      } else {
        ucb_values <- rewards / counts + sqrt(2 * log(t) / counts)
        idx <- which.max(ucb_values)
      }
    } else if (algorithm == "ucb-tuned") {
      if (any(counts == 0)) {
        idx <- which(counts == 0)[1]
      } else {
        means <- rewards / counts
        variances <- pmin(1/4, means * (1 - means) + sqrt(2 * log(t) / counts))
        ucb_tuned_values <- means + sqrt(log(t) / counts * variances)
        idx <- which.max(ucb_tuned_values)
      }
    } else if (algorithm == "ts") {
      samples <- rbeta(K, 1 + rewards, 1 + counts - rewards)
      idx <- which.max(samples)
    }

    reward <- rbinom(1, 1, probs[idx])
    counts[idx] <- counts[idx] + 1
    rewards[idx] <- rewards[idx] + reward
    
    regret <- regret + (max(probs) - probs[idx])
    total_regret[t] <- regret
    inst_regret[t] <- max(probs) - probs[idx]
    chosen_arm[t] <- idx
  }
  
  list(
    cumulative = total_regret,
    instantaneous = inst_regret,
    optimal_pulls = chosen_arm == best_arm,
    last_simple_regret = max(probs) - probs[which.max(rewards / counts)]
  )
}
```

## Simulation and Results

```{r simulation}
set.seed(42)
probs <- c(0.7, 0.6, 0.4)
T <- 1000
runs <- 1000
methods <- c("ucb", "ucb-tuned", "ts")
labels <- c("Standard UCB", "Tuned UCB", "Thompson Sampling")

simulate_all <- function(method) {
  replicate(runs, simulate_bandit_full(method, probs, T), simplify = FALSE)
}

results <- lapply(methods, simulate_all)
names(results) <- labels
```

## Cumulative Regret over Time

```{r}
cumulative_df <- do.call(rbind, lapply(names(results), function(name) {
  do.call(cbind, lapply(results[[name]], function(r) r$cumulative)) |>
    rowMeans() |>
    data.frame(time = 1:T, Regret = _) |>
    mutate(Algorithm = name)
})) |>
  bind_rows()

ggplot(cumulative_df, aes(x = time, y = Regret, color = Algorithm)) +
  geom_line(size = 0.8) +
  scale_color_manual(values = c("Standard UCB" = "#D62728", 
                                "Thompson Sampling" = "#1F77B4", 
                                "Tuned UCB" = "#2CA02C")) +
  labs(x = "Time Step", y = "Average Cumulative Regret") +
  theme_classic(base_size = 12) +
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Instantaneous Regret over Time

```{r}
inst_df <- do.call(rbind, lapply(names(results), function(name) {
  do.call(cbind, lapply(results[[name]], function(r) r$instantaneous)) |>
    rowMeans() |>
    data.frame(time = 1:T, InstantRegret = _) |>
    mutate(Algorithm = name)
})) |>
  bind_rows()

ggplot(inst_df, aes(x = time, y = InstantRegret, color = Algorithm)) +
  geom_line(size = 0.7, alpha = 0.85) +
  scale_color_manual(values = c("Standard UCB" = "#D62728", 
                                "Thompson Sampling" = "#1F77B4", 
                                "Tuned UCB" = "#2CA02C")) +
  labs(x = "Time Step", y = "Instantaneous Regret") +
  theme_classic(base_size = 12) +
  theme(legend.position = "bottom", legend.title = element_blank())
```


## Percentage of Optimal Arm Pulls

```{r}
opt_df <- do.call(rbind, lapply(names(results), function(name) {
  do.call(cbind, lapply(results[[name]], function(r) r$optimal_pulls)) |>
    rowMeans() |>
    data.frame(time = 1:T, OptimalRate = _) |>
    mutate(Algorithm = name)
})) |>
  bind_rows()

ggplot(opt_df, aes(x = time, y = OptimalRate, color = Algorithm)) +
  geom_line(size = 0.7) +
  scale_color_manual(values = c("Standard UCB" = "#D62728", 
                                "Thompson Sampling" = "#1F77B4", 
                                "Tuned UCB" = "#2CA02C")) +
  labs(x = "Time Step", y = "Optimal Arm Selection Rate") +
  theme_classic(base_size = 12) +
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Simple Regret

Not correct yet .... 

```{r}
simple_df <- do.call(rbind, lapply(names(results), function(name) {
  sapply(results[[name]], function(r) r$last_simple_regret) |>
    data.frame(SimpleRegret = _, Algorithm = name)
})) |>
  bind_rows()

ggplot(simple_df, aes(x = SimpleRegret, fill = Algorithm)) +
  geom_histogram(alpha = 0.65, bins = 30, position = "identity", color = "white") +
  scale_fill_manual(values = c("Standard UCB" = "#D62728", 
                               "Thompson Sampling" = "#1F77B4", 
                               "Tuned UCB" = "#2CA02C")) +
  labs(x = "Final Simple Regret", y = "Frequency") +
  theme_classic(base_size = 12) +
  theme(legend.position = "bottom", legend.title = element_blank())
```



## Histogram of Final Regret

Store the result:
```{r}
final_regrets <- data.frame(
  Standard_UCB = ucb_results[T, ],
  Tuned_UCB = ucb_tuned_results[T, ],
  Thompson_Sampling = ts_results[T, ]
) %>%
  pivot_longer(cols = everything(), names_to = "Algorithm", values_to = "FinalRegret")

final_regrets$Algorithm <- factor(final_regrets$Algorithm,
                                  levels = c("Standard_UCB", "Thompson_Sampling", "Tuned_UCB"))
```

Plot:
```{r}
ggplot(final_regrets, aes(x = FinalRegret, fill = Algorithm)) +
  geom_histogram(alpha = 0.65, bins = 30, position = "identity", color = "white") +
  scale_fill_manual(values = c("Standard_UCB" = "#D62728", 
                               "Thompson_Sampling" = "#1F77B4", 
                               "Tuned_UCB" = "#2CA02C")) +
  labs(x = "Final Cumulative Regret", y = "Frequency") +
  theme_classic(base_size = 12) +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    axis.text = element_text(color = "black"),
    panel.grid = element_blank()
  )
```


## Summary

The results show clear differences in algorithm performance. Standard UCB accumulates significantly more regret over time, indicating inefficient exploration. In contrast, Tuned UCB and Thompson Sampling perform substantially better, with Tuned UCB showing the lowest cumulative regret and highest stability across runs. This suggests that accounting for reward variance or using Bayesian sampling leads to faster and more reliable convergence in static environments.

