---
title: "Global State Bandit Simulation"
author: "Marvin Ernst, Oriol Gelabert, Melisa Vadenja"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
library(ggplot2)
library(ggthemes)
```

# Problem Setup: Global Latent State

We simulate a $K$-armed bandit problem where **all arms share the same underlying latent state process** governed by a Hidden Markov Model (HMM). This represents a scenario where there's a common environment or external condition that affects the reward probability of all arms.

# 1. Generating Ground Truth Data for Global State

## 1.1 Simulate Ground Truth: 

```{r simulate-local-state}
set.seed(123)
K <- 3
N <- 1000

theta <- matrix(c(0.1, 0.9,
                  0.5, 0.7,
                  0.9, 0.1),
                nrow = K, byrow = TRUE)

pi_indiv <- list(
  matrix(c(0.9, 0.1,
           0.1, 0.9), 2, 2, byrow = TRUE),
  matrix(c(0.85, 0.15,
           0.2, 0.8), 2, 2, byrow = TRUE),
  matrix(c(0.95, 0.05,
           0.3, 0.7), 2, 2, byrow = TRUE)
)

z_local <- matrix(0, nrow = K, ncol = N)
y_local <- matrix(0, nrow = K, ncol = N)

for (i in 1:K) {
  z_local[i, 1] <- rbinom(1, 1, 0.5)
  y_local[i, 1] <- rbinom(1, 1, theta[i, z_local[i, 1] + 1])
  for (t in 2:N) {
    z_local[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_local[i, t - 1] + 1, 2])
    y_local[i, t] <- rbinom(1, 1, theta[i, z_local[i, t] + 1])
  }
}
```

# 1.2 Visualize Latent States

```{r visualize-state}
z_df <- as.data.frame(z_local)
z_df <- z_df |> mutate(arm = factor(1:K)) |> 
  pivot_longer(cols = starts_with("V"), names_to = "time", values_to = "state") |>
  mutate(time = as.numeric(gsub("V", "", time)))

ggplot(z_df, aes(x = time, y = state, color = arm)) +
  geom_line(size=0.8) +
  scale_color_manual(values = c("1" = "#D62728", 
                                "2" = "#1F77B4", 
                                "3" = "#2CA02C")) +
  facet_wrap(~ arm, ncol = 1) +
  labs(title = "States Per Arm",
       x = "Time", y = "State",color = "Arms") +
  theme(legend.position = "bottom")
```

## 1.3 Visualize Rewards Per Arm

```{r visualize-rewards}
y_df <- as.data.frame(y_local)
y_df <- y_df |> mutate(arm = factor(1:K)) |> 
  pivot_longer(cols = starts_with("V"), names_to = "time", values_to = "reward") |>
  mutate(time = as.numeric(gsub("V", "", time)))

ggplot(y_df, aes(x = time, y = reward, color = arm)) +
  geom_line(size=0.8) +
  facet_wrap(~ arm, ncol = 1) +
  scale_color_manual(values = c("1" = "#D62728", 
                                "2" = "#1F77B4", 
                                "3" = "#2CA02C")) +
  labs(title = "Rewards Per Arm",
       x = "Time", y = "Reward",color = "Arms") +
  theme(legend.position = "bottom")
```

## 1.4 Save Ground Truth for Inference

```{r save-data}
y_true <- y_local
z_true <- z_local
saveRDS(list(y = y_true, z = z_true, theta = theta, pi = pi_indiv), "local_truth_data.rds")
```

## 1.5 Baselines Algorithms
```{r bandit-functions}
bandit_baselines <- function(algorithm, K, N,y_true,z_true,theta) {

  
  # Oracle computation of best arm probability based
  theta_selected <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      theta_selected[i, t] <- theta[i, z_true[i,t] + 1]
    }
  }
  oracle_arm <- apply(theta_selected, 2, which.max)
  
  counts <- rep(0, K)
  rewards <- rep(0, K)

  warmup_order = sample(1:K)
  
  total_regret <- numeric(N)
  total_reward <- numeric(N)
  received_rewards <- numeric(N)
  chosen_arm <- numeric(N)
  regret <- 0

  for (t in 1:N) {
    if (t <= K) {
      idx <- warmup_order[t]
    } else {
      if (algorithm == "ucb") {
        ucb_values <- rewards / counts + sqrt(2 * log(t) / counts)
        idx <- which.max(ucb_values)
      } else if (algorithm == "ucb-tuned") {
        means <- rewards / counts
        variances <- pmin(1/4, means * (1 - means) + sqrt(2 * log(t) / counts))
        ucb_tuned_values <- means + sqrt(log(t) / counts * variances)
        idx <- which.max(ucb_tuned_values)
      } else if (algorithm == "ts") {
        samples <- rbeta(K, 1 + rewards, 1 + counts - rewards)
        idx <- which.max(samples)
      }
    }

    reward <- y_true[idx,t]
    counts[idx] <- counts[idx] + 1
    rewards[idx] <- rewards[idx] + reward
    
    received_rewards[t]<- reward

    regret <- regret + theta_selected[oracle_arm[t], t] - theta[idx, z_true[idx,t] + 1]
    total_regret[t] <- regret
    total_reward[t] <- sum(rewards)
    chosen_arm[t] <- idx
  }

  list(
    cumulative_regret = total_regret,
    cumulative_reward = total_reward,
    arm=chosen_arm
  )
}

baseline_ts_results <- bandit_baselines(algorithm='ts', K, N,y_true,z_true,theta)

baseline_ts_results_df<- data.frame(
  time = seq_along(baseline_ts_results$cumulative_reward),
  cumulative_reward = baseline_ts_results$cumulative_reward,
  cumulative_regret = baseline_ts_results$cumulative_regret
)


ggplot(baseline_ts_results_df, aes(x = time, y = cumulative_reward)) +
  geom_line(color = "#2CA02C", size = 1) +
  labs(title = "Baseline Thompson",
       x = "Time",
       y = "Cumulative Reward") +
  theme_minimal()

ggplot(baseline_ts_results_df, aes(x = time, y = cumulative_regret)) +
  geom_line(color = "#D62728", size = 1) +
  labs(title = "Baseline Thompson",
       x = "Time",
       y = "Cumulative Reward") +
  theme_minimal()

baseline_ucb_results <- bandit_baselines(algorithm='ucb-tuned', K, N,y_true,z_true,theta)

baseline_ucb_results_df<- data.frame(
  time = seq_along(baseline_ucb_results$cumulative_reward),
  cumulative_reward = baseline_ucb_results$cumulative_reward,
  cumulative_regret = baseline_ucb_results$cumulative_regret
)


ggplot(baseline_ucb_results_df, aes(x = time, y = cumulative_reward)) +
  geom_line(color = "#2CA02C", size = 1) +
  labs(title = "Baseline UCB",
       x = "Time",
       y = "Cumulative Reward") +
  theme_minimal()

ggplot(baseline_ucb_results_df, aes(x = time, y = cumulative_regret)) +
  geom_line(color = "#D62728", size = 1) +
  labs(title = "Baseline UCB",
       x = "Time",
       y = "Cumulative Reward") +
  theme_minimal()
```

## Baselines in expectation
```{r}
#set a number of runs
runs <- 100

reward_matrix_base_ts <- matrix(0, nrow = runs, ncol = N)
regret_matrix_base_ts <- matrix(0, nrow = runs, ncol = N)

set.seed(52)

for (run in 1:runs){
  
  z_true <- matrix(0, nrow = K, ncol = N)
  y_true <- matrix(0, nrow = K, ncol = N)
  
  for (i in 1:K) {
    z_true[i, 1] <- rbinom(1, 1, 0.5)
    y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
    for (t in 2:N) {
      z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
      y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
    }
  }
  
  results_baseline_ts <- bandit_baselines(algorithm='ts', K, N,y_true,z_true,theta)
  
  reward_matrix_base_ts[run,] <- results_baseline_ts$cumulative_reward
  regret_matrix_base_ts[run,] <- results_baseline_ts$cumulative_regret
  
  if (run%%10 == 0){cat("Finished run", run, "\n")}
}

expected_rewards_base_ts <- colMeans(reward_matrix_base_ts)
expected_regrets_base_ts <- colMeans(regret_matrix_base_ts)

base_ts_exp_results_df<- data.frame(
  time = seq_along(expected_rewards_base_ts),
  expected_reward = expected_rewards_base_ts,
  expected_regret = expected_regrets_base_ts
)

ggplot(base_ts_exp_results_df, aes(x = time, y = expected_reward)) +
  geom_line(color = "#2CA02C", size = 1) +
  labs(title = "Baseline Thompson",
       x = "Time",
       y = "Expected Cumulative Reward") +
  theme_minimal()

ggplot(base_ts_exp_results_df, aes(x = time, y = expected_regret)) +
  geom_line(color = "#D62728", size = 1) +
  labs(title = "Expected Regret vs Oracle",
       x = "Time",
       y = "Expected Cumulative Regret") +
  theme_minimal()
```

```{r}
#set a number of runs
runs <- 100

reward_matrix_base_ucb <- matrix(0, nrow = runs, ncol = N)
regret_matrix_base_ucb <- matrix(0, nrow = runs, ncol = N)

set.seed(52)

for (run in 1:runs){
  
  z_true <- matrix(0, nrow = K, ncol = N)
  y_true <- matrix(0, nrow = K, ncol = N)
  
  for (i in 1:K) {
    z_true[i, 1] <- rbinom(1, 1, 0.5)
    y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
    for (t in 2:N) {
      z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
      y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
    }
  }
  
  results_baseline_ucb <- bandit_baselines(algorithm='ucb-tuned', K, N,y_true,z_true,theta)
  
  reward_matrix_base_ucb[run,] <- results_baseline_ucb$cumulative_reward
  regret_matrix_base_ucb[run,] <- results_baseline_ucb$cumulative_regret
  
  if (run%%10 == 0){cat("Finished run", run, "\n")}
}

expected_rewards_base_ucb <- colMeans(reward_matrix_base_ucb)
expected_regrets_base_ucb <- colMeans(regret_matrix_base_ucb)

base_ucb_exp_results_df<- data.frame(
  time = seq_along(expected_rewards_base_ucb),
  expected_reward = expected_rewards_base_ucb,
  expected_regret = expected_regrets_base_ucb
)

ggplot(base_ucb_exp_results_df, aes(x = time, y = expected_reward)) +
  geom_line(color = "#2CA02C", size = 1) +
  labs(title = "Baseline UCB",
       x = "Time",
       y = "Expected Cumulative Reward") +
  theme_minimal()

ggplot(base_ucb_exp_results_df, aes(x = time, y = expected_regret)) +
  geom_line(color = "#D62728", size = 1) +
  labs(title = "Expected Regret vs Oracle",
       x = "Time",
       y = "Expected Cumulative Regret") +
  theme_minimal()
```


# 2. Poor Model - Thompson Sampling

## 2.1 Import JAGS Model

```{r define-ts-model}
model_file <- "poor_model.jags"
```

## 2.2 Sequential Thompson Sampling Loop with Reward-Only Observations

```{r}
#recover same simulation we ran before
y_true <- y_local
z_true <- z_local
```

Create a dummy model once per arm:
```{r}
dummy_data <- list(y = c(0,1), N = 2)  # minimal dummy data just to initialize
base_models <- lapply(1:K, function(k) jags.model(model_file, data = dummy_data, n.chains = 1, quiet = TRUE))
```


```{r thompson-sampling-loop-z-aware}
thompson_poor <- function(K,N,theta,y_true,z_true,Batch_size){
  model_file <- "poor_model.jags"
  observed_rewards <- vector("list", K)
  selected_arms <- numeric(N)
  received_rewards <- numeric(N)
  regret <- numeric(N)

  # Oracle computation of best arm probability based
  theta_selected <- matrix(NA, nrow = K, ncol = N)
  for (k in 1:K) {
    for (t in 1:N) {
      theta_selected[k, t] <- theta[k, z_true[k, t] + 1]
    }
  }
  
  oracle_arm <- apply(theta_selected, 2, which.max)


  posterior_matrices <- rep(list(1), K) #create a list of list to store the matrices (we initiate a 1 to avoid NULL errors)
  z_last <- list() #list that will contain last samples of latent states

  a_t <- sample(1:K, 1) #pick first arm to pull

  for (t in 1:N) {
    sampled_values <- numeric(K)
  
    if (length(observed_rewards[[a_t]]) >= 2 && t%%Batch_size == 0) { # we only do inference if the arm has been pulled on last iteration
    
      # JAGS inference
      data_list <- list(y = observed_rewards[[a_t]], N = length(observed_rewards[[a_t]]))
      #model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
      # CHANGE 2
      # Create a fresh clone from the base model
      model <- jags.model(textConnection(model_string), data = data_list, n.chains = 1, quiet = TRUE)
      # CHANGE 3
      update(model, 500) # was before 1000
    
      #Store the posteriors in the posteriors matrix
      # CAHNGE 4
      post <- coda.samples(model, c("theta0", "theta1", "pi",paste0("z[", length(observed_rewards[[a_t]]), "]")), n.iter = 100) # was before 100
      post_matrix <- as.matrix(post)
      posterior_matrices[[a_t]] <- post_matrix
    
      #Store z_last provided by JAGS
      z_last[[a_t]] <- post_matrix[, paste0("z[", length(observed_rewards[[a_t]]), "]")]
    
    }
  
    for (l in 1:K){ #Now for each arm we evolve the states from z_last to t , sample one and get the theta form the posterior
    
      if (!is.null(nrow(posterior_matrices[[l]])) > 0){
        idx <- sample(1:nrow(posterior_matrices[[l]]), 1) # get one sample index
      
        # get theta for each state
        theta0 <- posterior_matrices[[l]][idx, "theta0"] 
        theta1 <- posterior_matrices[[l]][idx, "theta1"]
      
        #get transition probabilities form the posterior
        pi0 <- posterior_matrices[[l]][, "pi[1]"]
        pi1 <- posterior_matrices[[l]][, "pi[2]"]
        sampled_pi <- ifelse(z_last[[l]] == 0, pi0, pi1) #take the transition probability corresponding to last state
      
        z_sampled <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi) #evolve the state
        z_last[[l]] <- z_sampled #store the evolved state ad last state for next iteration
      
        z_t <- z_sampled[idx] #sample from evolved states
      
        sampled_values[l] <- (1-z_t)* theta0 + z_t * theta1 #get corresponding theta
      }
      else {
      sampled_values[l] <- runif(1) #if we don't have enough samples to do inference we just sample from an uniform distribution
      }
    }
  
    a_t <- which.max(sampled_values) #take the arm with highes sampled probability
    selected_arms[t] <- a_t
    r_t <- y_true[a_t, t] #observe reward
    received_rewards[t] <- r_t
    regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[a_t,t] + 1] #compute regret -- WHY PLUS ONE
    observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t) #apend the reward to the list of corresponding arm
  }
  
  cumulative_reward <- cumsum(received_rewards)
  cumulative_regret <- cumsum(regret)
  return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrices=posterior_matrices))
}

system.time({
poor_ts_results <- thompson_poor(K,N,theta,y_true,z_true,Batch_size=1)
})

poor_ts_results_df<- data.frame(
  time = seq_along(poor_ts_results$cumulative_reward),
  cumulative_reward = poor_ts_results$cumulative_reward,
  cumulative_regret = poor_ts_results$cumulative_regret
)


ggplot(poor_ts_results_df, aes(x = time, y = cumulative_reward)) +
  geom_line(color = "#2CA02C", size = 1) +
  labs(title = "Baseline Thompson",
       x = "Time",
       y = "Cumulative Reward") +
  theme_minimal()

ggplot(poor_ts_results_df, aes(x = time, y = cumulative_regret)) +
  geom_line(color = "#D62728", size = 1) +
  labs(title = "Baseline Thompson",
       x = "Time",
       y = "Cumulative Regret") +
  theme_minimal()
```

698.045 seconds to run initially

We see how good the model has achieved inference on each arm dynamics (even if the ground truth is a global state):
```{r}
for (arm in 1:K){
  mean_pi_01 <- mean(poor_ts_results$posterior_matrices[[arm]][, "pi[1]"])
  mean_pi_11 <- mean(poor_ts_results$posterior_matrices[[arm]][, "pi[2]"])
  mean_pi_00 <- 1 - mean_pi_01
  mean_pi_10 <- 1 - mean_pi_11
  
  estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
                           mean_pi_10, mean_pi_11),
                         nrow = 2, byrow = TRUE,
                         dimnames = list(c("From A", "From B"),
                                         c("To A", "To B")))
  
  mean_theta_0 <- mean(poor_ts_results$posterior_matrices[[arm]][, "theta0"])
  mean_theta_1 <- mean(poor_ts_results$posterior_matrices[[arm]][, "theta1"])
  
  estimated_theta <- matrix(c(mean_theta_0, mean_theta_1),
                         nrow = 1, byrow = TRUE,
                         dimnames = list(NULL,c("State A", "State B")))

  cat("\nEstimated Transition Matrix for arm ",arm,":\n")
  print(estimated_pi)
  cat("\nEstimated Reward probabilities for arm ", arm,":\n")
  print(estimated_theta)
}
```

# Run in Expectation

```{r}
#We define the parameters of the simulations
runs <- 100
Batch_size <-10

reward_matrix_poor_ts <- matrix(0, nrow = runs, ncol = N)
regret_matrix_poor_ts <- matrix(0, nrow = runs, ncol = N)

set.seed(52)

for (run in 1:runs){
  
  z_true <- matrix(0, nrow = K, ncol = N)
  y_true <- matrix(0, nrow = K, ncol = N)
  
  for (i in 1:K) {
    z_true[i, 1] <- rbinom(1, 1, 0.5)
    y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
    for (t in 2:N) {
      z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
      y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
    }
  }
  
  results <- thompson_poor(K,N,theta,y_true,z_true,Batch_size)
  
  reward_matrix_poor_ts[run,] <- results$cumulative_reward
  regret_matrix_poor_ts[run,] <- results$cumulative_regret
  
  if (run%%10 == 0){cat("Finished run", run, "\n")}
}

expected_rewards_poor_ts <- colMeans(reward_matrix_poor_ts)
expected_regrets_poor_ts <- colMeans(regret_matrix_poor_ts)

poor_ts_exp_results_df<- data.frame(
  time = seq_along(expected_rewards_poor_ts),
  expected_reward = expected_rewards_poor_ts,
  expected_regret = expected_regrets_poor_ts
)

ggplot(poor_ts_exp_results_df, aes(x = time, y = expected_reward)) +
  geom_line(color = "#2CA02C", size = 1) +
  labs(title = "Arm-Specific Latent-State Thompson Sampling",
       x = "Time",
       y = "Expected Cumulative Reward") +
  theme_minimal()

ggplot(poor_ts_exp_results_df, aes(x = time, y = expected_regret)) +
  geom_line(color = "#D62728", size = 1) +
  labs(title = "Expected Regret vs Oracle",
       x = "Time",
       y = "Expected Cumulative Regret") +
  theme_minimal()
``` 


# 3. Poor Model - UCB


```{r}
#recover same simulation we ran before
y_true <- y_local
z_true <- z_local
```

```{r}
ucb_poor <- function(K,N,theta,y_true,z_true,Batch_size) {
  
  model_file <- "poor_model.jags"
  observed_rewards <- vector("list", K)
  selected_arms <- numeric(N)
  received_rewards <- numeric(N)
  regret <- numeric(N)

  # Oracle computation using ground-truth global latent state:
  theta_selected <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      theta_selected[i, t] <- theta[i, z_true[i,t] + 1]
    }
  }
  oracle_arm <- apply(theta_selected, 2, which.max)

  posterior_matrices <- rep(list(1), K)
  z_last <- list()
  a_t <- sample(1:K, 1) #pick first arm to pull

  for (t in 1:N) {
    sampled_values <- numeric(K)
  
    if (length(observed_rewards[[a_t]]) >= 2 && t%%Batch_size==0) {
      
      data_list <- list(y = observed_rewards[[a_t]], N = length(observed_rewards[[a_t]]))
      model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
      update(model, 1000)
      
      post <- coda.samples(model, c("theta0", "theta1", "pi",paste0("z[", length(observed_rewards[[a_t]]), "]")), n.iter = 100)
      post_matrix <- as.matrix(post)
      posterior_matrices[[a_t]] <- post_matrix
      
      z_last[[a_t]] <- post_matrix[, paste0("z[", length(observed_rewards[[a_t]]), "]")]
      
    }
    
    for (l in 1:K){
      
      if (!is.null(nrow(posterior_matrices[[l]])) > 0){
        idx <- sample(1:nrow(posterior_matrices[[l]]), 1)
        
        theta0 <- posterior_matrices[[l]][idx, "theta0"]
        theta1 <- posterior_matrices[[l]][idx, "theta1"]
        
        pi0 <- posterior_matrices[[l]][, "pi[1]"]
        pi1 <- posterior_matrices[[l]][, "pi[2]"]
        sampled_pi <- ifelse(z_last[[l]] == 0, pi0, pi1)
        
        z_sampled <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi)
        z_last[[l]] <- z_sampled
        
        sampled_theta <- ifelse(z_sampled == 0, theta0, theta1)
        
        sampled_values[l] <- mean(sampled_theta) + sd(sampled_theta)/sqrt(length(sampled_theta))*log(t)
      }
      else {
      sampled_values[l] <- runif(1)
      }
    }
    
    a_t <- which.max(sampled_values)
    selected_arms[t] <- a_t
    r_t <- y_true[a_t, t]
    received_rewards[t] <- r_t
    regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[a_t,t] + 1]
    observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
  }

  cumulative_reward <- cumsum(received_rewards)
  cumulative_regret <- cumsum(regret)
  return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrices=posterior_matrices))
}

system.time({
poor_ucb_results <- ucb_poor(K,N,theta,y_true,z_true,Batch_size=1)
})

poor_ucb_results_df<- data.frame(
  time = seq_along(poor_ucb_results$cumulative_reward),
  cumulative_reward = poor_ucb_results$cumulative_reward,
  cumulative_regret = poor_ucb_results$cumulative_regret
)


ggplot(poor_ucb_results_df, aes(x = time, y = cumulative_reward)) +
  geom_line(color = "#2CA02C", size = 1) +
  labs(title = "Baseline Thompson",
       x = "Time",
       y = "Cumulative Reward") +
  theme_minimal()

ggplot(poor_ucb_results_df, aes(x = time, y = cumulative_regret)) +
  geom_line(color = "#D62728", size = 1) +
  labs(title = "Baseline Thompson",
       x = "Time",
       y = "Cumulative Reward") +
  theme_minimal()
```

```{r}
for (arm in 1:K){
  mean_pi_01 <- mean(poor_ucb_results$posterior_matrices[[arm]][, "pi[1]"])
  mean_pi_11 <- mean(poor_ucb_results$posterior_matrices[[arm]][, "pi[2]"])
  mean_pi_00 <- 1 - mean_pi_01
  mean_pi_10 <- 1 - mean_pi_11
  
  estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
                           mean_pi_10, mean_pi_11),
                         nrow = 2, byrow = TRUE,
                         dimnames = list(c("From A", "From B"),
                                         c("To A", "To B")))
  
  mean_theta_0 <- mean(poor_ucb_results$posterior_matrices[[arm]][, "theta0"])
  mean_theta_1 <- mean(poor_ucb_results$posterior_matrices[[arm]][, "theta1"])
  
  estimated_theta <- matrix(c(mean_theta_0, mean_theta_1),
                         nrow = 1, byrow = TRUE,
                         dimnames = list(NULL,c("State A", "State B")))

  cat("\nEstimated Transition Matrix for arm ",arm,":\n")
  print(estimated_pi)
  cat("\nEstimated Reward probabilities for arm ", arm,":\n")
  print(estimated_theta)
}
  
```
# Run in Expectation

```{r}
#We define the parameters of the simulations
runs <- 100
Batch_size <-10

reward_matrix_poor_ucb <- matrix(0, nrow = runs, ncol = N)
regret_matrix_poor_ucb <- matrix(0, nrow = runs, ncol = N)

set.seed(52)

for (run in 1:runs){
  
  z_true <- matrix(0, nrow = K, ncol = N)
  y_true <- matrix(0, nrow = K, ncol = N)
  
  for (i in 1:K) {
    z_true[i, 1] <- rbinom(1, 1, 0.5)
    y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
    for (t in 2:N) {
      z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
      y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
    }
  }
  
  results <- ucb_poor(K,N,theta,y_true,z_true,Batch_size)
  
  reward_matrix_poor_ucb[run,] <- results$cumulative_reward
  regret_matrix_poor_ucb[run,] <- results$cumulative_regret
  
  if (run%%10 == 0){cat("Finished run", run, "\n")}
}

expected_rewards_poor_ucb <- colMeans(reward_matrix_poor_ucb)
expected_regrets_poor_ucb <- colMeans(regret_matrix_poor_ucb)

poor_ucb_exp_results_df<- data.frame(
  time = seq_along(expected_rewards_poor_ucb),
  expected_reward = expected_rewards_poor_ucb,
  expected_regret = expected_regrets_poor_ucb
)

ggplot(poor_ucb_exp_results_df, aes(x = time, y = expected_reward)) +
  geom_line(color = "#2CA02C", size = 1) +
  labs(title = "Arm-Specific Latent-State UCB",
       x = "Time",
       y = "Expected Cumulative Reward") +
  theme_minimal()

ggplot(poor_ucb_exp_results_df, aes(x = time, y = expected_regret)) +
  geom_line(color = "#D62728", size = 1) +
  labs(title = "Expected Regret vs Oracle",
       x = "Time",
       y = "Expected Cumulative Regret") +
  theme_minimal()
``` 


# 4. Advanced Model - TS

## 4.1 Load the Model
```{r}
model_adv <- "advanced_model.jags"
```

```{r}
#recover same simulation we ran before
y_true <- y_local
z_true <- z_local
```
## 4.2 Sequential Thompson Sampling Loop with Reward-Only Observations

```{r}
thompson_advanced <- function(K,N,theta,y_true,z_true,Batch_size){
  
  model_adv <- "advanced_model.jags"
  observed_rewards <- list()
  selected_arms <- numeric(N)
  received_rewards <- numeric(N)
  regret <- numeric(N)
  
  # Oracle computation using ground-truth global latent state:
  theta_selected <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      theta_selected[i, t] <- theta[i, z_true[i,t] + 1]
    }
  }
  oracle_arm <- apply(theta_selected, 2, which.max)
  
  # Storage for posterior tracking:
  post_matrix<- NULL
  
  # Initialization:
  
  selected_arms[1] <- sample(1:K, 1)
  observed_data <- matrix(NA, nrow = K, ncol = N)
  observed_data[selected_arms[1], 1] <- y_true[selected_arms[1], 1]
  received_rewards[1] <- y_true[selected_arms[1], 1]
  regret[1] <- theta_selected[oracle_arm[1], 1] - theta[selected_arms[1], z_true[selected_arms[1],1] + 1]
  
  # Start Thompson Sampling loop:
  for (t in 2:N) {
    sampled_values <- numeric(K)
  
    # Check if we have enough observed data:
    if (sum(!is.na(observed_data)) >= K * 2 && t%%Batch_size==0) {
      data_list <- list(
        y_obs = observed_data[, 1:(t-1)],
        K = K,
        N = t - 1
      )
  
      model <- jags.model(model_adv, data = data_list, n.chains = 1, quiet = TRUE)
      update(model, 1000)
      post <- coda.samples(model, c("theta", "pi", paste0("z[", t - 1, "]")), n.iter = 100)
      post_matrix <- as.matrix(post)
    }
    
      if (is.matrix(post_matrix)) {
        idx <- sample(1:nrow(post_matrix), 1)
      
        pi0 <- post_matrix[idx,"pi[1]"]
        pi1 <- post_matrix[idx,"pi[2]"]
        
        if (t%%Batch_size==0) { z_last <- post_matrix[idx, paste0("z[", t-1, "]")]}
        
        sampled_pi <- ifelse(z_last == 0, pi0, pi1)
        z_t <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi)
        z_last <- z_t
      }
    
    # Arm selection:
    for (i in 1:K) {
      if (is.matrix(post_matrix)) {
        theta0 <- post_matrix[idx, paste0("theta[", i, ",1]")]
        theta1 <- post_matrix[idx, paste0("theta[", i, ",2]")]
        sampled_values[i] <- (1 - z_t) * theta0 + z_t * theta1
      } else {
        sampled_values[i] <- runif(1)
      }
    }
  
    selected_arms[t] <- which.max(sampled_values)
    r_t <- y_true[selected_arms[t], t]
    received_rewards[t] <- r_t
    regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[i,t] + 1]
    observed_data[selected_arms[t], t] <- r_t
  }
  
  cumulative_reward <- cumsum(received_rewards)
  cumulative_regret <- cumsum(regret)
  return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrix=post_matrix))
}

system.time({
results<- thompson_advanced (K,N,theta,y_true,z_true,Batch_size=1)
})

plot(results$cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling (Advanced Model)")

plot(results$cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```

The latest posterior matrix that exists (the most recent sample from JAGS):
```{r}
last_posterior <- results$posterior_matrix

if (all(c("pi[1]", "pi[2]") %in% colnames(last_posterior))) {
  mean_pi_01 <- mean(last_posterior[, "pi[1]"])
  mean_pi_11 <- mean(last_posterior[, "pi[2]"])
  mean_pi_00 <- 1 - mean_pi_01
  mean_pi_10 <- 1 - mean_pi_11
  
  estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
                           mean_pi_10, mean_pi_11),
                         nrow = 2, byrow = TRUE,
                         dimnames = list(c("From A", "From B"),
                                         c("To A", "To B")))
  
  mean_theta_01 <- mean(last_posterior[, "theta[1,1]"])
  mean_theta_11 <- mean(last_posterior[, "theta[1,2]"])
  mean_theta_02 <- mean(last_posterior[, "theta[2,1]"])
  mean_theta_12 <- mean(last_posterior[, "theta[2,2]"])
  mean_theta_03 <- mean(last_posterior[, "theta[3,1]"])
  mean_theta_13 <- mean(last_posterior[, "theta[3,2]"])
  
  estimated_theta <- matrix(c(mean_theta_01, mean_theta_11,
                           mean_theta_02,mean_theta_12,
                           mean_theta_03,mean_theta_13),
                         nrow = 3, byrow = TRUE,
                         dimnames = list(c("Arm 1", "Arm 2", "Arm 3"),
                                         c("State A", "State B")))

  cat("\nEstimated Common Transition Matrix:\n")
  print(estimated_pi)
    cat("\nEstimated Reward probabilities:\n")
  print(estimated_theta)
} else {
  cat("Transition probabilities pi[1] and pi[2] not found in posterior samples.\n")
}
```

#Run on Expectation

```{r,results=False}
#We define the parameters of the simulations
runs <- 10
Batch_size <-10

reward_matrix <- matrix(0, nrow = runs, ncol = N)
regret_matrix <- matrix(0, nrow = runs, ncol = N)

for (run in 1:runs){
  
  z_true <- matrix(0, nrow = K, ncol = N)
  y_true <- matrix(0, nrow = K, ncol = N)
  
  for (i in 1:K) {
    z_true[i, 1] <- rbinom(1, 1, 0.5)
    y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
    for (t in 2:N) {
      z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
      y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
    }
  }

  results <- thompson_advanced(K,N,theta,y_true,z_true,Batch_size)
  
  reward_matrix[run,] <- results$cumulative_reward
  regret_matrix[run,] <- results$cumulative_regret
  
  if (run%%10 == 0){cat("Finished run", run, "\n")}
}

expected_rewards_thompson_advanced <- colMeans(reward_matrix)
expected_regrets_thompson_advanced <- colMeans(regret_matrix)

plot(expected_rewards_thompson_advanced, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Reward", main = "Thompson Sampling (Advanced Model)")

plot(expected_regrets_thompson_advanced, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Regret", main = "Expected Regret vs Oracle")
```
## Global UCB

```{r}
#recover same simulation we ran before
y_true <- y_local
z_true <- z_local
K <- 3
N <- 100
```

```{r}
ucb_advanced <- function(K,N,theta,y_true,z_true,Batch_size){
  
  model_adv <- "advanced_model.jags"
  observed_rewards <- list()
  selected_arms <- numeric(N)
  received_rewards <- numeric(N)
  regret <- numeric(N)
  
  # Oracle computation using ground-truth global latent state:
  theta_selected <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      theta_selected[i, t] <- theta[i, z_true[i,t] + 1]
    }
  }
  oracle_arm <- apply(theta_selected, 2, which.max)
  
  # Storage for posterior tracking:
  post_matrix<- NULL
  
  # Initialization:
  selected_arms[1] <- sample(1:K, 1)
  observed_data <- matrix(NA, nrow = K, ncol = N)
  observed_data[selected_arms[1], 1] <- y_true[selected_arms[1], 1]
  received_rewards[1] <- y_true[selected_arms[1], 1]
  regret[1] <- theta_selected[oracle_arm[1], 1] - theta[selected_arms[1], z_true[selected_arms[1],1] + 1]
  
  # Start Thompson Sampling loop:
  for (t in 2:N) {
    sampled_values <- numeric(K)
  
    # Check if we have enough observed data:
    if (sum(!is.na(observed_data)) >= K * 2  && t%%Batch_size == 0) {
      data_list <- list(
        y_obs = observed_data[, 1:(t-1)],
        K = K,
        N = t - 1
      )
  
      model <- jags.model(model_adv, data = data_list, n.chains = 1, quiet = TRUE)
      update(model, 1000)
      post <- coda.samples(model, c("theta", "pi", paste0("z[", t - 1, "]")), n.iter = 100)
      post_matrix <- as.matrix(post)
    }
  
    if (is.matrix(post_matrix)) {
      pi0 <- post_matrix[,"pi[1]"]
      pi1 <- post_matrix[,"pi[2]"]
      
      if (t%%Batch_size==0) { z_last <- post_matrix[, paste0("z[", t-1, "]")]}
      
      sampled_pi <- ifelse(z_last == 0, pi0, pi1)
      
      z_t <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi)
      z_last <- z_t
    }
    
    # Arm selection:
    for (i in 1:K) {
      if (is.matrix(post_matrix)) {
        
        theta0 <- post_matrix[, paste0("theta[", i, ",1]")]
        theta1 <- post_matrix[, paste0("theta[", i, ",2]")]
        
        sampled_theta <- ifelse(z_t == 0, theta0, theta1)
        
        sampled_values[i] <- mean(sampled_theta) + sd(sampled_theta)/sqrt(length(sampled_theta))*log(t)
      } else {
        sampled_values[i] <- runif(1)
      }
    }
  
    selected_arms[t] <- which.max(sampled_values)
    r_t <- y_true[selected_arms[t], t]
    received_rewards[t] <- r_t
    regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[i,t] + 1]
    observed_data[selected_arms[t], t] <- r_t
  }
  
  cumulative_reward <- cumsum(received_rewards)
  cumulative_regret <- cumsum(regret)
  return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrix=post_matrix))
}

system.time({
results <- ucb_advanced(K,N,theta,y_true,z_true,Batch_size=1)
})
  
plot(results$cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "UCB (Advanced Model)")

plot(results$cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```

```{r}
last_posterior <- results$posterior_matrix

if (all(c("pi[1]", "pi[2]") %in% colnames(last_posterior))) {
  mean_pi_01 <- mean(last_posterior[, "pi[1]"])
  mean_pi_11 <- mean(last_posterior[, "pi[2]"])
  mean_pi_00 <- 1 - mean_pi_01
  mean_pi_10 <- 1 - mean_pi_11
  
  estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
                           mean_pi_10, mean_pi_11),
                         nrow = 2, byrow = TRUE,
                         dimnames = list(c("From A", "From B"),
                                         c("To A", "To B")))
  
  mean_theta_01 <- mean(last_posterior[, "theta[1,1]"])
  mean_theta_11 <- mean(last_posterior[, "theta[1,2]"])
  mean_theta_02 <- mean(last_posterior[, "theta[2,1]"])
  mean_theta_12 <- mean(last_posterior[, "theta[2,2]"])
  mean_theta_03 <- mean(last_posterior[, "theta[3,1]"])
  mean_theta_13 <- mean(last_posterior[, "theta[3,2]"])
  
  estimated_theta <- matrix(c(mean_theta_01, mean_theta_11,
                           mean_theta_02,mean_theta_12,
                           mean_theta_03,mean_theta_13),
                         nrow = 3, byrow = TRUE,
                         dimnames = list(c("Arm 1", "Arm 2", "Arm 3"),
                                         c("State A", "State B")))

  cat("\nEstimated Common Transition Matrix:\n")
  print(estimated_pi)
    cat("\nEstimated Reward probabilities:\n")
  print(estimated_theta)
} else {
  cat("Transition probabilities pi[1] and pi[2] not found in posterior samples.\n")
}
```


#Run on Expectation

```{r,results=False}
#We define the parameters of the simulations
runs <- 10
Batch_size <-10

reward_matrix <- matrix(0, nrow = runs, ncol = N)
regret_matrix <- matrix(0, nrow = runs, ncol = N)

for (run in 1:runs){
  
  z_true <- matrix(0, nrow = K, ncol = N)
  y_true <- matrix(0, nrow = K, ncol = N)
  
  for (i in 1:K) {
    z_true[i, 1] <- rbinom(1, 1, 0.5)
    y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
    for (t in 2:N) {
      z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
      y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
    }
  }
  
  results <- ucb_advanced(K,N,theta,y_true,z_true,Batch_size)
  
  reward_matrix[run,] <- results$cumulative_reward
  regret_matrix[run,] <- results$cumulative_regret
  
  if (run%%10 == 0){cat("Finished run", run, "\n")}
}

expected_rewards_ucb_advanced <- colMeans(reward_matrix)
expected_regrets_ucb_advanced <- colMeans(regret_matrix)

plot(expected_rewards_ucb_advanced, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Reward", main = "UCB (Advanced Model)")

plot(expected_regrets_ucb_advanced, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Regret", main = "Expected Regret vs Oracle")
```
```{r}
library(DiagrammeR)

graph<-grViz("
digraph {
  rankdir=LR;
  splines=false;

  subgraph cluster_s2 {
    label = 't = 1 : T';
    style = dashed;
    
    O [label=<<I>Y</I><SUB>t</SUB><SUP>o</SUP>>, shape=circle, style=filled, fillcolor=gray90];

    subgraph cluster_s1 {
      label = 'a = 1 : K';
      style = rounded;

      P [shape=point];
      Z [label=<<I>Z</I><SUB>t</SUB><SUP>a</SUP>>, shape=circle];
      T [label=<<I>µ</I><SUB>t</SUB><SUP>a</SUP>>, shape=circle];
      Y [label=<<I>Y</I><SUB>t</SUB><SUP>a</SUP>>, shape=circle];
    }
  }


  P -> Z[xlabel=<<B>P</B><SUB>a</SUB>>, fontsize=10, labeljust='r', labelangle=90];
  Z -> Z;
  Z -> T;
  T -> Y;
  Y -> O;
}
")

svg <- export_svg(graph)
rsvg_png(charToRaw(svg), file = "poor_plate_diagram.png")


```

```{r}
library(DiagrammeR)
library(DiagrammeRsvg)
library(rsvg)

graph<-grViz("
digraph {
  rankdir=LR;
  splines=false;

  subgraph cluster_s2 {
    label = 't = 1 : T';
    style = dashed;
    
    A [label=<<I>Z</I><SUB>t</SUB>>, shape=circle];
    D [label=<<I>Y</I><SUB>t</SUB><SUP>o</SUP>>, shape=circle, style=filled, fillcolor=gray90];

    subgraph cluster_s1 {
      label = 'a = 1 : K';
      style = rounded;

      B [label=<<I>µ</I><SUB>t</SUB><SUP>a</SUP>>, shape=circle];
      C [label=<<I>Y</I><SUB>t</SUB><SUP>a</SUP>>, shape=circle];
    }
  }

  P [shape=point];
  
  P -> A[xlabel=<<B>P</B>>, fontsize=12];
  A -> A;
  A -> B;
  B -> C;
  C -> D;
}
")

svg <- export_svg(graph)
rsvg_png(charToRaw(svg), file = "adv_plate_diagram.png")

```
