---
title: "Formal Setup for HMM-Based Bandit Models"
author: "Marvin Ernst, Oriol Gelabert, Melisa Vadenja"
date: "`r Sys.Date()`"
output: html_document
---

If you don't have it installed yet, **you first need to download and install JAGS to your local system**, then run this:
```{r}
#install.packages("rjags")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
```

# Problem Setting

We consider a $K$-armed bandit problem where rewards are driven by latent (hidden) Markov processes. At each time $t = 1, \dots, T$, a learner selects one arm $a_t \in \{1, \dots, K\}$ and observes a binary reward $y_{a_t, t} \in \{0,1\}$.

## Two-State Setting

Each arm $i$ has a latent state $z_{i,t} \in \{0,1\}$. Rewards are modeled as:

$$y_{i,t} \mid z_{i,t} \sim \text{Bernoulli}(\theta_{i, z_{i,t}})$$

Transitions:

$$z_{i,t} \mid z_{i,t-1} \sim \text{Bernoulli}(\pi_{i, z_{i,t-1}, \cdot})$$

Initial state:

$$z_{i,1} \sim \text{Bernoulli}(\pi_i^{(0)})$$

Priors:

$$\theta_{i,0}, \theta_{i,1} \sim \text{Beta}(1,1), \quad \pi_{i,01}, \pi_{i,10} \sim \text{Beta}(1,1)$$

## More-Than-Two-State Setting

Latent states $z_{i,t} \in \{1, \dots, S\}$:

$$y_{i,t} \mid z_{i,t} \sim \text{Bernoulli}(\theta_{i, z_{i,t}})$$

Transitions:

$$z_{i,t} \mid z_{i,t-1} \sim \text{Categorical}(\pi_{i, z_{i,t-1}, \cdot})$$

Initial state:

$$z_{i,1} \sim \text{Categorical}(\pi_{i}^{(0)})$$

Priors:

$$\theta_{i,s} \sim \text{Beta}(1,1), \quad \pi_{i,l\cdot} \sim \text{Dirichlet}(1, \dots, 1)$$

# Models

## Model 1: Arm-Specific Latent Dynamics ("Poor Model")
- Each arm has its own transition matrix and latent state process.
- This allows inference to be done independently per arm.
- Implemented in `JAGS` for tractability and simplicity.

## Model 2: Global Latent State ("Advanced Model")
- All arms share a common latent state $z_t$ and a shared transition matrix.
- More complex, as inference must be joint across all arms.
- Implementing this in `JAGS` is harder due to shared state across arms; may require customization or marginalization strategies - this we will have to figure out.

# Inference and Implementation Plan

In `JAGS`, we will begin with the "poor model" because:
- It allows easier modular implementation per arm.
- It's parallelizable.
- It helps validate inference machinery and synthetic generation.

Here is how we can simulate the data with a Global Latent State Shared Across Arms:
```{r simulate-global-state}
set.seed(123)
K <- 3 # number of arms
N <- 100 # time steps

theta <- matrix(c(0.2, 0.8,
                  0.5, 0.7,
                  0.3, 0.9),
                nrow = K, byrow = TRUE)

pi_global <- matrix(c(0.9, 0.1,
                      0.1, 0.9), nrow = 2, byrow = TRUE)

z_global <- numeric(N)
z_global[1] <- rbinom(1, 1, 0.5)

for (t in 2:N) {
  z_global[t] <- rbinom(1, 1, pi_global[z_global[t - 1] + 1, 2])
}

y_global <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
  for (t in 1:N) {
    y_global[i, t] <- rbinom(1, 1, theta[i, z_global[t] + 1])
  }
}
```

However, we will first consider a setup where we will first investigate a scnario where the ground truth is that we have arm-specifc states.

---

# Sequential Thompson Sampling Setup

This section shows a simulation where we choose one arm at each time step using **Thompson Sampling**, and update the corresponding posterior only using the rewards observed so far for that arm. We are in the **Two-Sate-Setup**.

## 1. Simulate Full Ground Truth (Arm-Specific Model)

```{r simulate-truth}
set.seed(456)
K <- 3
N <- 1000

theta <- matrix(c(0.2, 0.8,
                  0.5, 0.7,
                  0.3, 0.9),
                nrow = K, byrow = TRUE)

pi_indiv <- list(
  matrix(c(0.9, 0.1,
           0.1, 0.9), 2, 2, byrow = TRUE),
  matrix(c(0.85, 0.15,
           0.2, 0.8), 2, 2, byrow = TRUE),
  matrix(c(0.95, 0.05,
           0.3, 0.7), 2, 2, byrow = TRUE)
)

z_true <- matrix(0, nrow = K, ncol = N)
y_true <- matrix(0, nrow = K, ncol = N)

for (i in 1:K) {
  z_true[i, 1] <- rbinom(1, 1, 0.5)
  y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
  for (t in 2:N) {
    z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
    y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
  }
}
```

## 2. Import JAGS Model (Poor Model)

```{r define-ts-model}
model_file <- "poor_model.jags"
```

## 3. Sequential Thompson Sampling Loop with Reward-Only Observations

We sample latent states via forward filtering to compute $\theta_{i, t+1}$.

```{r thompson-sampling-loop-z-aware}
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)

oracle_rewards <- apply(y_true, 1, sum)
oracle_arm <- which.max(oracle_rewards)

for (t in 1:N) {
  sampled_values <- numeric(K)

  for (i in 1:K) {
    if (length(observed_rewards[[i]]) >= 2) {
      data_list <- list(y = observed_rewards[[i]], N = length(observed_rewards[[i]]))
      model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
      update(model, 100)
      post <- coda.samples(model, c("theta0", "theta1", "pi"), n.iter = 100)
      post_matrix <- as.matrix(post)
      idx <- sample(1:nrow(post_matrix), 1)
      theta0 <- post_matrix[idx, "theta0"]
      theta1 <- post_matrix[idx, "theta1"]
      pi1 <- post_matrix[idx, "pi[1]"]
      pi2 <- post_matrix[idx, "pi[2]"]
      pz0 <- 0.5
      pz1 <- 0.5
      for (s in 1:5) {
        new_pz0 <- pz0 * (1 - pi1) + pz1 * pi2
        new_pz1 <- pz0 * pi1 + pz1 * (1 - pi2)
        norm <- new_pz0 + new_pz1
        pz0 <- new_pz0 / norm
        pz1 <- new_pz1 / norm
      }
      sampled_values[i] <- pz0 * theta0 + pz1 * theta1
    } else {
      sampled_values[i] <- runif(1)
    }
  }

  a_t <- which.max(sampled_values)
  selected_arms[t] <- a_t
  r_t <- y_true[a_t, t]
  received_rewards[t] <- r_t
  regret[t] <- y_true[oracle_arm, t] - r_t
  observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}

cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)

plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")

plot(cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```
Based on the **Cumulative Rewards**, I would say Thompson Sampling is performing robustly, selecting mostly good arms over the long run.

Based on the **Cumulative Regret vs Oracle**, we see that regret increases steadily. AT 1000, it is at 40 which is relatively small compared to the total reward (>500). 

However, regret keeps growing! I would argue that this suggests that the model still has not converged or that some of the posterior estimates might be too conservative. I guess this all has to do with the setup, that arms are in different states and we loose track of the state in which an arm is in, but please let me know what you think?

I also don't really see a point in time bases on these results, where I would say, yeah, know I can tell it is definitely performing much better.

Performance is still good, but there is room for improvement, especially in faster convergence or better identification of the best arm!






