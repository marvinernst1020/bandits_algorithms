---
title: "Formal Setup for HMM-Based Bandit Models"
author: "Marvin Ernst, Oriol Gelabert, Melisa Vadenja"
date: "`r Sys.Date()`"
output: html_document
---

If you don't have it installed yet, **you first need to download and install JAGS to your local system**, then run this:
```{r}
#install.packages("rjags")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
```

# Problem Setting

We consider a $K$-armed bandit problem where rewards are driven by latent (hidden) Markov processes. At each time $t = 1, \dots, T$, a learner selects one arm $a_t \in \{1, \dots, K\}$ and observes a binary reward $y_{a_t, t} \in \{0,1\}$.

## Two-State Setting

Each arm $i$ has a latent state $z_{i,t} \in \{0,1\}$. Rewards are modeled as:

$$y_{i,t} \mid z_{i,t} \sim \text{Bernoulli}(\theta_{i, z_{i,t}})$$

Transitions:

$$z_{i,t} \mid z_{i,t-1} \sim \text{Bernoulli}(\pi_{i, z_{i,t-1}, \cdot})$$

Initial state:

$$z_{i,1} \sim \text{Bernoulli}(\pi_i^{(0)})$$

Priors:

$$\theta_{i,0}, \theta_{i,1} \sim \text{Beta}(1,1), \quad \pi_{i,01}, \pi_{i,10} \sim \text{Beta}(1,1)$$

## More-Than-Two-State Setting

Latent states $z_{i,t} \in \{1, \dots, S\}$:

$$y_{i,t} \mid z_{i,t} \sim \text{Bernoulli}(\theta_{i, z_{i,t}})$$

Transitions:

$$z_{i,t} \mid z_{i,t-1} \sim \text{Categorical}(\pi_{i, z_{i,t-1}, \cdot})$$

Initial state:

$$z_{i,1} \sim \text{Categorical}(\pi_{i}^{(0)})$$

Priors:

$$\theta_{i,s} \sim \text{Beta}(1,1), \quad \pi_{i,l\cdot} \sim \text{Dirichlet}(1, \dots, 1)$$

# Models

## Model 1: Arm-Specific Latent Dynamics ("Poor Model")
- Each arm has its own transition matrix and latent state process.
- This allows inference to be done independently per arm.
- Implemented in `JAGS` for tractability and simplicity.

## Model 2: Global Latent State ("Advanced Model")
- All arms share a common latent state $z_t$ and a shared transition matrix.
- More complex, as inference must be joint across all arms.
- Implementing this in `JAGS` is harder due to shared state across arms; may require customization or marginalization strategies - this we will have to figure out.

# Inference and Implementation Plan

In `JAGS`, we will begin with the "poor model" because:
- It allows easier modular implementation per arm.
- It's parallelizable.
- It helps validate inference machinery and synthetic generation.

Here is how we can simulate the data with a Global Latent State Shared Across Arms:
```{r simulate-global-state}
set.seed(123)
K <- 3 # number of arms
N <- 100 # time steps

theta <- matrix(c(0.2, 0.8,
                  0.5, 0.7,
                  0.3, 0.9),
                nrow = K, byrow = TRUE)

pi_global <- matrix(c(0.9, 0.1,
                      0.1, 0.9), nrow = 2, byrow = TRUE)

z_global <- numeric(N)
z_global[1] <- rbinom(1, 1, 0.5)

for (t in 2:N) {
  z_global[t] <- rbinom(1, 1, pi_global[z_global[t - 1] + 1, 2])
}

y_global <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
  for (t in 1:N) {
    y_global[i, t] <- rbinom(1, 1, theta[i, z_global[t] + 1])
  }
}
```

However, we will first consider a setup where we will first investigate a scenario where the ground truth is that we have arm-specifc states.

---

# Sequential Thompson Sampling Setup

This section shows a simulation where we choose one arm at each time step using **Thompson Sampling**, and update the corresponding posterior only using the rewards observed so far for that arm. We are in the **Two-Sate-Setup**.

## 1. Simulate Full Ground Truth (Arm-Specific Model)

```{r simulate-truth}
set.seed(456)
K <- 3
N <- 500

theta <- matrix(c(0.2, 0.8,
                  0.5, 0.7,
                  0.3, 0.9),
                nrow = K, byrow = TRUE)

pi_indiv <- list(
  matrix(c(0.9, 0.1,
           0.1, 0.9), 2, 2, byrow = TRUE),
  matrix(c(0.85, 0.15,
           0.2, 0.8), 2, 2, byrow = TRUE),
  matrix(c(0.95, 0.05,
           0.3, 0.7), 2, 2, byrow = TRUE)
)

z_true <- matrix(0, nrow = K, ncol = N)
y_true <- matrix(0, nrow = K, ncol = N)

for (i in 1:K) {
  z_true[i, 1] <- rbinom(1, 1, 0.5)
  y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
  for (t in 2:N) {
    z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
    y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
  }
}
```

## 2. Import JAGS Model (Poor Model)

```{r define-ts-model}
model_file <- "poor_model.jags"
```

## 3. Sequential Thompson Sampling Loop with Reward-Only Observations

We sample latent states via forward filtering to compute $\theta_{i, t+1}$.

```{r thompson-sampling-loop-z-aware}
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)

# This matrix contains the probability of each arm at each time step:
theta_selected <- matrix(0, nrow = K, ncol = N)
# We are computing the arm that has the highest expected reward at each time, as the oracle:
for (i in 1:K) {
  for (t in 1:N) {
    theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
  }
}
oracle_arm <- apply(theta_selected, 2, which.max)


posterior_matrices <- rep(list(1), K)
last_states <- list()
a_t <- sample(1:K, 1) # pick first arm to pull

for (t in 1:N) {
  sampled_values <- numeric(K)
  
  if (length(observed_rewards[[a_t]]) >= 2) {
    
    data_list <- list(y = observed_rewards[[a_t]], N = length(observed_rewards[[a_t]]))
    model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
    update(model, 1000)
    
    post <- coda.samples(model, c("theta0", "theta1", "pi", paste0("z[", length(observed_rewards[[a_t]]), "]")), n.iter = 100)
    post_matrix <- as.matrix(post)
    
    posterior_matrices[[a_t]] <- post_matrix
    last_states[[a_t]] <-length(observed_rewards[[a_t]])
  }
  
  for (l in 1:K){
    
    if (!is.null(nrow(posterior_matrices[[l]])) > 0){
      idx <- sample(1:nrow(posterior_matrices[[l]]), 1)
      theta0 <- posterior_matrices[[l]][idx, "theta0"]
      theta1 <- posterior_matrices[[l]][idx, "theta1"]
      pi1 <- posterior_matrices[[l]][idx, "pi[1]"]
      pi2 <- posterior_matrices[[l]][idx, "pi[2]"]
      z_t <- posterior_matrices[[l]][idx, paste0("z[", last_states[[l]], "]")]
      sampled_values[l] <- (1-z_t)* theta0 + z_t * theta1
    }
    else {
    sampled_values[l] <- runif(1)
    }
  }
  
  a_t <- which.max(sampled_values)
  selected_arms[t] <- a_t
  r_t <- y_true[a_t, t]
  received_rewards[t] <- r_t
  # we use the expected regret:
  regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[selected_arms[t], t] + 1]
  observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}

cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)

plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")

plot(cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```
Based on the **Cumulative Rewards**, I would say Thompson Sampling is performing robustly, selecting mostly good arms over the long run.

Based on the **Cumulative Regret vs Oracle**, we see that regret increases steadily (this is the expected regret). AT 1000, it is at 80 which is relatively small compared to the total reward (>500). 

(I also don't really see a point in time bases on these results, where I would say, yeah, know I can tell it is definitely performing much better.)

Performance is still good, but there is room for improvement, especially in faster convergence or better identification of the best arm!


## 4. Posterior Recovery Check on Long Simulation

```{r}
set.seed(999)
N_long <- 10000
theta_true <- c(0.3, 0.9)
pi_true <- c(0.10, 0.95)  # pi[1] = P(0 to 1), pi[2] = P(1 to 1)

# Simulate z and y
z_check <- numeric(N_long)
y_check <- numeric(N_long)
z_check[1] <- rbinom(1, 1, pi_true[1] / (pi_true[1] + pi_true[2]))  # start in stationary dist
y_check[1] <- rbinom(1, 1, ifelse(z_check[1] == 0, theta_true[1], theta_true[2]))

for (t in 2:N_long) {
  z_check[t] <- rbinom(1, 1, ifelse(z_check[t-1] == 0, pi_true[1], pi_true[2]))
  y_check[t] <- rbinom(1, 1, ifelse(z_check[t] == 0, theta_true[1], theta_true[2]))
}

# Run JAGS
data_check <- list(y = y_check, N = N_long)
model_check <- jags.model("poor_model.jags", data = data_check, n.chains = 1, quiet = TRUE)
update(model_check, 1000)
post_check <- coda.samples(model_check, c("theta0", "theta1", "pi"), n.iter = 1000)
summary(post_check)
```



