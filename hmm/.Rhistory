for (t in 1:N) {
sampled_values <- numeric(K)
if (length(observed_rewards[[a_t]]) >= 2 && t%%Batch_size==0) {
data_list <- list(y = observed_rewards[[a_t]], N = length(observed_rewards[[a_t]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 1000)
post <- coda.samples(model, c("theta0", "theta1", "pi",paste0("z[", length(observed_rewards[[a_t]]), "]")), n.iter = 100)
post_matrix <- as.matrix(post)
posterior_matrices[[a_t]] <- post_matrix
z_last[[a_t]] <- post_matrix[, paste0("z[", length(observed_rewards[[a_t]]), "]")]
}
for (l in 1:K){
if (!is.null(nrow(posterior_matrices[[l]])) > 0){
idx <- sample(1:nrow(posterior_matrices[[l]]), 1)
theta0 <- posterior_matrices[[l]][idx, "theta0"]
theta1 <- posterior_matrices[[l]][idx, "theta1"]
pi0 <- posterior_matrices[[l]][, "pi[1]"]
pi1 <- posterior_matrices[[l]][, "pi[2]"]
sampled_pi <- ifelse(z_last[[l]] == 0, pi0, pi1)
z_sampled <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi)
z_last[[l]] <- z_sampled
sampled_theta <- ifelse(z_sampled == 0, theta0, theta1)
sampled_values[l] <- mean(sampled_theta) + sd(sampled_theta)/sqrt(length(sampled_theta))*log(t)
}
else {
sampled_values[l] <- runif(1)
}
}
a_t <- which.max(sampled_values)
selected_arms[t] <- a_t
r_t <- y_true[a_t, t]
received_rewards[t] <- r_t
regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[t] + 1]
observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrices=posterior_matrices))
}
system.time({
results <- ucb_poor(K,N,theta,y_true,z_true,Batch_size=1)
})
plot(results$cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "UCB")
plot(results$cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
for (arm in 1:K){
mean_pi_01 <- mean(results$posterior_matrices[[arm]][, "pi[1]"])
mean_pi_11 <- mean(results$posterior_matrices[[arm]][, "pi[2]"])
mean_pi_00 <- 1 - mean_pi_01
mean_pi_10 <- 1 - mean_pi_11
estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
mean_pi_10, mean_pi_11),
nrow = 2, byrow = TRUE,
dimnames = list(c("From A", "From B"),
c("To A", "To B")))
mean_theta_0 <- mean(results$posterior_matrices[[arm]][, "theta0"])
mean_theta_1 <- mean(results$posterior_matrices[[arm]][, "theta1"])
estimated_theta <- matrix(c(mean_theta_0, mean_theta_1),
nrow = 1, byrow = TRUE,
dimnames = list(NULL,c("State A", "State B")))
cat("\nEstimated Transition Matrix for arm ",arm,":\n")
print(estimated_pi)
cat("\nEstimated Reward probabilities for arm ", arm,":\n")
print(estimated_theta)
}
#We define the parameters of the simulations
system.time({
runs <- 10
Batch_size <-10
K <- 3
N <-100
theta <- matrix(c(0.2, 0.8,
0.8, 0.3,
0.5, 0.6),
nrow = K, byrow = TRUE)  # rows = arms, cols = states
pi_global <- matrix(c(0.90, 0.10,
0.05, 0.95),
nrow = 2, byrow = TRUE)  # transition matrix
reward_matrix <- matrix(0, nrow = runs, ncol = N)
regret_matrix <- matrix(0, nrow = runs, ncol = N)
for (run in 1:runs){
z_true <- numeric(N)
z_true[1] <- rbinom(1, 1, 0.5)
for (t in 2:N) {
z_true[t] <- rbinom(1, 1, pi_global[z_true[t - 1] + 1, 2])
}
# simulate rewards
y_true <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
y_true[i, t] <- rbinom(1, 1, theta[i, z_true[t] + 1])
}
}
system.time({
results <- ucb_poor(K,N,theta,y_true,z_true,Batch_size)
})
reward_matrix[run,] <- results$cumulative_reward
regret_matrix[run,] <- results$cumulative_regret
if (run%%10 == 0){cat("Finished run", run, "\n")}
}
expected_rewards_ucb_poor <- colMeans(reward_matrix)
expected_regrets_ucb_poor <- colMeans(regret_matrix)
})
plot(expected_rewards_ucb_poor, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Expected Cumulative Reward", main = "UCB")
plot(expected_regrets_ucb_poor, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Expected Cumulative Regret", main = "Expected Regret vs Oracle")
model_adv <- "advanced_model.jags"
#recover same simulation we ran before
y_true <- y_global
z_true <- z_global
K <- 3
N <- 100
thompson_advanced <- function(K,N,theta,y_true,z_true,Batch_size){
model_adv <- "advanced_model.jags"
observed_rewards <- list()
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
# Oracle computation using ground-truth global latent state:
theta_selected <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
theta_selected[i, t] <- theta[i, z_true[t] + 1]
}
}
oracle_arm <- apply(theta_selected, 2, which.max)
# Storage for posterior tracking:
post_matrix<- NULL
# Initialization:
selected_arms[1] <- sample(1:K, 1)
observed_data <- matrix(NA, nrow = K, ncol = N)
observed_data[selected_arms[1], 1] <- y_true[selected_arms[1], 1]
received_rewards[1] <- y_true[selected_arms[1], 1]
regret[1] <- theta_selected[oracle_arm[1], 1] - theta[selected_arms[1], z_true[1] + 1]
# Start Thompson Sampling loop:
for (t in 2:N) {
sampled_values <- numeric(K)
# Check if we have enough observed data:
if (sum(!is.na(observed_data)) >= K * 2 && t%%Batch_size==0) {
data_list <- list(
y_obs = observed_data[, 1:(t-1)],
K = K,
N = t - 1
)
model <- jags.model(model_adv, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 1000)
post <- coda.samples(model, c("theta", "pi", paste0("z[", t - 1, "]")), n.iter = 100)
post_matrix <- as.matrix(post)
}
if (is.matrix(post_matrix)) {
idx <- sample(1:nrow(post_matrix), 1)
pi0 <- post_matrix[idx,"pi[1]"]
pi1 <- post_matrix[idx,"pi[2]"]
if (t%%Batch_size==0) { z_last <- post_matrix[idx, paste0("z[", t-1, "]")]}
sampled_pi <- ifelse(z_last == 0, pi0, pi1)
z_t <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi)
z_last <- z_t
}
# Arm selection:
for (i in 1:K) {
if (is.matrix(post_matrix)) {
theta0 <- post_matrix[idx, paste0("theta[", i, ",1]")]
theta1 <- post_matrix[idx, paste0("theta[", i, ",2]")]
sampled_values[i] <- (1 - z_t) * theta0 + z_t * theta1
} else {
sampled_values[i] <- runif(1)
}
}
selected_arms[t] <- which.max(sampled_values)
r_t <- y_true[selected_arms[t], t]
received_rewards[t] <- r_t
regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[t] + 1]
observed_data[selected_arms[t], t] <- r_t
}
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrix=post_matrix))
}
system.time({
results<- thompson_advanced (K,N,theta,y_true,z_true,Batch_size=1)
})
plot(results$cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling (Advanced Model)")
plot(results$cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
last_posterior <- results$posterior_matrix
if (all(c("pi[1]", "pi[2]") %in% colnames(last_posterior))) {
mean_pi_01 <- mean(last_posterior[, "pi[1]"])
mean_pi_11 <- mean(last_posterior[, "pi[2]"])
mean_pi_00 <- 1 - mean_pi_01
mean_pi_10 <- 1 - mean_pi_11
estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
mean_pi_10, mean_pi_11),
nrow = 2, byrow = TRUE,
dimnames = list(c("From A", "From B"),
c("To A", "To B")))
mean_theta_01 <- mean(last_posterior[, "theta[1,1]"])
mean_theta_11 <- mean(last_posterior[, "theta[1,2]"])
mean_theta_02 <- mean(last_posterior[, "theta[2,1]"])
mean_theta_12 <- mean(last_posterior[, "theta[2,2]"])
mean_theta_03 <- mean(last_posterior[, "theta[3,1]"])
mean_theta_13 <- mean(last_posterior[, "theta[3,2]"])
estimated_theta <- matrix(c(mean_theta_01, mean_theta_11,
mean_theta_02,mean_theta_12,
mean_theta_03,mean_theta_13),
nrow = 3, byrow = TRUE,
dimnames = list(c("Arm 1", "Arm 2", "Arm 3"),
c("State A", "State B")))
cat("\nEstimated Common Transition Matrix:\n")
print(estimated_pi)
cat("\nEstimated Reward probabilities:\n")
print(estimated_theta)
} else {
cat("Transition probabilities pi[1] and pi[2] not found in posterior samples.\n")
}
#We define the parameters of the simulations
runs <- 10
Batch_size <-10
K <- 3
N <-100
theta <- matrix(c(0.2, 0.8,
0.8, 0.3,
0.5, 0.6),
nrow = K, byrow = TRUE)  # rows = arms, cols = states
pi_global <- matrix(c(0.90, 0.10,
0.05, 0.95),
nrow = 2, byrow = TRUE)  # transition matrix
reward_matrix <- matrix(0, nrow = runs, ncol = N)
regret_matrix <- matrix(0, nrow = runs, ncol = N)
for (run in 1:runs){
z_true <- numeric(N)
z_true[1] <- rbinom(1, 1, 0.5)
for (t in 2:N) {
z_true[t] <- rbinom(1, 1, pi_global[z_true[t - 1] + 1, 2])
}
# simulate rewards
y_true <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
y_true[i, t] <- rbinom(1, 1, theta[i, z_true[t] + 1])
}
}
results <- thompson_advanced(K,N,theta,y_true,z_true,Batch_size)
reward_matrix[run,] <- results$cumulative_reward
regret_matrix[run,] <- results$cumulative_regret
if (run%%10 == 0){cat("Finished run", run, "\n")}
}
expected_rewards_thompson_advanced <- colMeans(reward_matrix)
expected_regrets_thompson_advanced <- colMeans(regret_matrix)
plot(expected_rewards_thompson_advanced, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Expected Cumulative Reward", main = "Thompson Sampling (Advanced Model)")
plot(expected_regrets_thompson_advanced, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Expected Cumulative Regret", main = "Expected Regret vs Oracle")
#recover same simulation we ran before
y_true <- y_global
z_true <- z_global
K <- 3
N <- 100
ucb_advanced <- function(K,N,theta,y_true,z_true,Batch_size){
model_adv <- "advanced_model.jags"
observed_rewards <- list()
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
# Oracle computation using ground-truth global latent state:
theta_selected <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
theta_selected[i, t] <- theta[i, z_true[t] + 1]
}
}
oracle_arm <- apply(theta_selected, 2, which.max)
# Storage for posterior tracking:
post_matrix<- NULL
# Initialization:
selected_arms[1] <- sample(1:K, 1)
observed_data <- matrix(NA, nrow = K, ncol = N)
observed_data[selected_arms[1], 1] <- y_true[selected_arms[1], 1]
received_rewards[1] <- y_true[selected_arms[1], 1]
regret[1] <- theta_selected[oracle_arm[1], 1] - theta[selected_arms[1], z_true[1] + 1]
# Start Thompson Sampling loop:
for (t in 2:N) {
sampled_values <- numeric(K)
# Check if we have enough observed data:
if (sum(!is.na(observed_data)) >= K * 2  && t%%Batch_size == 0) {
data_list <- list(
y_obs = observed_data[, 1:(t-1)],
K = K,
N = t - 1
)
model <- jags.model(model_adv, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 1000)
post <- coda.samples(model, c("theta", "pi", paste0("z[", t - 1, "]")), n.iter = 100)
post_matrix <- as.matrix(post)
}
if (is.matrix(post_matrix)) {
pi0 <- post_matrix[,"pi[1]"]
pi1 <- post_matrix[,"pi[2]"]
if (t%%Batch_size==0) { z_last <- post_matrix[, paste0("z[", t-1, "]")]}
sampled_pi <- ifelse(z_last == 0, pi0, pi1)
z_t <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi)
z_last <- z_t
}
# Arm selection:
for (i in 1:K) {
if (is.matrix(post_matrix)) {
theta0 <- post_matrix[, paste0("theta[", i, ",1]")]
theta1 <- post_matrix[, paste0("theta[", i, ",2]")]
sampled_theta <- ifelse(z_t == 0, theta0, theta1)
sampled_values[i] <- mean(sampled_theta) + sd(sampled_theta)/sqrt(length(sampled_theta))*log(t)
} else {
sampled_values[i] <- runif(1)
}
}
selected_arms[t] <- which.max(sampled_values)
r_t <- y_true[selected_arms[t], t]
received_rewards[t] <- r_t
regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[t] + 1]
observed_data[selected_arms[t], t] <- r_t
}
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrix=post_matrix))
}
system.time({
results <- ucb_advanced(K,N,theta,y_true,z_true,Batch_size=1)
})
plot(results$cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "UCB (Advanced Model)")
plot(results$cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
last_posterior <- results$posterior_matrix
if (all(c("pi[1]", "pi[2]") %in% colnames(last_posterior))) {
mean_pi_01 <- mean(last_posterior[, "pi[1]"])
mean_pi_11 <- mean(last_posterior[, "pi[2]"])
mean_pi_00 <- 1 - mean_pi_01
mean_pi_10 <- 1 - mean_pi_11
estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
mean_pi_10, mean_pi_11),
nrow = 2, byrow = TRUE,
dimnames = list(c("From A", "From B"),
c("To A", "To B")))
mean_theta_01 <- mean(last_posterior[, "theta[1,1]"])
mean_theta_11 <- mean(last_posterior[, "theta[1,2]"])
mean_theta_02 <- mean(last_posterior[, "theta[2,1]"])
mean_theta_12 <- mean(last_posterior[, "theta[2,2]"])
mean_theta_03 <- mean(last_posterior[, "theta[3,1]"])
mean_theta_13 <- mean(last_posterior[, "theta[3,2]"])
estimated_theta <- matrix(c(mean_theta_01, mean_theta_11,
mean_theta_02,mean_theta_12,
mean_theta_03,mean_theta_13),
nrow = 3, byrow = TRUE,
dimnames = list(c("Arm 1", "Arm 2", "Arm 3"),
c("State A", "State B")))
cat("\nEstimated Common Transition Matrix:\n")
print(estimated_pi)
cat("\nEstimated Reward probabilities:\n")
print(estimated_theta)
} else {
cat("Transition probabilities pi[1] and pi[2] not found in posterior samples.\n")
}
#We define the parameters of the simulations
runs <- 10
Batch_size <-10
K <- 3
N <-100
theta <- matrix(c(0.2, 0.8,
0.8, 0.3,
0.5, 0.6),
nrow = K, byrow = TRUE)  # rows = arms, cols = states
pi_global <- matrix(c(0.90, 0.10,
0.05, 0.95),
nrow = 2, byrow = TRUE)  # transition matrix
reward_matrix <- matrix(0, nrow = runs, ncol = N)
regret_matrix <- matrix(0, nrow = runs, ncol = N)
for (run in 1:runs){
z_true <- numeric(N)
z_true[1] <- rbinom(1, 1, 0.5)
for (t in 2:N) {
z_true[t] <- rbinom(1, 1, pi_global[z_true[t - 1] + 1, 2])
}
# simulate rewards
y_true <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
y_true[i, t] <- rbinom(1, 1, theta[i, z_true[t] + 1])
}
}
results <- ucb_advanced(K,N,theta,y_true,z_true,Batch_size)
reward_matrix[run,] <- results$cumulative_reward
regret_matrix[run,] <- results$cumulative_regret
if (run%%10 == 0){cat("Finished run", run, "\n")}
}
expected_rewards_ucb_advanced <- colMeans(reward_matrix)
expected_regrets_ucb_advanced <- colMeans(regret_matrix)
plot(expected_rewards_ucb_advanced, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Expected Cumulative Reward", main = "UCB (Advanced Model)")
plot(expected_regrets_ucb_advanced, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Expected Cumulative Regret", main = "Expected Regret vs Oracle")
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
library(here)
install.packages('here')
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
library(here)
library(future.apply)
install.packages('future')
library(future.apply)
install.packages('future.apply')
library(future.apply)
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
library(here)
library(future.apply)
plan(multisession)
source(here("src/load_all.R"))
set.seed(123)
K <- 2
N <- 5000
pi_global <- matrix(c(0.999, 0.001,
0.001, 0.999),
nrow = 2, byrow = TRUE)
mu <- matrix(c(0.1, 0.95,
0.95, 0.1),
nrow = K, byrow = TRUE)
generate_global_datasets(
K = K,
N = N,
mu = mu,
pi_global = pi_global,
n_runs = 1,
scenario_name = "single_run",
root_path = "data_global"
)
truth <- readRDS("data_global/single_run/global_truth_1.rds")
y_global <- truth$y
z_global <- truth$z
qplot(1:N, z_global, geom = "line") +
labs(title = "Latent Global State Over Time",
x = "Time", y = "State")
y_df <- as.data.frame(y_global)
y_df <- y_df |> mutate(arm = factor(1:K)) |>
pivot_longer(cols = starts_with("V"), names_to = "time", values_to = "reward") |>
mutate(time = as.numeric(gsub("V", "", time)))
ggplot(y_df, aes(x = time, y = reward, color = arm)) +
geom_line() +
facet_wrap(~ arm, ncol = 1) +
labs(title = "Reward Streams Per Arm",
x = "Time", y = "Reward")
baseline_ts_results <- bandit_baselines("ts",
K, N, y_global, z_global, mu,
dynamics = "common",
batch_size = 100)
baseline_ts_results_df <- data.frame(
time = seq_along(baseline_ts_results$cumulative_reward),
cumulative_reward = baseline_ts_results$cumulative_reward,
cumulative_regret = baseline_ts_results$cumulative_regret,
model = "Baseline TS"
)
system.time({
res_adv_ts_common <- thompson_advanced(K, N, mu, y_global, z_global,
batch_size = 100,
burn = 500, n_iter = 100,
dynamics = "common")
})
adv_ts_results_df <- data.frame(
time = seq_along(res_adv_ts_common$cumulative_reward),
cumulative_reward = res_adv_ts_common$cumulative_reward,
cumulative_regret = res_adv_ts_common$cumulative_regret,
model = "Advanced TS"
)
ts_compare_df_s1 <- bind_rows(baseline_ts_results_df, adv_ts_results_df)
ggplot(ts_compare_df_s1, aes(x = time, y = cumulative_reward, color = model)) +
geom_line(size = 1) +
labs(title = "Cumulative Reward: Baseline vs Advanced Thompson Sampling",
x = "Time",
y = "Cumulative Reward") +
theme_minimal()
ggplot(ts_compare_df_s1, aes(x = time, y = cumulative_regret, color = model)) +
geom_line(size = 1) +
labs(title = "Cumulative Regret: Baseline vs Advanced Thompson Sampling",
x = "Time",
y = "Cumulative Regret") +
theme_minimal()
baseline_ucb_results <- bandit_baselines(
algorithm='ucb-tuned',
K, N, y_global, z_global, mu,
dynamics = "common",
batch_size = 100
)
baseline_ucb_results_df <- data.frame(
time = seq_along(baseline_ucb_results$cumulative_reward),
cumulative_reward = baseline_ucb_results$cumulative_reward,
cumulative_regret = baseline_ucb_results$cumulative_regret,
model = "Baseline UCB"
)
system.time({
res_adv_ucb_common <- ucb_advanced(
K, N, mu, y_global, z_global,
batch_size = 100,
dynamics = "common"
)
})
