---
title: "Formal Setup for HMM-Based Bandit Models"
author: "Marvin Ernst, Oriol Gelabert, Melisa Vadenja"
date: "`r Sys.Date()`"
output: html_document
---

If you don't have it installed yet, **you first need to download and install JAGS to your local system**, then run this:
```{r}
install.packages("rjags")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
```

# Problem Setting

We consider a $K$-armed bandit problem where rewards are driven by latent (hidden) Markov processes. At each time $t = 1, \dots, T$, a learner selects one arm $a_t \in \{1, \dots, K\}$ and observes a binary reward $y_{a_t, t} \in \{0,1\}$.

## Two-State Setting

Each arm $i$ has a latent state $z_{i,t} \in \{0,1\}$. Rewards are modeled as:

$$y_{i,t} \mid z_{i,t} \sim \text{Bernoulli}(\theta_{i, z_{i,t}})$$

Transitions:

$$z_{i,t} \mid z_{i,t-1} \sim \text{Bernoulli}(\pi_{i, z_{i,t-1}, \cdot})$$

Initial state:

$$z_{i,1} \sim \text{Bernoulli}(\pi_i^{(0)})$$

Priors:

$$\theta_{i,0}, \theta_{i,1} \sim \text{Beta}(1,1), \quad \pi_{i,01}, \pi_{i,10} \sim \text{Beta}(1,1)$$

## More-Than-Two-State Setting

Latent states $z_{i,t} \in \{1, \dots, S\}$:

$$y_{i,t} \mid z_{i,t} \sim \text{Bernoulli}(\theta_{i, z_{i,t}})$$

Transitions:

$$z_{i,t} \mid z_{i,t-1} \sim \text{Categorical}(\pi_{i, z_{i,t-1}, \cdot})$$

Initial state:

$$z_{i,1} \sim \text{Categorical}(\pi_{i}^{(0)})$$

Priors:

$$\theta_{i,s} \sim \text{Beta}(1,1), \quad \pi_{i,l\cdot} \sim \text{Dirichlet}(1, \dots, 1)$$

# Models

## Model 1: Arm-Specific Latent Dynamics ("Poor Model")
- Each arm has its own transition matrix and latent state process.
- This allows inference to be done independently per arm.
- Implemented in `JAGS` for tractability and simplicity.

## Model 2: Global Latent State ("Advanced Model")
- All arms share a common latent state $z_t$ and a shared transition matrix.
- More complex, as inference must be joint across all arms.
- Implementing this in `JAGS` is harder due to shared state across arms; may require customization or marginalization strategies - this we will have to figure out.

# Inference and Implementation Plan

In `JAGS`, we will begin with the "poor model" because:
- It allows easier modular implementation per arm.
- It's parallelizable.
- It helps validate inference machinery and synthetic generation.

Here is how we can simulate the data with a Global Latent State Shared Across Arms:
```{r simulate-global-state}
set.seed(123)
K <- 3 # number of arms
N <- 100 # time steps

theta <- matrix(c(0.2, 0.8,
                  0.5, 0.7,
                  0.3, 0.9),
                nrow = K, byrow = TRUE)

pi_global <- matrix(c(0.9, 0.1,
                      0.1, 0.9), nrow = 2, byrow = TRUE)

z_global <- numeric(N)
z_global[1] <- rbinom(1, 1, 0.5)

for (t in 2:N) {
  z_global[t] <- rbinom(1, 1, pi_global[z_global[t - 1] + 1, 2])
}

y_global <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
  for (t in 1:N) {
    y_global[i, t] <- rbinom(1, 1, theta[i, z_global[t] + 1])
  }
}
```

However, we will first consider a setup where we will first investigate a scenario where the ground truth is that we have arm-specifc states.

---

# Sequential Thompson Sampling Setup

This section shows a simulation where we choose one arm at each time step using **Thompson Sampling**, and update the corresponding posterior only using the rewards observed so far for that arm. We are in the **Two-Sate-Setup**.

## 1. Simulate Full Ground Truth (Arm-Specific Model)

```{r simulate-truth}
set.seed(456)
K <- 3
N <- 500

theta <- matrix(c(0.2, 0.8,
                  0.5, 0.7,
                  0.3, 0.9),
                nrow = K, byrow = TRUE)

pi_indiv <- list(
  matrix(c(0.9, 0.1,
           0.1, 0.9), 2, 2, byrow = TRUE),
  matrix(c(0.85, 0.15,
           0.2, 0.8), 2, 2, byrow = TRUE),
  matrix(c(0.95, 0.05,
           0.3, 0.7), 2, 2, byrow = TRUE)
)

z_true <- matrix(0, nrow = K, ncol = N)
y_true <- matrix(0, nrow = K, ncol = N)

for (i in 1:K) {
  z_true[i, 1] <- rbinom(1, 1, 0.5)
  y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
  for (t in 2:N) {
    z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
    y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
  }
}
```

## 2. Import JAGS Model (Poor Model)

```{r define-ts-model}
model_file <- "poor_model.jags"
```

## 3. Sequential Thompson Sampling Loop with Reward-Only Observations

We sample latent states via forward filtering to compute $\theta_{i, t+1}$.

```{r thompson-sampling-loop-z-aware}
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)

# This matrix contains the probability of each arm at each time step:
theta_selected <- matrix(0, nrow = K, ncol = N)
# We are computing the arm that has the highest expected reward at each time, as the oracle:
for (i in 1:K) {
  for (t in 1:N) {
    theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
  }
}
oracle_arm <- apply(theta_selected, 2, which.max)


posterior_matrices <- rep(list(1), K)
last_states <- list()
a_t <- sample(1:K, 1) # pick first arm to pull

for (t in 1:N) {
  sampled_values <- numeric(K)
  
  if (length(observed_rewards[[a_t]]) >= 2) {
    
    data_list <- list(y = observed_rewards[[a_t]], N = length(observed_rewards[[a_t]]))
    model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
    update(model, 1000)
    
    post <- coda.samples(model, c("theta0", "theta1", "pi", paste0("z[", length(observed_rewards[[a_t]]), "]")), n.iter = 100)
    post_matrix <- as.matrix(post)
    
    posterior_matrices[[a_t]] <- post_matrix
    last_states[[a_t]] <-length(observed_rewards[[a_t]])
  }
  
  for (l in 1:K){
    
    if (!is.null(nrow(posterior_matrices[[l]])) > 0){
      idx <- sample(1:nrow(posterior_matrices[[l]]), 1)
      theta0 <- posterior_matrices[[l]][idx, "theta0"]
      theta1 <- posterior_matrices[[l]][idx, "theta1"]
      pi1 <- posterior_matrices[[l]][idx, "pi[1]"]
      pi2 <- posterior_matrices[[l]][idx, "pi[2]"]
      z_t <- posterior_matrices[[l]][idx, paste0("z[", last_states[[l]], "]")]
      sampled_values[l] <- (1-z_t)* theta0 + z_t * theta1
    }
    else {
    sampled_values[l] <- runif(1)
    }
  }
  
  a_t <- which.max(sampled_values)
  selected_arms[t] <- a_t
  r_t <- y_true[a_t, t]
  received_rewards[t] <- r_t
  # we use the expected regret:
  regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[selected_arms[t], t] + 1]
  observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}

cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)

plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")

plot(cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```
Based on the **Cumulative Rewards**, I would say Thompson Sampling is performing robustly, selecting mostly good arms over the long run.

Based on the **Cumulative Regret vs Oracle**, we see that regret increases steadily (this is the expected regret). AT 1000, it is at 80 which is relatively small compared to the total reward (>500). 

(I also don't really see a point in time bases on these results, where I would say, yeah, know I can tell it is definitely performing much better.)

Performance is still good, but there is room for improvement, especially in faster convergence or better identification of the best arm!


## 4. Posterior Recovery Check on Long Simulation

```{r}
set.seed(999)
N_long <- 10000
theta_true <- c(0.3, 0.9)
pi_true <- c(0.10, 0.95)  # pi[1] = P(0 to 1), pi[2] = P(1 to 1)

# Simulate z and y
z_check <- numeric(N_long)
y_check <- numeric(N_long)
z_check[1] <- rbinom(1, 1, pi_true[1] / (pi_true[1] + pi_true[2]))  # start in stationary dist
y_check[1] <- rbinom(1, 1, ifelse(z_check[1] == 0, theta_true[1], theta_true[2]))

for (t in 2:N_long) {
  z_check[t] <- rbinom(1, 1, ifelse(z_check[t-1] == 0, pi_true[1], pi_true[2]))
  y_check[t] <- rbinom(1, 1, ifelse(z_check[t] == 0, theta_true[1], theta_true[2]))
}

# Run JAGS
data_check <- list(y = y_check, N = N_long)
model_check <- jags.model("poor_model.jags", data = data_check, n.chains = 1, quiet = TRUE)
update(model_check, 1000)
post_check <- coda.samples(model_check, c("theta0", "theta1", "pi"), n.iter = 1000)
summary(post_check)
```

# Setting up UCB

UCB1 assigns each arm an “upper confidence bound” on its expected reward, and then always picks the arm with the highest bound. Concretely, for arm i after **n_i** pulls and cumulative reward **S_i**, the empirical mean is $$\hat\mu_i = \frac{S_i}{n_i}$$. UCB1 adds another term that grows when **n_i** is small, namely
$$\text{UCB}_i(t) = \hat\mu_i \;+\; \sqrt{\frac{2\ln t}{n_i}}$$. So intuitively, the $$\sqrt{2\ln t/n_i}$$ term is large when **n_i** is small (forcing exploration), and decays as **n_i** grows.As t increases, the **ln_t** term nudges you to revisit arms occasionally to maintain confidence.


```{r}
set.seed(456)
# here we are setting the parameters: 
K <- 3 #for 3 arms
N <- 500 # for 500 time steps

# this is our matrix of true thetas and filled by row for the 3 arms.
theta <- matrix(c(0.2, 0.8,
                  0.5, 0.7,
                  0.3, 0.9),
                nrow = K, byrow = TRUE)

# these are our matrices of transition probabilities for each arm.
pi_indiv <- list(
  matrix(c(0.9, 0.1,
           0.1, 0.9), 2, 2, byrow = TRUE),
  matrix(c(0.85, 0.15,
           0.2, 0.8), 2, 2, byrow = TRUE),
  matrix(c(0.95, 0.05,
           0.3, 0.7), 2, 2, byrow = TRUE)
)


# simulate latent states z_true and observed rewards y_true
# we initialize our matrices of K by N dimensions of as matrices of 0s.
# zs are our states while ys are our observed rewards.
z_true <- matrix(0, K, N)
y_true <- matrix(0, K, N)



#for each arm we sample z from a Bernoulli and also y forma Bernoulli dependent on theta 
for(i in 1:K){
  z_true[i,1] <- rbinom(1,1,0.5) # the structure of this is nr of draws,bin,success probability.
  y_true[i,1] <- rbinom(1,1, theta[i, z_true[i,1]+1]) # i do it this way so that when its z is 0 it picks column 1 but for 1 picks col 2
  for(t in 2:N){ #then per iteration, I'm sampling 
    z_true[i,t] <- rbinom(1,1, pi_indiv[[i]][ z_true[i,t-1]+1 , 2 ]) # the probability of transitioning into state 1 (column 2) from whatever your last state was. its coompl
    y_true[i,t] <- rbinom(1,1, theta[i, z_true[i,t]+1 ])
  }
}

# FIX the var?
theta_selected <- matrix(0, nrow = K, ncol = N)
for(i in 1:K){
  for(t in 1:N){
    theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
  }
}

# for each arm select the index of the arm where theta selected is the largest.
oracle_arm <- apply(theta_selected, 2, which.max)


```

Now we move on to initalizing the algorithm for this after having set the model.

```{r}
# Initializing algorithm structure 
counts       <- rep(0, K)   # storing number of times each arm has been pulled
sum_rewards  <- rep(0, K)   # cumulative reward per arm
selected_arms_ucb    <- integer(N) # which arm is picked at time t
received_rewards_ucb <- numeric(N) # reward at time t
regret_ucb           <- numeric(N) # regret at time t

# Initialization: play each arm once
for(t in 1:K){
  arm <- t
  r   <- y_true[arm, t]
  counts[arm]      <- 1
  sum_rewards[arm] <- r
  selected_arms_ucb[t]    <- arm
  received_rewards_ucb[t] <- r
  regret_ucb[t] <- theta_selected[oracle_arm[t], t] -
                   theta_selected[arm, t] # best arm - selected arm
}

# to be repeaated 500 tims
for(t in (K+1):N){
  ucb_values <- numeric(K)
  for(i in 1:K){
    mu_hat <- sum_rewards[i] / counts[i] #nvr /0 bcs we initialized as 1
    bonus  <- sqrt(2 * log(t) / counts[i])
    ucb_values[i] <- mu_hat + bonus
  }
  arm <- which.max(ucb_values)
  r   <- y_true[arm, t]
  
  counts[arm]      <- counts[arm] + 1 #adding count after the arm chosen again
  sum_rewards[arm] <- sum_rewards[arm] + r #ading reward after arm chosen again
  
  selected_arms_ucb[t]    <- arm
  received_rewards_ucb[t] <- r
  regret_ucb[t] <- theta_selected[oracle_arm[t], t] -
                   theta_selected[arm, t]
}

#Evaluate
cumu_reward_ucb <- cumsum(received_rewards_ucb)
cumu_regret_ucb <- cumsum(regret_ucb)


par(mfrow=c(1,2))
plot(cumu_reward_ucb, type="l", lwd=2,
     xlab="Time", ylab="Cumulative Reward",
     main="UCB: Cumulative Reward")
plot(cumu_regret_ucb, type="l", lwd=2, col="red",
     xlab="Time", ylab="Cumulative Regret",
     main="UCB: Regret vs Oracle")
```
UCB’s growing reward line is basically straight bcs it quickly finds good arms and the roughly linear regret (≈80 by 500 pulls) simply reflects that in a changing (hidden‐Markov) environment you can never stop exploring.


```{r}
library(rjags)

K       <- 2        # number of arms
T_max   <- 200      # total pulls (increase later)
burn_in <- 1000     # JAGS burn-in
n_samps <-  500     # posterior draws per arm per round

# true params for simulating rewards
theta_true <- c(0.3, 0.9)
pi_true    <- c(0.10, 0.95)

#pre simulate 
sim_z <- matrix(0, nrow=K, ncol=T_max)
sim_y <- matrix(0, nrow=K, ncol=T_max)
for(i in 1:K){
  rho0       <- pi_true[2]/sum(pi_true)
  sim_z[i,1] <- rbinom(1,1,1-rho0)
  sim_y[i,1] <- rbinom(1,1,theta_true[ sim_z[i,1]+1 ])
  for(t in 2:T_max){
    p01        <- ifelse(sim_z[i,t-1]==0, pi_true[1], pi_true[2])
    sim_z[i,t] <- rbinom(1,1,p01)
    sim_y[i,t] <- rbinom(1,1,theta_true[ sim_z[i,t]+1 ])
  }
}

hist_y      <- vector("list", K)
for(i in 1:K) hist_y[[i]] <- integer(0)

posteriors  <- vector("list", K)
for(i in 1:K) posteriors[[i]] <- vector("list", T_max)

chosen_arm  <- integer(T_max)
rewards     <- integer(T_max)

for(t in 1:T_max){
  ucb_index <- numeric(K)

  for(i in 1:K){
    n_i <- length(hist_y[[i]])

    if(n_i == 0){
      # never pulled → force one pull
      ucb_index[i] <- Inf

    } else {
      dat <- list(y = hist_y[[i]], N = n_i)
      jm  <- jags.model("poor_model.jags",
                        data     = dat,
                        n.chains = 2, quiet=TRUE)
      update(jm, burn_in)

      # sample theta0, theta1, z[n_i]
      vars <- c("theta0","theta1", paste0("z[", n_i, "]"))
      post <- coda.samples(jm, variable.names=vars, n.iter=n_samps)
      M    <- as.matrix(post)  # dims = (n_chains*n_samps) × 3

      posteriors[[i]][[t]] <- M

      theta0 <- M[,"theta0"]
      theta1 <- M[,"theta1"]
      zN      <- M[,paste0("z[",n_i,"]")]

      # compound-theta draws
      tilde   <- ifelse(zN==0, theta0, theta1)

      # new UCB = mean + (sd/sqrt(n_i)) * ln(t)
      ucb_index[i] <- mean(tilde) +
                      ( sd(tilde)/sqrt(n_i) ) * log(t)
    }
  }

  # pick best arm, observe reward, update history
  arm_t          <- which.max(ucb_index)
  chosen_arm[t]  <- arm_t
  r_t            <- sim_y[arm_t, t]
  rewards[t]     <- r_t
  hist_y[[arm_t]]<- c(hist_y[[arm_t]], r_t)
}

cat("Total reward =", sum(rewards), "out of", T_max, "\n")
for(i in 1:K){
  cat(sprintf("Arm %d – pulls: %3d  mean reward: %.3f\n",
              i, length(hist_y[[i]]), mean(hist_y[[i]])))
}
```


```{r}
for(i in 1:K){
  Ni <- length(hist_y[[i]])
  cat("\n= Arm", i, "JAGS fit; N =", Ni, "=\n")
  data_i <- list(y = hist_y[[i]], N = Ni)

  jm_i <- jags.model(file     = "poor_model.jags",
                     data     = data_i,
                     n.chains = 2, quiet = FALSE)
  
  update(jm_i, 2000)

  post_i <- coda.samples(model         = jm_i,
                         variable.names= c("pi[1]","pi[2]",
                                           "theta0","theta1"),
                         n.iter         = 1000)
  print(summary(post_i))
}
```
```{r}
df <- data.frame(
  t = 1:T_max,
  cum_reward = cumsum(rewards)
)
ggplot(df, aes(x=t, y=cum_reward)) +
  geom_line() +
  labs(title="Cumulative Reward Over Time",
       x="Round", y="Cumulative successes") +
  theme_minimal()
```


