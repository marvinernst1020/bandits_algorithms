---
title: "Global State Bandit Simulation"
author: "Marvin Ernst, Oriol Gelabert, Melisa Vadenja"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
```

# Problem Setup: Global Latent State

We simulate a $K$-armed bandit problem where **all arms share the same underlying latent state process** governed by a Hidden Markov Model (HMM). This represents a scenario where there's a common environment or external condition that affects the reward probability of all arms.

# 1. Generating Ground Truth Data for Global State

## 1.1 Simulate Ground Truth: 

```{r simulate-global-state}
set.seed(123)
K <- 3 # number of arms
N <- 500 # number of time steps

# true parameters
theta <- matrix(c(0.2, 0.8,
                  0.5, 0.7,
                  0.3, 0.9),
                nrow = K, byrow = TRUE)  # rows = arms, cols = states

pi_global <- matrix(c(0.90, 0.10,
                      0.05, 0.95),
                    nrow = 2, byrow = TRUE)  # transition matrix

# simulate latent states (common to all arms)
z_global <- numeric(N)
z_global[1] <- rbinom(1, 1, 0.5)
for (t in 2:N) {
  z_global[t] <- rbinom(1, 1, pi_global[z_global[t - 1] + 1, 2])
}

# simulate rewards
y_global <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
  for (t in 1:N) {
    y_global[i, t] <- rbinom(1, 1, theta[i, z_global[t] + 1])
  }
}
```

# 1.2 Visualize Latent States

```{r visualize-state}
qplot(1:N, z_global, geom = "line") +
  labs(title = "Latent Global State Over Time",
       x = "Time", y = "State")
```

## 1.3 Visualize Rewards Per Arm

```{r visualize-rewards}
y_df <- as.data.frame(y_global)
y_df <- y_df |> mutate(arm = factor(1:K)) |> 
  pivot_longer(cols = starts_with("V"), names_to = "time", values_to = "reward") |>
  mutate(time = as.numeric(gsub("V", "", time)))

ggplot(y_df, aes(x = time, y = reward, color = arm)) +
  geom_line() +
  facet_wrap(~ arm, ncol = 1) +
  labs(title = "Reward Streams Per Arm",
       x = "Time", y = "Reward")
```

## 1.4 Save Ground Truth for Inference

```{r save-data}
y_true <- y_global
z_true <- z_global
saveRDS(list(y = y_true, z = z_true, theta = theta, pi = pi_global), "global_truth_data.rds")
```

# 2. Poor Model - Thompson Sampling

## 2.1 Import JAGS Model

```{r define-ts-model}
model_file <- "poor_model.jags"
```

## 2.2 Sequential Thompson Sampling Loop with Reward-Only Observations

We sample latent states via forward filtering to compute \(\theta_{i, t+1}\).

```{r thompson-sampling-loop-z-aware}
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)

theta_selected <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
  for (t in 1:N) {
    theta_selected[i, t] <- theta[i, z_true[t] + 1]  # shared global state
  }
}
oracle_arm <- apply(theta_selected, 2, which.max)

posterior_matrices <- rep(list(1), K)
last_states <- list()
a_t <- sample(1:K, 1)  # pick first arm to pull

for (t in 1:N) {
  sampled_values <- numeric(K)

  if (length(observed_rewards[[a_t]]) >= 2) {
    data_list <- list(y = observed_rewards[[a_t]], N = length(observed_rewards[[a_t]]))
    model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
    update(model, 1000)

    post <- coda.samples(model, c("theta0", "theta1", "pi", paste0("z[", length(observed_rewards[[a_t]]), "]")), n.iter = 100)
    post_matrix <- as.matrix(post)

    posterior_matrices[[a_t]] <- post_matrix
    last_states[[a_t]] <- length(observed_rewards[[a_t]])
  }

  for (l in 1:K) {
    if (!is.null(nrow(posterior_matrices[[l]])) > 0) {
      idx <- sample(1:nrow(posterior_matrices[[l]]), 1)
      theta0 <- posterior_matrices[[l]][idx, "theta0"]
      theta1 <- posterior_matrices[[l]][idx, "theta1"]
      pi1 <- posterior_matrices[[l]][idx, "pi[1]"]
      pi2 <- posterior_matrices[[l]][idx, "pi[2]"]
      z_t <- posterior_matrices[[l]][idx, paste0("z[", last_states[[l]], "]")]
      sampled_values[l] <- (1 - z_t) * theta0 + z_t * theta1
    } else {
      sampled_values[l] <- runif(1)
    }
  }

  a_t <- which.max(sampled_values)
  selected_arms[t] <- a_t
  r_t <- y_true[a_t, t]
  received_rewards[t] <- r_t
  regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[t] + 1]
  observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}

# Estimate transition matrices for each arm from the final posterior:
estimated_pi_per_arm <- vector("list", K)

for (i in 1:K) {
  if (!is.null(dim(posterior_matrices[[i]]))) {
    post_mat <- posterior_matrices[[i]]
    mean_pi_01 <- mean(post_mat[, "pi[1]"])
    mean_pi_11 <- mean(post_mat[, "pi[2]"])
    mean_pi_00 <- 1 - mean_pi_01
    mean_pi_10 <- 1 - mean_pi_11

    estimated_pi_per_arm[[i]] <- matrix(c(mean_pi_00, mean_pi_01,
                                          mean_pi_10, mean_pi_11),
                                        nrow = 2, byrow = TRUE,
                                        dimnames = list(c("From 0", "From 1"),
                                                        c("To 0", "To 1")))
  } else {
    estimated_pi_per_arm[[i]] <- NA
  }
}

# Print estimated transition matrices:
for (i in 1:K) {
  cat(paste0("\nEstimated Transition Matrix for Arm ", i, ":\n"))
  print(estimated_pi_per_arm[[i]])
}

# All the output we return:
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)

plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")

plot(cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```

Posterior Means of the Estimated Transition Probabilities:
```{r}
for (i in 1:K) {
  cat(paste0("\nEstimated Transition Matrix for Arm ", i, ":\n"))
  print(estimated_pi_per_arm[[i]])
}
```


# 3. Poor Model - UCB


# 4. Advanced Model - TS

## 4.1 Load the Model
```{r}
model_adv <- "advanced_model.jags"
```

## 4.2 Sequential Thompson Sampling Loop with Reward-Only Observations

```{r}
observed_rewards <- list()
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)

# Oracle computation using ground-truth global latent state:
theta_selected <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
  for (t in 1:N) {
    theta_selected[i, t] <- theta[i, z_true[t] + 1]
  }
}
oracle_arm <- apply(theta_selected, 2, which.max)

# Storage for posterior tracking:
posterior_matrices <- list()
last_z_index <- 1  # only one global z[t], shared across all arms

# Initialization:
selected_arms[1] <- sample(1:K, 1)
observed_data <- matrix(NA, nrow = K, ncol = N)
observed_data[selected_arms[1], 1] <- y_true[selected_arms[1], 1]
received_rewards[1] <- y_true[selected_arms[1], 1]
regret[1] <- theta_selected[oracle_arm[1], 1] - theta[selected_arms[1], z_true[1] + 1]

# Start Thompson Sampling loop:
for (t in 2:N) {
  sampled_values <- numeric(K)

  # Check if we have enough observed data:
  if (sum(!is.na(observed_data)) >= K * 2) {
    data_list <- list(
      y_obs = observed_data[, 1:(t-1)],
      K = K,
      N = t - 1
    )

    model <- jags.model(model_adv, data = data_list, n.chains = 1, quiet = TRUE)
    update(model, 1000)
    post <- coda.samples(model, c("theta", "pi", paste0("z[", t - 1, "]")), n.iter = 100)
    post_matrix <- as.matrix(post)

    posterior_matrices[[t]] <- post_matrix
    last_z_index <- t - 1
  }

  # Arm selection:
  for (i in 1:K) {
    if (t <= length(posterior_matrices) && is.matrix(posterior_matrices[[t]])) {
      idx <- sample(1:nrow(posterior_matrices[[t]]), 1)
      theta0 <- posterior_matrices[[t]][idx, paste0("theta[", i, ",1]")]
      theta1 <- posterior_matrices[[t]][idx, paste0("theta[", i, ",2]")]
      z_t <- posterior_matrices[[t]][idx, paste0("z[", last_z_index, "]")]
      sampled_values[i] <- (1 - z_t) * theta0 + z_t * theta1
    } else {
      sampled_values[i] <- runif(1)
    }
  }

  selected_arms[t] <- which.max(sampled_values)
  r_t <- y_true[selected_arms[t], t]
  received_rewards[t] <- r_t
  regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[t] + 1]
  observed_data[selected_arms[t], t] <- r_t
}

# Plot results:
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)

plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling (Advanced Model)")

plot(cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```

The latest posterior matrix that exists (the most recent sample from JAGS):
```{r}
last_posterior <- posterior_matrices[[max(which(!sapply(posterior_matrices, is.null)))]]

if (all(c("pi[1]", "pi[2]") %in% colnames(last_posterior))) {
  mean_pi_01 <- mean(last_posterior[, "pi[1]"])
  mean_pi_11 <- mean(last_posterior[, "pi[2]"])
  mean_pi_00 <- 1 - mean_pi_01
  mean_pi_10 <- 1 - mean_pi_11
  
  estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
                           mean_pi_10, mean_pi_11),
                         nrow = 2, byrow = TRUE,
                         dimnames = list(c("From 0", "From 1"),
                                         c("To 0", "To 1")))

  cat("\nEstimated Common Transition Matrix:\n")
  print(estimated_pi)
} else {
  cat("Transition probabilities pi[1] and pi[2] not found in posterior samples.\n")
}
```




