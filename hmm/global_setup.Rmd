---
title: "Global State Bandit Simulation"
author: "Marvin Ernst, Oriol Gelabert, Melisa Vadenja"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
```

# Problem Setup: Global Latent State

We simulate a $K$-armed bandit problem where **all arms share the same underlying latent state process** governed by a Hidden Markov Model (HMM). This represents a scenario where there's a common environment or external condition that affects the reward probability of all arms.

# 1. Generating Ground Truth Data for Global State

## 1.1 Simulate Ground Truth: 

```{r simulate-global-state}
set.seed(123)
K <- 3 # number of arms
N <- 100 # number of time steps

# true parameters
theta <- matrix(c(0.2, 0.8,
                  0.8, 0.3,
                  0.5, 0.6),
                nrow = K, byrow = TRUE)  # rows = arms, cols = states

pi_global <- matrix(c(0.90, 0.10,
                      0.05, 0.95),
                    nrow = 2, byrow = TRUE)  # transition matrix

# simulate latent states (common to all arms)
z_global <- numeric(N)
z_global[1] <- rbinom(1, 1, 0.5)
for (t in 2:N) {
  z_global[t] <- rbinom(1, 1, pi_global[z_global[t - 1] + 1, 2])
}

# simulate rewards
y_global <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
  for (t in 1:N) {
    y_global[i, t] <- rbinom(1, 1, theta[i, z_global[t] + 1])
  }
}
```

# 1.2 Visualize Latent States

```{r visualize-state}
qplot(1:N, z_global, geom = "line") +
  labs(title = "Latent Global State Over Time",
       x = "Time", y = "State")
```

## 1.3 Visualize Rewards Per Arm

```{r visualize-rewards}
y_df <- as.data.frame(y_global)
y_df <- y_df |> mutate(arm = factor(1:K)) |> 
  pivot_longer(cols = starts_with("V"), names_to = "time", values_to = "reward") |>
  mutate(time = as.numeric(gsub("V", "", time)))

ggplot(y_df, aes(x = time, y = reward, color = arm)) +
  geom_line() +
  facet_wrap(~ arm, ncol = 1) +
  labs(title = "Reward Streams Per Arm",
       x = "Time", y = "Reward")
```

## 1.4 Save Ground Truth for Inference

```{r save-data}
y_true <- y_global
z_true <- z_global
saveRDS(list(y = y_true, z = z_true, theta = theta, pi = pi_global), "global_truth_data.rds")
```

# 2. Poor Model - Thompson Sampling

## 2.1 Import JAGS Model

```{r define-ts-model}
model_file <- "poor_model.jags"
```

## 2.2 Sequential Thompson Sampling Loop with Reward-Only Observations

```{r thompson-sampling-loop-z-aware}
thompson_poor <- function(K,N,theta,y_true,z_true,Batch_size){
  
  model_file <- "poor_model.jags"
  observed_rewards <- vector("list", K)
  selected_arms <- numeric(N)
  received_rewards <- numeric(N)
  regret <- numeric(N)

  # Oracle computation of best arm probability based
  theta_selected <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      theta_selected[i, t] <- theta[i, z_true[t] + 1]
    }
  }
  oracle_arm <- apply(theta_selected, 2, which.max)


  posterior_matrices <- rep(list(1), K) #create a list of list to store the matrices (we initiate a 1 to avoid NULL errors)
  z_last <- list() #list that will contain last samples of latent states

  a_t <- sample(1:K, 1) #pick first arm to pull

  for (t in 1:N) {
    sampled_values <- numeric(K)
  
    if (length(observed_rewards[[a_t]]) >= 2 && t%%Batch_size == 0) { # we only do inference is the arm has been pulled on last iteration
    
      # JAGS inference
      data_list <- list(y = observed_rewards[[a_t]], N = length(observed_rewards[[a_t]]))
      model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
      update(model, 1000)
    
      #Store the posteriors in the posteriors matrix
      post <- coda.samples(model, c("theta0", "theta1", "pi",paste0("z[", length(observed_rewards[[a_t]]), "]")), n.iter = 100)
      post_matrix <- as.matrix(post)
      posterior_matrices[[a_t]] <- post_matrix
    
      #Store z_last provided by JAGS
      z_last[[a_t]] <- post_matrix[, paste0("z[", length(observed_rewards[[a_t]]), "]")]
    
    }
  
    for (l in 1:K){ #Now for each arm we evolve the states from z_last to t , sample one and get the theta form the posterior
    
      if (!is.null(nrow(posterior_matrices[[l]])) > 0){
        idx <- sample(1:nrow(posterior_matrices[[l]]), 1) # get one sample index
      
        # get theta for each state
        theta0 <- posterior_matrices[[l]][idx, "theta0"] 
        theta1 <- posterior_matrices[[l]][idx, "theta1"]
      
        #get transition probabilities form the posterior
        pi0 <- posterior_matrices[[l]][, "pi[1]"]
        pi1 <- posterior_matrices[[l]][, "pi[2]"]
        sampled_pi <- ifelse(z_last[[l]] == 0, pi0, pi1) #take the transition probability corresponding to last state
      
        z_sampled <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi) #evolve the state
        z_last[[l]] <- z_sampled #store the evolved state ad last state for next iteration
      
        z_t <- z_sampled[idx] #sample from evolved states
      
        sampled_values[l] <- (1-z_t)* theta0 + z_t * theta1 #get corresponding theta
      }
      else {
      sampled_values[l] <- runif(1) #if we don't have enough samples to do inference we just sample from an uniform distribution
      }
    }
  
    a_t <- which.max(sampled_values) #take the arm with highes sampled probability
    selected_arms[t] <- a_t
    r_t <- y_true[a_t, t] #observe reward
    received_rewards[t] <- r_t
    regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[t] + 1] #compute regret
    observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t) #apend the reward to the list of corresponding arm
  }
  
  cumulative_reward <- cumsum(received_rewards)
  cumulative_regret <- cumsum(regret)
  return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrices=posterior_matrices))
}

system.time({
results <- thompson_poor(K,N,theta,y_true,z_true,Batch_size=1)
})

plot(results$cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")

plot(results$cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```

We see how good the model has achieved inference on each arm dynamics (even if the ground thruth is a global state):
```{r}
for (arm in 1:K){
  mean_pi_01 <- mean(results$posterior_matrices[[arm]][, "pi[1]"])
  mean_pi_11 <- mean(results$posterior_matrices[[arm]][, "pi[2]"])
  mean_pi_00 <- 1 - mean_pi_01
  mean_pi_10 <- 1 - mean_pi_11
  
  estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
                           mean_pi_10, mean_pi_11),
                         nrow = 2, byrow = TRUE,
                         dimnames = list(c("From A", "From B"),
                                         c("To A", "To B")))
  
  mean_theta_0 <- mean(results$posterior_matrices[[arm]][, "theta0"])
  mean_theta_1 <- mean(results$posterior_matrices[[arm]][, "theta1"])
  
  estimated_theta <- matrix(c(mean_theta_0, mean_theta_1),
                         nrow = 1, byrow = TRUE,
                         dimnames = list(NULL,c("State A", "State B")))

  cat("\nEstimated Transition Matrix for arm ",arm,":\n")
  print(estimated_pi)
  cat("\nEstimated Reward probabilities for arm ", arm,":\n")
  print(estimated_theta)
}
```

# Run in Expectation

```{r}
#We define the parameters of the simulations
runs <- 10
Batch_size <-10
K <- 3
N <-100
theta <- matrix(c(0.2, 0.8,
                  0.8, 0.3,
                  0.5, 0.6),
                nrow = K, byrow = TRUE)  # rows = arms, cols = states
pi_global <- matrix(c(0.90, 0.10,
                      0.05, 0.95),
                    nrow = 2, byrow = TRUE)  # transition matrix
reward_matrix <- matrix(0, nrow = runs, ncol = N)
regret_matrix <- matrix(0, nrow = runs, ncol = N)

for (run in 1:runs){
  
  z_true <- numeric(N)
  z_true[1] <- rbinom(1, 1, 0.5)
  for (t in 2:N) {
    z_true[t] <- rbinom(1, 1, pi_global[z_true[t - 1] + 1, 2])
  }

  # simulate rewards
  y_true <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      y_true[i, t] <- rbinom(1, 1, theta[i, z_true[t] + 1])
    }
  }
  
  results <- thompson_poor(K,N,theta,y_true,z_true,Batch_size)
  
  reward_matrix[run,] <- results$cumulative_reward
  regret_matrix[run,] <- results$cumulative_regret
  
  if (run%%10 == 0){cat("Finished run", run, "\n")}
}

expected_rewards_thompson_poor <- colMeans(reward_matrix)
expected_regrets_thompson_poor <- colMeans(regret_matrix)

plot(expected_rewards_thompson_poor, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Reward", main = "Thompson Sampling")

plot(expected_regrets_thompson_poor, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Regret", main = "Expected Regret vs Oracle")
``` 


# 3. Poor Model - UCB

```{r}
#recover same simulation we ran before
y_true <- y_global
z_true <- z_global
K <- 3
N <- 100
```

```{r}
ucb_poor <- function(K,N,theta,y_true,z_true,Batch_size) {
  
  observed_rewards <- vector("list", K)
  selected_arms <- numeric(N)
  received_rewards <- numeric(N)
  regret <- numeric(N)

  # Oracle computation using ground-truth global latent state:
  theta_selected <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      theta_selected[i, t] <- theta[i, z_true[t] + 1]
    }
  }
  oracle_arm <- apply(theta_selected, 2, which.max)

  posterior_matrices <- rep(list(1), K)
  z_last <- list()
  a_t <- sample(1:K, 1) #pick first arm to pull

  for (t in 1:N) {
    sampled_values <- numeric(K)
  
    if (length(observed_rewards[[a_t]]) >= 2 && t%%Batch_size==0) {
      
      data_list <- list(y = observed_rewards[[a_t]], N = length(observed_rewards[[a_t]]))
      model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
      update(model, 1000)
      
      post <- coda.samples(model, c("theta0", "theta1", "pi",paste0("z[", length(observed_rewards[[a_t]]), "]")), n.iter = 100)
      post_matrix <- as.matrix(post)
      posterior_matrices[[a_t]] <- post_matrix
      
      z_last[[a_t]] <- post_matrix[, paste0("z[", length(observed_rewards[[a_t]]), "]")]
      
    }
    
    for (l in 1:K){
      
      if (!is.null(nrow(posterior_matrices[[l]])) > 0){
        idx <- sample(1:nrow(posterior_matrices[[l]]), 1)
        
        theta0 <- posterior_matrices[[l]][idx, "theta0"]
        theta1 <- posterior_matrices[[l]][idx, "theta1"]
        
        pi0 <- posterior_matrices[[l]][, "pi[1]"]
        pi1 <- posterior_matrices[[l]][, "pi[2]"]
        sampled_pi <- ifelse(z_last[[l]] == 0, pi0, pi1)
        
        z_sampled <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi)
        z_last[[l]] <- z_sampled
        
        sampled_theta <- ifelse(z_sampled == 0, theta0, theta1)
        
        sampled_values[l] <- mean(sampled_theta) + sd(sampled_theta)/sqrt(length(sampled_theta))*log(t)
      }
      else {
      sampled_values[l] <- runif(1)
      }
    }
    
    a_t <- which.max(sampled_values)
    selected_arms[t] <- a_t
    r_t <- y_true[a_t, t]
    received_rewards[t] <- r_t
    regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[t] + 1]
    observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
  }

  cumulative_reward <- cumsum(received_rewards)
  cumulative_regret <- cumsum(regret)
  return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrices=posterior_matrices))
}

system.time({
results <- ucb_poor(K,N,theta,y_true,z_true,Batch_size=1)
})

plot(results$cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "UCB")

plot(results$cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```

```{r}
for (arm in 1:K){
  mean_pi_01 <- mean(results$posterior_matrices[[arm]][, "pi[1]"])
  mean_pi_11 <- mean(results$posterior_matrices[[arm]][, "pi[2]"])
  mean_pi_00 <- 1 - mean_pi_01
  mean_pi_10 <- 1 - mean_pi_11
  
  estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
                           mean_pi_10, mean_pi_11),
                         nrow = 2, byrow = TRUE,
                         dimnames = list(c("From A", "From B"),
                                         c("To A", "To B")))
  
  mean_theta_0 <- mean(results$posterior_matrices[[arm]][, "theta0"])
  mean_theta_1 <- mean(results$posterior_matrices[[arm]][, "theta1"])
  
  estimated_theta <- matrix(c(mean_theta_0, mean_theta_1),
                         nrow = 1, byrow = TRUE,
                         dimnames = list(NULL,c("State A", "State B")))

  cat("\nEstimated Transition Matrix for arm ",arm,":\n")
  print(estimated_pi)
  cat("\nEstimated Reward probabilities for arm ", arm,":\n")
  print(estimated_theta)
}
  
```
# Run in Expectation

```{r}
#We define the parameters of the simulations
system.time({
runs <- 10
Batch_size <-10
K <- 3
N <-100
theta <- matrix(c(0.2, 0.8,
                  0.8, 0.3,
                  0.5, 0.6),
                nrow = K, byrow = TRUE)  # rows = arms, cols = states
pi_global <- matrix(c(0.90, 0.10,
                      0.05, 0.95),
                    nrow = 2, byrow = TRUE)  # transition matrix
reward_matrix <- matrix(0, nrow = runs, ncol = N)
regret_matrix <- matrix(0, nrow = runs, ncol = N)

for (run in 1:runs){
  
  z_true <- numeric(N)
  z_true[1] <- rbinom(1, 1, 0.5)
  for (t in 2:N) {
    z_true[t] <- rbinom(1, 1, pi_global[z_true[t - 1] + 1, 2])
  }

  # simulate rewards
  y_true <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      y_true[i, t] <- rbinom(1, 1, theta[i, z_true[t] + 1])
    }
  }
  
  system.time({
  results <- ucb_poor(K,N,theta,y_true,z_true,Batch_size)
  })
  
  reward_matrix[run,] <- results$cumulative_reward
  regret_matrix[run,] <- results$cumulative_regret
  
  if (run%%10 == 0){cat("Finished run", run, "\n")}
}

expected_rewards_ucb_poor <- colMeans(reward_matrix)
expected_regrets_ucb_poor <- colMeans(regret_matrix)
})

plot(expected_rewards_ucb_poor, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Reward", main = "UCB")

plot(expected_regrets_ucb_poor, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Regret", main = "Expected Regret vs Oracle")
``` 


# 4. Advanced Model - TS

## 4.1 Load the Model
```{r}
model_adv <- "advanced_model.jags"
```

```{r}
#recover same simulation we ran before
y_true <- y_global
z_true <- z_global
K <- 3
N <- 100
```
## 4.2 Sequential Thompson Sampling Loop with Reward-Only Observations

```{r}
thompson_advanced <- function(K,N,theta,y_true,z_true,Batch_size){
  
  model_adv <- "advanced_model.jags"
  observed_rewards <- list()
  selected_arms <- numeric(N)
  received_rewards <- numeric(N)
  regret <- numeric(N)
  
  # Oracle computation using ground-truth global latent state:
  theta_selected <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      theta_selected[i, t] <- theta[i, z_true[t] + 1]
    }
  }
  oracle_arm <- apply(theta_selected, 2, which.max)
  
  # Storage for posterior tracking:
  post_matrix<- NULL
  
  # Initialization:
  
  selected_arms[1] <- sample(1:K, 1)
  observed_data <- matrix(NA, nrow = K, ncol = N)
  observed_data[selected_arms[1], 1] <- y_true[selected_arms[1], 1]
  received_rewards[1] <- y_true[selected_arms[1], 1]
  regret[1] <- theta_selected[oracle_arm[1], 1] - theta[selected_arms[1], z_true[1] + 1]
  
  # Start Thompson Sampling loop:
  for (t in 2:N) {
    sampled_values <- numeric(K)
  
    # Check if we have enough observed data:
    if (sum(!is.na(observed_data)) >= K * 2 && t%%Batch_size==0) {
      data_list <- list(
        y_obs = observed_data[, 1:(t-1)],
        K = K,
        N = t - 1
      )
  
      model <- jags.model(model_adv, data = data_list, n.chains = 1, quiet = TRUE)
      update(model, 1000)
      post <- coda.samples(model, c("theta", "pi", paste0("z[", t - 1, "]")), n.iter = 100)
      post_matrix <- as.matrix(post)
    }
    
      if (is.matrix(post_matrix)) {
        idx <- sample(1:nrow(post_matrix), 1)
      
        pi0 <- post_matrix[idx,"pi[1]"]
        pi1 <- post_matrix[idx,"pi[2]"]
        
        if (t%%Batch_size==0) { z_last <- post_matrix[idx, paste0("z[", t-1, "]")]}
        
        sampled_pi <- ifelse(z_last == 0, pi0, pi1)
        z_t <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi)
        z_last <- z_t
      }
    
    # Arm selection:
    for (i in 1:K) {
      if (is.matrix(post_matrix)) {
        theta0 <- post_matrix[idx, paste0("theta[", i, ",1]")]
        theta1 <- post_matrix[idx, paste0("theta[", i, ",2]")]
        sampled_values[i] <- (1 - z_t) * theta0 + z_t * theta1
      } else {
        sampled_values[i] <- runif(1)
      }
    }
  
    selected_arms[t] <- which.max(sampled_values)
    r_t <- y_true[selected_arms[t], t]
    received_rewards[t] <- r_t
    regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[t] + 1]
    observed_data[selected_arms[t], t] <- r_t
  }
  
  cumulative_reward <- cumsum(received_rewards)
  cumulative_regret <- cumsum(regret)
  return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrix=post_matrix))
}

system.time({
results<- thompson_advanced (K,N,theta,y_true,z_true,Batch_size=1)
})

plot(results$cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling (Advanced Model)")

plot(results$cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```

The latest posterior matrix that exists (the most recent sample from JAGS):
```{r}
last_posterior <- results$posterior_matrix

if (all(c("pi[1]", "pi[2]") %in% colnames(last_posterior))) {
  mean_pi_01 <- mean(last_posterior[, "pi[1]"])
  mean_pi_11 <- mean(last_posterior[, "pi[2]"])
  mean_pi_00 <- 1 - mean_pi_01
  mean_pi_10 <- 1 - mean_pi_11
  
  estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
                           mean_pi_10, mean_pi_11),
                         nrow = 2, byrow = TRUE,
                         dimnames = list(c("From A", "From B"),
                                         c("To A", "To B")))
  
  mean_theta_01 <- mean(last_posterior[, "theta[1,1]"])
  mean_theta_11 <- mean(last_posterior[, "theta[1,2]"])
  mean_theta_02 <- mean(last_posterior[, "theta[2,1]"])
  mean_theta_12 <- mean(last_posterior[, "theta[2,2]"])
  mean_theta_03 <- mean(last_posterior[, "theta[3,1]"])
  mean_theta_13 <- mean(last_posterior[, "theta[3,2]"])
  
  estimated_theta <- matrix(c(mean_theta_01, mean_theta_11,
                           mean_theta_02,mean_theta_12,
                           mean_theta_03,mean_theta_13),
                         nrow = 3, byrow = TRUE,
                         dimnames = list(c("Arm 1", "Arm 2", "Arm 3"),
                                         c("State A", "State B")))

  cat("\nEstimated Common Transition Matrix:\n")
  print(estimated_pi)
    cat("\nEstimated Reward probabilities:\n")
  print(estimated_theta)
} else {
  cat("Transition probabilities pi[1] and pi[2] not found in posterior samples.\n")
}
```

#Run on Expectation

```{r,results=False}
#We define the parameters of the simulations
runs <- 10
Batch_size <-10
K <- 3
N <-100
theta <- matrix(c(0.2, 0.8,
                  0.8, 0.3,
                  0.5, 0.6),
                nrow = K, byrow = TRUE)  # rows = arms, cols = states
pi_global <- matrix(c(0.90, 0.10,
                      0.05, 0.95),
                    nrow = 2, byrow = TRUE)  # transition matrix
reward_matrix <- matrix(0, nrow = runs, ncol = N)
regret_matrix <- matrix(0, nrow = runs, ncol = N)

for (run in 1:runs){
  
  z_true <- numeric(N)
  z_true[1] <- rbinom(1, 1, 0.5)
  for (t in 2:N) {
    z_true[t] <- rbinom(1, 1, pi_global[z_true[t - 1] + 1, 2])
  }

  # simulate rewards
  y_true <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      y_true[i, t] <- rbinom(1, 1, theta[i, z_true[t] + 1])
    }
  }
  
  results <- thompson_advanced(K,N,theta,y_true,z_true,Batch_size)
  
  reward_matrix[run,] <- results$cumulative_reward
  regret_matrix[run,] <- results$cumulative_regret
  
  if (run%%10 == 0){cat("Finished run", run, "\n")}
}

expected_rewards_thompson_advanced <- colMeans(reward_matrix)
expected_regrets_thompson_advanced <- colMeans(regret_matrix)

plot(expected_rewards_thompson_advanced, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Reward", main = "Thompson Sampling (Advanced Model)")

plot(expected_regrets_thompson_advanced, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Regret", main = "Expected Regret vs Oracle")
```
## Global UCB

```{r}
#recover same simulation we ran before
y_true <- y_global
z_true <- z_global
K <- 3
N <- 100
```

```{r}
ucb_advanced <- function(K,N,theta,y_true,z_true,Batch_size){
  
  model_adv <- "advanced_model.jags"
  observed_rewards <- list()
  selected_arms <- numeric(N)
  received_rewards <- numeric(N)
  regret <- numeric(N)
  
  # Oracle computation using ground-truth global latent state:
  theta_selected <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      theta_selected[i, t] <- theta[i, z_true[t] + 1]
    }
  }
  oracle_arm <- apply(theta_selected, 2, which.max)
  
  # Storage for posterior tracking:
  post_matrix<- NULL
  
  # Initialization:
  selected_arms[1] <- sample(1:K, 1)
  observed_data <- matrix(NA, nrow = K, ncol = N)
  observed_data[selected_arms[1], 1] <- y_true[selected_arms[1], 1]
  received_rewards[1] <- y_true[selected_arms[1], 1]
  regret[1] <- theta_selected[oracle_arm[1], 1] - theta[selected_arms[1], z_true[1] + 1]
  
  # Start Thompson Sampling loop:
  for (t in 2:N) {
    sampled_values <- numeric(K)
  
    # Check if we have enough observed data:
    if (sum(!is.na(observed_data)) >= K * 2  && t%%Batch_size == 0) {
      data_list <- list(
        y_obs = observed_data[, 1:(t-1)],
        K = K,
        N = t - 1
      )
  
      model <- jags.model(model_adv, data = data_list, n.chains = 1, quiet = TRUE)
      update(model, 1000)
      post <- coda.samples(model, c("theta", "pi", paste0("z[", t - 1, "]")), n.iter = 100)
      post_matrix <- as.matrix(post)
    }
  
    if (is.matrix(post_matrix)) {
      pi0 <- post_matrix[,"pi[1]"]
      pi1 <- post_matrix[,"pi[2]"]
      
      if (t%%Batch_size==0) { z_last <- post_matrix[, paste0("z[", t-1, "]")]}
      
      sampled_pi <- ifelse(z_last == 0, pi0, pi1)
      
      z_t <- rbinom(length(sampled_pi), size = 1, prob = sampled_pi)
      z_last <- z_t
    }
    
    # Arm selection:
    for (i in 1:K) {
      if (is.matrix(post_matrix)) {
        
        theta0 <- post_matrix[, paste0("theta[", i, ",1]")]
        theta1 <- post_matrix[, paste0("theta[", i, ",2]")]
        
        sampled_theta <- ifelse(z_t == 0, theta0, theta1)
        
        sampled_values[i] <- mean(sampled_theta) + sd(sampled_theta)/sqrt(length(sampled_theta))*log(t)
      } else {
        sampled_values[i] <- runif(1)
      }
    }
  
    selected_arms[t] <- which.max(sampled_values)
    r_t <- y_true[selected_arms[t], t]
    received_rewards[t] <- r_t
    regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[t] + 1]
    observed_data[selected_arms[t], t] <- r_t
  }
  
  cumulative_reward <- cumsum(received_rewards)
  cumulative_regret <- cumsum(regret)
  return(list(cumulative_reward=cumulative_reward,cumulative_regret=cumulative_regret,posterior_matrix=post_matrix))
}

system.time({
results <- ucb_advanced(K,N,theta,y_true,z_true,Batch_size=1)
})
  
plot(results$cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Cumulative Reward", main = "UCB (Advanced Model)")

plot(results$cumulative_regret, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
```

```{r}
last_posterior <- results$posterior_matrix

if (all(c("pi[1]", "pi[2]") %in% colnames(last_posterior))) {
  mean_pi_01 <- mean(last_posterior[, "pi[1]"])
  mean_pi_11 <- mean(last_posterior[, "pi[2]"])
  mean_pi_00 <- 1 - mean_pi_01
  mean_pi_10 <- 1 - mean_pi_11
  
  estimated_pi <- matrix(c(mean_pi_00, mean_pi_01,
                           mean_pi_10, mean_pi_11),
                         nrow = 2, byrow = TRUE,
                         dimnames = list(c("From A", "From B"),
                                         c("To A", "To B")))
  
  mean_theta_01 <- mean(last_posterior[, "theta[1,1]"])
  mean_theta_11 <- mean(last_posterior[, "theta[1,2]"])
  mean_theta_02 <- mean(last_posterior[, "theta[2,1]"])
  mean_theta_12 <- mean(last_posterior[, "theta[2,2]"])
  mean_theta_03 <- mean(last_posterior[, "theta[3,1]"])
  mean_theta_13 <- mean(last_posterior[, "theta[3,2]"])
  
  estimated_theta <- matrix(c(mean_theta_01, mean_theta_11,
                           mean_theta_02,mean_theta_12,
                           mean_theta_03,mean_theta_13),
                         nrow = 3, byrow = TRUE,
                         dimnames = list(c("Arm 1", "Arm 2", "Arm 3"),
                                         c("State A", "State B")))

  cat("\nEstimated Common Transition Matrix:\n")
  print(estimated_pi)
    cat("\nEstimated Reward probabilities:\n")
  print(estimated_theta)
} else {
  cat("Transition probabilities pi[1] and pi[2] not found in posterior samples.\n")
}
```


#Run on Expectation

```{r,results=False}
#We define the parameters of the simulations
runs <- 10
Batch_size <-10
K <- 3
N <-100
theta <- matrix(c(0.2, 0.8,
                  0.8, 0.3,
                  0.5, 0.6),
                nrow = K, byrow = TRUE)  # rows = arms, cols = states
pi_global <- matrix(c(0.90, 0.10,
                      0.05, 0.95),
                    nrow = 2, byrow = TRUE)  # transition matrix
reward_matrix <- matrix(0, nrow = runs, ncol = N)
regret_matrix <- matrix(0, nrow = runs, ncol = N)

for (run in 1:runs){
  
  z_true <- numeric(N)
  z_true[1] <- rbinom(1, 1, 0.5)
  for (t in 2:N) {
    z_true[t] <- rbinom(1, 1, pi_global[z_true[t - 1] + 1, 2])
  }

  # simulate rewards
  y_true <- matrix(0, nrow = K, ncol = N)
  for (i in 1:K) {
    for (t in 1:N) {
      y_true[i, t] <- rbinom(1, 1, theta[i, z_true[t] + 1])
    }
  }
  
  results <- ucb_advanced(K,N,theta,y_true,z_true,Batch_size)
  
  reward_matrix[run,] <- results$cumulative_reward
  regret_matrix[run,] <- results$cumulative_regret
  
  if (run%%10 == 0){cat("Finished run", run, "\n")}
}

expected_rewards_ucb_advanced <- colMeans(reward_matrix)
expected_regrets_ucb_advanced <- colMeans(regret_matrix)

plot(expected_rewards_ucb_advanced, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Reward", main = "UCB (Advanced Model)")

plot(expected_regrets_ucb_advanced, type = "l", col = "red", lwd = 2,
     xlab = "Time", ylab = "Expected Cumulative Regret", main = "Expected Regret vs Oracle")
```