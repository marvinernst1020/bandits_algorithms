---
title: "Global State Bandit Simulation â€“ KFAS AR(1) vs. Baseline TS"
author: "Marvin Ernst, Oriol Gelabert, Melisa Vadenja"
date: "`r Sys.Date()`"
output: html_document
---

only once:
```{r}
#install.packages("KFAS")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
library(here)
library(glue)
library(future.apply)
library(stats)
library(KFAS)
plan(multisession)
```



# Problem Setup: Global Latent State

We simulate a $K$-armed bandit problem where **all arms share the same underlying latent state process** governed by a Hidden Markov Model (HMM). This represents a scenario where there's a common environment or external condition that affects the reward probability of all arms.

### Loading all the Models and Functions
```{r}
source(here("src/load_all.R"))
```

Setting a seed:
```{r}
set.seed(123) 
```

---

# GLOABL SETTING

Number of arms:
```{r}
K <- 2
```

Number of time steps:
```{r}
N <- 5000
```

Transition Probability:
```{r}
pi_global <- matrix(c(0.999, 0.001,
                     0.001, 0.999),
                     nrow = 2, byrow = TRUE)  
```

Probability of Reward:
```{r}
mu <- matrix(c(0.95, 0.1,
               0.1, 0.95),
                nrow = K, byrow = TRUE)
```

---

# 1. Generating Ground Truth Data for Global State

## 1.1 Simulate Ground Truth

For only one Run:
```{r simulate-global-state}
generate_global_datasets(
  K = K,
  N = N,
  mu = mu,
  pi_global = pi_global,
  n_runs = 1,
  scenario_name = "single_run",
  root_path = "data_global"
)
```

Load the generated data:
```{r}
truth <- readRDS("data_global/single_run/global_truth_1.rds")
y_global <- truth$y
z_global <- truth$z
```


# 1.2 Visualize Latent States

```{r visualize-state}
qplot(1:N, z_global, geom = "line") +
  labs(title = "Latent Global State Over Time",
       x = "Time", y = "State")
```

## 1.3 Visualize Rewards Per Arm

```{r visualize-rewards}
y_df <- as.data.frame(y_global)
y_df <- y_df |> mutate(arm = factor(1:K)) |> 
  pivot_longer(cols = starts_with("V"), names_to = "time", values_to = "reward") |>
  mutate(time = as.numeric(gsub("V", "", time)))

ggplot(y_df, aes(x = time, y = reward, color = arm)) +
  geom_line() +
  facet_wrap(~ arm, ncol = 1) +
  labs(title = "Reward Streams Per Arm",
       x = "Time", y = "Reward")
```

---

# 2. Single Run - Model Comparison

## 2.1 Thompson Sampling

### Baseline
```{r}
baseline_ts_results <- bandit_baselines("ts", 
                                        K, N, y_global, z_global, mu, 
                                        dynamics = "common",
                                        batch_size = 1)
```

Store in df:
```{r}
baseline_ts_results_df <- data.frame(
  time = seq_along(baseline_ts_results$cumulative_reward),
  cumulative_reward = baseline_ts_results$cumulative_reward,
  cumulative_regret = baseline_ts_results$cumulative_regret,
  model = "Baseline TS"
)
```

### Adavanced Model
```{r}
system.time({
res_kfas_ar <- thompson_ar_kfas(K, N, mu, y_global, z_global, batch_size = 10)
})
```

Store in df:
```{r}
kfas_ar_results_df <- data.frame(
  time = seq_along(res_kfas_ar$cumulative_reward),
  cumulative_reward = res_kfas_ar$cumulative_reward,
  cumulative_regret = res_kfas_ar$cumulative_regret,
  model = "KFAS AR(1) TS"
)
```

Combining results into one df:
```{r}
ts_compare_df <- bind_rows(baseline_ts_results_df, kfas_ar_results_df)
```

### Cumulative Reward:
```{r}
ggplot(ts_compare_df, aes(x = time, y = cumulative_reward, color = model)) +
  geom_line(size = 1) +
  labs(title = "Cumulative Reward: Baseline vs KFAS AR(1) Thompson Sampling",
       x = "Time",
       y = "Cumulative Reward") +
  theme_minimal()
```

### Cumulative Regret:
```{r}
ggplot(ts_compare_df, aes(x = time, y = cumulative_regret, color = model)) +
  geom_line(size = 1) +
  labs(title = "Cumulative Regret: Baseline vs KFAS AR(1) Thompson Sampling",
       x = "Time",
       y = "Cumulative Regret") +
  theme_minimal()
```

