{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c12953",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# GPU available?\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Global State Bandit Simulation - Fastest Python Version with GPU Support\n",
    "\n",
    "# Cell 1: Imports and Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import pyro.poutine as poutine\n",
    "import pandas as pd\n",
    "\n",
    "# GPU available?\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cell 2: Parameters\n",
    "K = 3\n",
    "N = 1000\n",
    "\n",
    "theta = torch.tensor([\n",
    "    [0.1, 0.9],\n",
    "    [0.5, 0.7],\n",
    "    [0.9, 0.1]\n",
    "], device=device)\n",
    "\n",
    "pi_indiv = torch.tensor([\n",
    "    [[0.9, 0.1], [0.1, 0.9]],\n",
    "    [[0.85, 0.15], [0.2, 0.8]],\n",
    "    [[0.95, 0.05], [0.3, 0.7]]\n",
    "], device=device)\n",
    "\n",
    "# Cell 3: Simulate Ground Truth Data\n",
    "def simulate_bandit_data(K, N, theta, pi_indiv):\n",
    "    z = torch.zeros((K, N), dtype=torch.int64, device=device)\n",
    "    y = torch.zeros((K, N), dtype=torch.int64, device=device)\n",
    "\n",
    "    for i in range(K):\n",
    "        z[i, 0] = torch.bernoulli(torch.tensor(0.5, device=device)).long()\n",
    "        y[i, 0] = torch.bernoulli(theta[i, z[i, 0]])\n",
    "        for t in range(1, N):\n",
    "            prob = pi_indiv[i, z[i, t-1], 1]\n",
    "            z[i, t] = torch.bernoulli(prob).long()\n",
    "            y[i, t] = torch.bernoulli(theta[i, z[i, t]])\n",
    "    return y, z\n",
    "\n",
    "# Cell 4: Generate and Store Data\n",
    "y_true, z_true = simulate_bandit_data(K, N, theta, pi_indiv)\n",
    "\n",
    "# Cell 5: Baseline Thompson Sampling\n",
    "def baseline_thompson(K, N, y_true, z_true, theta):\n",
    "    rewards = torch.zeros(K, device=device)\n",
    "    counts = torch.ones(K, device=device)\n",
    "    cum_regret = torch.zeros(N, device=device)\n",
    "    cum_reward = torch.zeros(N, device=device)\n",
    "    selected = torch.zeros(N, dtype=torch.int64, device=device)\n",
    "\n",
    "    for t in range(N):\n",
    "        samples = torch.distributions.Beta(rewards + 1, counts - rewards + 1).sample()\n",
    "        a_t = torch.argmax(samples).item()\n",
    "\n",
    "        r = y_true[a_t, t].float()\n",
    "        rewards[a_t] += r\n",
    "        counts[a_t] += 1\n",
    "\n",
    "        oracle = torch.argmax(torch.tensor([theta[k, z_true[k, t]] for k in range(K)], device=device))\n",
    "        opt_reward = theta[oracle, z_true[oracle, t]]\n",
    "        regret = opt_reward - theta[a_t, z_true[a_t, t]]\n",
    "\n",
    "        cum_reward[t] = rewards.sum()\n",
    "        cum_regret[t] = cum_regret[t - 1] + regret if t > 0 else regret\n",
    "        selected[t] = a_t\n",
    "\n",
    "    return cum_reward.cpu().numpy(), cum_regret.cpu().numpy(), selected.cpu().numpy()\n",
    "\n",
    "# Cell 6: Run Baseline Thompson\n",
    "c_reward, c_regret, chosen = baseline_thompson(K, N, y_true, z_true, theta)\n",
    "\n",
    "# Cell 7: Plot Results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(c_reward, label='Cumulative Reward', color='green')\n",
    "plt.title('Baseline Thompson Sampling - Reward')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(c_regret, label='Cumulative Regret', color='red')\n",
    "plt.title('Baseline Thompson Sampling - Regret')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Cell 8: Expected Regret (Optional Multiple Runs)\n",
    "def run_expectation(K, N, theta, pi_indiv, runs=100):\n",
    "    rewards_all = []\n",
    "    regrets_all = []\n",
    "    for _ in range(runs):\n",
    "        y, z = simulate_bandit_data(K, N, theta, pi_indiv)\n",
    "        r, rg, _ = baseline_thompson(K, N, y, z, theta)\n",
    "        rewards_all.append(r)\n",
    "        regrets_all.append(rg)\n",
    "    return np.mean(rewards_all, axis=0), np.mean(regrets_all, axis=0)\n",
    "\n",
    "# Cell 9: Run and Plot Expectation\n",
    "exp_reward, exp_regret = run_expectation(K, N, theta, pi_indiv, runs=100)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(exp_reward, label='Expected Reward', color='green')\n",
    "plt.title('Expected Reward over Time (100 Runs)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Expected Cumulative Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(exp_regret, label='Expected Regret', color='red')\n",
    "plt.title('Expected Regret over Time (100 Runs)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Expected Cumulative Regret')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Pyro model for single arm with discrete latent state\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "\n",
    "def hmm_arm_model(y_obs):\n",
    "    N = len(y_obs)\n",
    "    theta0 = pyro.sample(\"theta0\", dist.Beta(1., 1.))\n",
    "    theta1 = pyro.sample(\"theta1\", dist.Beta(1., 1.))\n",
    "    pi0 = pyro.sample(\"pi0\", dist.Beta(1., 1.))\n",
    "    pi1 = pyro.sample(\"pi1\", dist.Beta(1., 1.))\n",
    "\n",
    "    z_prev = pyro.sample(\"z_0\", dist.Bernoulli(0.5))\n",
    "    for t in range(N):\n",
    "        z_t = pyro.sample(f\"z_{t}\", dist.Bernoulli(pi1 if z_prev == 1 else pi0))\n",
    "        p = theta1 if z_t == 1 else theta0\n",
    "        pyro.sample(f\"y_{t}\", dist.Bernoulli(p), obs=y_obs[t])\n",
    "        z_prev = z_t\n",
    "\n",
    "# Cell 6: Thompson Sampling with posterior samples from Pyro\n",
    "from pyro.infer import config_enumerate\n",
    "\n",
    "def thompson_sampling_pyro(K, N, theta, y_true, batch_size=10):\n",
    "    posterior_samples = [None] * K\n",
    "    observed_rewards = [[] for _ in range(K)]\n",
    "    selected_arms = []\n",
    "    regrets = []\n",
    "\n",
    "    for t in range(N):\n",
    "        sampled_probs = []\n",
    "        for k in range(K):\n",
    "            if posterior_samples[k] is not None:\n",
    "                sample = posterior_samples[k]\n",
    "                theta0 = sample[\"theta0\"]\n",
    "                theta1 = sample[\"theta1\"]\n",
    "                pi0 = sample[\"pi0\"]\n",
    "                pi1 = sample[\"pi1\"]\n",
    "                z_prev = sample.get(\"z_prev\", 0.0)\n",
    "                z_t = torch.bernoulli(pi1 if z_prev == 1 else pi0)\n",
    "                prob = (1 - z_t) * theta0 + z_t * theta1\n",
    "            else:\n",
    "                prob = torch.rand(1)\n",
    "            sampled_probs.append(prob)\n",
    "\n",
    "        a_t = torch.argmax(torch.tensor(sampled_probs)).item()\n",
    "        selected_arms.append(a_t)\n",
    "\n",
    "        reward = y_true[a_t, t].item()\n",
    "        observed_rewards[a_t].append(reward)\n",
    "\n",
    "        opt_arm = torch.argmax(torch.tensor([theta[i, z_true[i, t]] for i in range(K)])).item()\n",
    "        opt_reward = theta[opt_arm, z_true[opt_arm, t]]\n",
    "        actual_reward = theta[a_t, z_true[a_t, t]]\n",
    "        regret = (opt_reward - actual_reward).item()\n",
    "        regrets.append(regret)\n",
    "\n",
    "        if len(observed_rewards[a_t]) % batch_size == 0 and len(observed_rewards[a_t]) >= 5:\n",
    "            y_obs = torch.tensor(observed_rewards[a_t], dtype=torch.float32)\n",
    "            nuts_kernel = NUTS(hmm_arm_model)\n",
    "            mcmc = MCMC(nuts_kernel, num_samples=25, warmup_steps=100, disable_progbar=True)\n",
    "            mcmc.run(y_obs)\n",
    "            posterior_samples[a_t] = {k: v[0].detach() for k, v in mcmc.get_samples().items()}\n",
    "            posterior_samples[a_t][\"z_prev\"] = posterior_samples[a_t].get(f\"z_{len(y_obs)-1}\", torch.tensor(0.))\n",
    "\n",
    "    return np.cumsum([y_true[a, t].item() for t, a in enumerate(selected_arms)]), np.cumsum(regrets)\n",
    "\n",
    "# Cell 7: Run Poor Model Thompson Sampling\n",
    "poor_reward, poor_regret = thompson_sampling_pyro(K, N, theta, y_true, batch_size=10)\n",
    "\n",
    "# Cell 8: Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(poor_reward, label='Cumulative Reward', color='blue')\n",
    "plt.title('Poor Model Thompson Sampling (Pyro - Discrete Latent State)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(poor_regret, label='Cumulative Regret', color='orange')\n",
    "plt.title('Poor Model Thompson Sampling (Pyro - Discrete Latent State)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bandit)",
   "language": "python",
   "name": "bandit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
