---
title: "Bandit Algorithms in a Markov Switching Environment"
author: "Marvin Ernst & Oriol Gelabert & Melisa Vadenja"
date: "2025-04-13"
output: html_document
editor_options: 
  chunk_output_type: inline
---

Clear environment:
```{r}
rm(list = ls())
```

Install from GitHub - devtools:
```{r}
#install.packages("devtools")
#devtools::install_github("stan-dev/cmdstanr", dependencies = TRUE)
```

Load necessary libraries:
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores()) # use all cores
```


# 1. Simulate Markov Switching Model (MSM)

We simulate a 2-state MSM where:
- State 1: $\pi_1 = 0.8$ (high success)
- State 2: $\pi_2 = 0.2$ (low success)
- Transition matrix:
  $\begin{bmatrix} 0.9 & 0.1 \\ 0.1 & 0.9 \end{bmatrix}$

```{r simulate-msm}
set.seed(42)
n <- 500
states <- numeric(n)
y <- numeric(n)

# Transition matrix
P <- matrix(c(0.9, 0.1,
              0.1, 0.9), nrow = 2, byrow = TRUE)

# State-dependent success probabilities
pi_vals <- c(0.8, 0.2)
states[1] <- 1
for (t in 2:n) {
  states[t] <- sample(1:2, 1, prob = P[states[t-1], ])
}
y <- rbinom(n, size = 1, prob = pi_vals[states])

data_msm <- tibble(t = 1:n, state = states, y = y)
stan_data <- list(N = n, y = y)
```

# 2. Estimate Parameters Using Stan

```{r}
fit <- stan(file = "msm_bernoulli.stan",
            data = stan_data,
            iter = 2000,
            warmup = 1000,
            chains = 4,
            seed = 123)
```

```{r}
print(fit, pars = c("pi", "p"))
```

pi[1] and pi[2] are the mixture proportions, i.e., how often the model believes the system is in state 1 vs state 2.

Our model is unsure about which state dominates – it splits the weight evenly. This is expected since:

- We gave it no state labels.

- The success probabilities p[1] and p[2] are very similar. 

The model couldn’t clearly separate the states, and estimates both states with similar success probabilities (~0.55), centered between 0.8 and 0.2.

**Why didn’t the model recover the true parameters?**

Possible Reasons:

1.	Label switching: The model doesn’t know which state is “state 1” vs “state 2” — it’s symmetric. You just get a mix of the two.
	
2.	No time structure: The Stan model was a mixture model, not a full Markov switching model. So it didn’t learn the transition matrix or sequence of states.
	
3.	Identifiability: Without constraints or state information, the model can’t confidently tell apart the two distributions.
	
	
Saving the model (to avoid recompiling every time):
```{r}
saveRDS(fit, "fit_msm_model.rds")
fit <- readRDS("fit_msm_model.rds")
```

# 3. Simulate Hidden Markov Model (HMM)

```{r simulate-hmm}
data_hmm <- tibble(t = 1:n, state = states, y = y)
```

Fit HMM with rstan:

```{r fit-hmm-stan, eval=FALSE}
fit <- stan(file = "hmm_bernoulli.stan",
            data = stan_data,
            iter = 2000,
            warmup = 1000,
            chains = 4,
            seed = 123,
            control = list(adapt_delta = 0.99, 
                           max_treedepth = 15))
```

## Interpret Parameters

- `init`: estimated initial state probabilities.

- `trans`: estimated transition matrix between states.

- `theta`: estimated success probabilities in each state.

```{r}
print(fit, pars = c("init", "trans", "theta"))
```

### Third Version - this is what we **currently** observe:

This version shows that we have significantly improved the model’s performance and stability compared to previous attempts. By reparameterizing the emission probabilities using a logit transformation (theta[1] = inv_logit(alpha), theta[2] = inv_logit(alpha + delta)) and adding informative priors on the transition matrix and emission separation, the sampler is now able to:

- Converge cleanly across all chains (Rhat = 1.00)

-	Avoid divergent transitions entirely

-	Accurately recover the true transition probabilities and emission values used in the simulation:

-	$\theta_1 \approx 0.2$, $\theta_2 \approx 0.8$

-	$P_{11} \approx 0.9$, $P_{22} \approx 0.9$

In contrast to earlier versions where the sampler was unstable (high Rhat, low n_eff, divergences), this version demonstrates that reparameterization and identifiability constraints are crucial when working with Hidden Markov Models in Stan.

This model can now serve as a stable basis for simulating environments and evaluating bandit algorithms in non-stationary, partially observed settings.

### Second Version - after some improvements:

Right now, Stan is struggling because:

- It’s unsure how to separate the emission probabilities.

-	There’s still symmetry in the model.

So let’s do two concrete things that will almost always help:

1. Add Informative Priors to Help Identify States

2. Add Priors on Transition Probabilities

### First Version - this is what we observed before doing any improvements:

These transition probabilities correctly recover the transition matrix we simulated!

Issues: We have a very low n_eff and High Rhat!

So, our chains did not mix well. The sampler is very uncertain about what the success probabilities should be. It’s likely stuck in different modes or swapping labels (more below).

Our model likely suffers from label switching — the sampler can’t tell whether:

-	State 1 is the “high success” state, or

- State 2 is the “high success” state

It keeps switching during sampling, making the average estimates look like:

$$
theta[1] ≈ 0.47 \\

theta[2] ≈ 0.47
$$
even though the true values were 0.8 and 0.2.

1. We will now orce the model to order the theta values so it breaks the symmetry. 

This ensures:
$$\theta[1] < \theta[2]$$
or vice versa, depending on your interpretation.

This does not affect inference, but ensures consistent labeling across chains.

2. We change the sampler by adding:

control = list(adapt_delta = 0.99, max_treedepth = 15)


# 4. Generating Simulated Data using the Stan Model

We fit a 2-state Bernoulli HMM, which is a special case of a Markov Switching Model. Now, we can use the estimated parameters from the fitted model to generate a new synthetic dataset that mimics the regime-switching structure observed in the original simulation.

```{r generate-simulated-hmm-data}
# Extract posterior means from fit
posterior <- rstan::extract(fit)
init_prob <- colMeans(posterior$init)
trans_mat <- apply(posterior$trans, c(2,3), mean)
theta_vals <- colMeans(posterior$theta)

# Simulate new state sequence and observations
set.seed(2025)
n_sim <- 500
states_sim <- numeric(n_sim)
y_sim <- numeric(n_sim)

# Initial state
states_sim[1] <- sample(1:2, 1, prob = init_prob)

# Forward simulation
for (t in 2:n_sim) {
  states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t-1], ])
}

# Generate observations
for (t in 1:n_sim) {
  y_sim[t] <- rbinom(1, 1, prob = theta_vals[states_sim[t]])
}

simulated_data <- tibble(t = 1:n_sim, state = states_sim, y = y_sim)
head(simulated_data)
```


# 6. Bandit Algorithms (UCB and Thompson Sampling)

(Note: We call the tuned UCB version simply "UCB" from now on.)

```{r bandit-algorithms}
# Initialize storage
n_bandit <- n_sim
ucb_rewards <- numeric(n_bandit)
ts_rewards <- numeric(n_bandit)

# True probabilities (hidden from bandit)
p_true <- theta_vals[states_sim]

# UCB parameters
ucb_success <- rep(0, 2)
ucb_trials <- rep(0, 2)

# Thompson parameters
ts_alpha <- rep(1, 2)
ts_beta <- rep(1, 2)

for (t in 1:n_bandit) {
  # UCB selection (tuned version)
  ucb_values <- ifelse(
    ucb_trials == 0,
    1,
    ucb_success / ucb_trials + 
      sqrt(2 * log(t) / ucb_trials)
  )
  a_ucb <- which.max(ucb_values)
  r_ucb <- rbinom(1, 1, prob = theta_vals[states_sim[t]])
  ucb_rewards[t] <- r_ucb
  ucb_success[a_ucb] <- ucb_success[a_ucb] + r_ucb
  ucb_trials[a_ucb] <- ucb_trials[a_ucb] + 1

  # Thompson Sampling selection
  ts_draws <- rbeta(2, ts_alpha, ts_beta)
  a_ts <- which.max(ts_draws)
  r_ts <- rbinom(1, 1, prob = theta_vals[states_sim[t]])
  ts_rewards[t] <- r_ts
  ts_alpha[a_ts] <- ts_alpha[a_ts] + r_ts
  ts_beta[a_ts] <- ts_beta[a_ts] + (1 - r_ts)
}

# Compute cumulative regret (assuming oracle knows true best arm at each t)
best_arm_probs <- pmax(theta_vals[states_sim], 1 - theta_vals[states_sim])
ucb_regret <- cumsum(best_arm_probs - ucb_rewards)
ts_regret <- cumsum(best_arm_probs - ts_rewards)

# Plot cumulative regret
regret_df <- tibble(
  t = 1:n_bandit,
  UCB = ucb_regret,
  Thompson = ts_regret
) %>% pivot_longer(-t)

ggplot(regret_df, aes(x = t, y = value, color = name)) +
  geom_line() +
  labs(title = "Cumulative Regret: UCB vs Thompson Sampling",
       x = "Time Step", y = "Cumulative Regret",
       color = "Algorithm") +
  theme_minimal()
```
From time step 0 to 300, their cumulative regret curves are nearly identical. This means:

-	Both algorithms adapt at roughly the same rate

-	The regime switches weren’t too punishing for either approach

-	This part of the run is relatively balanced

Between time steps 300–450, Thompson Sampling performs slightly worse

-	Thompson’s regret increases sharper than UCB’s during that stretch.

-	Suggests that in this segment, UCB may have adapted more quickly to changes in the hidden state.

-	This could be due to how UCB handles uncertainty more aggressively (through confidence bounds), while Thompson may remain more exploratory.

In the end (t = 500) both algorithms end up with similar regret, so they learned, and neither dominates.

---


# 6. Generating Simulated State Transitions

We will start generating state transitions that will guide the probabilities of each arm of the bandit problem. We initiate using a stable setup where the probability of transition is low (10 %) and the transitions between states are not very frequent:

```{r generate-simulated-hmm-data}

# I directly define a transition matrix ( we can try to get the values from stan but im not totally sure how to get the model) 

init_prob <- c(0.5,0.5) # I set initial probability of each state as equal

# On all the code I use trans_mat as transition matrix
trans_mat <- matrix(c(0.9, 0.1,
              0.1, 0.9), nrow = 2, byrow = TRUE)

# Simulate new state sequence and observations
set.seed(2025)
n_sim <- 500
states_sim <- numeric(n_sim)

# Initial state
states_sim[1] <- sample(1:2, 1, prob = init_prob)

# Forward simulation
for (t in 2:n_sim) {
  states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t-1], ])
}

# we plot the state transitions

plot(states_sim, type = "s", ylim = c(1, 2), 
     main = "State Transitions", xlab = "Time Step", ylab = "",yaxt = "n",
     col = "blue", lwd = 2)
axis(2, at = c(1, 2), labels = c("State 1", "State 2"))

```


# 7. Bandit Algorithms (UCB and Thompson Sampling)

(Note: We call the tuned UCB version simply "UCB" from now on.)

We will consider now a setting where each arm of the bandit, change its success probability depending on the general state. In this basic setting, we considered symmetric bandits, hence one arm has 80% success probability at State 1 and the other just 20%. When the state transitions to State 2 this probabilities are inverted between arms.

We start running just one simulation of the algorithms:

```{r bandit-algorithms}

# I directly define a transition matrix ( we can try to get the values from stan) 

init_prob <- c(0.5,0.5) #probability to initiate at each state

trans_mat <- matrix(c(0.9, 0.1,
              0.1, 0.9), nrow = 2, byrow = TRUE)

# we define bandit probabilities for each state [state,bandit] THAT IS THE MAIN CHANGE, each bandit has different probabilities depending on the global state
bandit_probs <- matrix(c(0.8, 0.2,
              0.2, 0.8), nrow = 2, byrow = TRUE)

# Initialize storage
n_bandit <- n_sim
ucb_rewards <- numeric(n_bandit)
ts_rewards <- numeric(n_bandit)

# UCB parameters
ucb_success <- rep(0, 2)
ucb_trials <- rep(0, 2)

# Thompson parameters
ts_alpha <- rep(1, 2)
ts_beta <- rep(1, 2)

for (t in 1:n_bandit) {
  
  # UCB selection (tuned version)
  
  ucb_values <- ifelse(
    ucb_trials == 0,
    1,
    ucb_success / ucb_trials + 
      sqrt(2 * log(t) / ucb_trials)
  )
  
  #select the arm with highest value
  a_ucb <- which.max(ucb_values)
  
  #draw a reward from this arm
  r_ucb <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a_ucb])
  
  #update
  ucb_rewards[t] <- r_ucb
  ucb_success[a_ucb] <- ucb_success[a_ucb] + r_ucb
  ucb_trials[a_ucb] <- ucb_trials[a_ucb] + 1

  # Thompson Sampling selection
  
  ts_draws <- rbeta(2, ts_alpha, ts_beta)
  
  #select the arm with highest value
  a_ts <- which.max(ts_draws)
  
  #draw a reward from this arm
  r_ts <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a_ts])
  
  #update
  ts_rewards[t] <- r_ts
  ts_alpha[a_ts] <- ts_alpha[a_ts] + r_ts
  ts_beta[a_ts] <- ts_beta[a_ts] + (1 - r_ts)
}

# Compute cumulative regret (assuming oracle knows true best arm at each t)

# we consider highest probability at each state ( in this case is easy 0.8 on any state)
best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))

ucb_regret <- cumsum(best_arm_probs - ucb_rewards)
ts_regret <- cumsum(best_arm_probs - ts_rewards)

# Store regret in a tibble for plotting
regret_df <- tibble(
  t = 1:n_bandit,
  UCB = ucb_regret,
  Thompson = ts_regret
) %>% pivot_longer(-t)

#Plot regrets

ggplot(regret_df, aes(x = t, y = value, color = name)) +
  geom_line() +
  labs(title = "Cumulative Regret: UCB vs Thompson Sampling",
       x = "Time Step", y = "Cumulative Regret",
       color = "Algorithm") +
  theme_minimal()
```

DERIVE CONCLUSIONS 

# 8. Computing Average Performance

Now, we repeat the simulation multiple times (200 runs) and plot average regret with confidence intervals.

```{r average-performance, warning=FALSE}
set.seed(2025)
n_runs <- 200
n_bandit <- 500

regret_ucb_all <- matrix(0, nrow = n_bandit, ncol = n_runs)
regret_ts_all <- matrix(0, nrow = n_bandit, ncol = n_runs)

for (r in 1:n_runs) {
  
  # Simulate new state sequence for each run
  states_sim <- numeric(n_bandit)
  states_sim[1] <- sample(1:2, 1, prob = init_prob) #initiate at one state
  for (t in 2:n_bandit) {
    states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t-1], ])
  }
  
  #compute the highest probability at each state THIS IS ALSO A MAIN CHANGE, now i take the highest probability of a GIVEN state
  best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))

  # Initialize bandit stats
  ucb_success <- rep(0, 2)
  ucb_trials <- rep(0, 2)
  ts_alpha <- rep(1, 2)
  ts_beta <- rep(1, 2)
  
  regret_ucb <- numeric(n_bandit)
  regret_ts <- numeric(n_bandit)

  
  for (t in 1:n_bandit) {
    # --- UCB ---
    ucb_values <- ifelse(
      ucb_trials == 0,
      1,
      ucb_success / ucb_trials + sqrt(2 * log(t) / ucb_trials)
    )
    a_ucb <- which.max(ucb_values)
    r_ucb <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a_ucb])
    regret_ucb[t] <- best_arm_probs[t] - r_ucb
    ucb_success[a_ucb] <- ucb_success[a_ucb] + r_ucb
    ucb_trials[a_ucb] <- ucb_trials[a_ucb] + 1

    # --- Thompson Sampling ---
    ts_draws <- rbeta(2, ts_alpha, ts_beta)
    a_ts <- which.max(ts_draws)
    r_ts <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a_ts]) # AND ALSO REWARD COMES FROM THE MATRIX
    
    regret_ts[t] <- best_arm_probs[t] - r_ts
    ts_alpha[a_ts] <- ts_alpha[a_ts] + r_ts
    ts_beta[a_ts] <- ts_beta[a_ts] + (1 - r_ts)
  }
  
  regret_ucb_all[,r] <- cumsum(regret_ucb)
  regret_ts_all[,r] <- cumsum(regret_ts)
}

# Compute average and 95% CI
regret_summary <- tibble(
  t = 1:n_bandit,
  UCB_mean = rowMeans(regret_ucb_all),
  TS_mean = rowMeans(regret_ts_all),
  UCB_sd = apply(regret_ucb_all, 1, sd),
  TS_sd = apply(regret_ts_all, 1, sd)
) %>% 
  mutate(
    UCB_lower = UCB_mean - 1.96 * UCB_sd / sqrt(n_runs),
    UCB_upper = UCB_mean + 1.96 * UCB_sd / sqrt(n_runs),
    TS_lower = TS_mean - 1.96 * TS_sd / sqrt(n_runs),
    TS_upper = TS_mean + 1.96 * TS_sd / sqrt(n_runs)
  )

regret_summary_long <- regret_summary %>%
  select(t, UCB_mean, TS_mean, UCB_lower, UCB_upper, TS_lower, TS_upper) %>%
  pivot_longer(-t, names_to = c("Algorithm", ".value"), names_pattern = "(.*)_([a-z]+)")

ggplot(regret_summary_long, aes(x = t, y = mean, color = Algorithm, fill = Algorithm)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Average Cumulative Regret over 200 Simulations",
       y = "Average Cumulative Regret", x = "Time Step") +
  theme_minimal()
```

DERIVE CONCLUSIONS

# 9. Lower Transition Stickiness

In the next steps we will start with more challenging environments. Here we first lower the transition stickiness. The first transition matrix is:
$$ P = \begin{bmatrix} 0.6 & 0.4 \\ 0.4 & 0.6 \end{bmatrix} $$

We simulate this more dynamic environment and compare the average performance of UCB and Thompson Sampling over 200 simulations.

```{r low-stickiness-performance, warning=FALSE}
# Lower stickiness transition matrix
trans_mat <- matrix(c(0.6, 0.4,
                  0.4, 0.6), nrow = 2, byrow = TRUE)
set.seed(2042)
n_runs <- 200
n_bandit <- 500

regret_ucb_all <- matrix(0, nrow = n_bandit, ncol = n_runs)
regret_ts_all <- matrix(0, nrow = n_bandit, ncol = n_runs)

for (r in 1:n_runs) {
  # Simulate new state sequence
  states_sim <- numeric(n_bandit)
  states_sim[1] <- sample(1:2, 1, prob = init_prob) #initiate at one state
  for (t in 2:n_bandit) {
    states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t-1], ])
  }
  
  best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))

  # Initialize bandit stats
  ucb_success <- rep(0, 2)
  ucb_trials <- rep(0, 2)
  ts_alpha <- rep(1, 2)
  ts_beta <- rep(1, 2)
  
  regret_ucb <- numeric(n_bandit)
  regret_ts <- numeric(n_bandit)

  
  for (t in 1:n_bandit) {
    # --- UCB ---
    ucb_values <- ifelse(
      ucb_trials == 0,
      1,
      ucb_success / ucb_trials + sqrt(2 * log(t) / ucb_trials)
    )
    a_ucb <- which.max(ucb_values)
    r_ucb <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a_ucb])
    regret_ucb[t] <- best_arm_probs[t] - r_ucb
    ucb_success[a_ucb] <- ucb_success[a_ucb] + r_ucb
    ucb_trials[a_ucb] <- ucb_trials[a_ucb] + 1

    # --- Thompson Sampling ---
    ts_draws <- rbeta(2, ts_alpha, ts_beta)
    a_ts <- which.max(ts_draws)
    r_ts <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a_ts])
    
    regret_ts[t] <- best_arm_probs[t] - r_ts
    ts_alpha[a_ts] <- ts_alpha[a_ts] + r_ts
    ts_beta[a_ts] <- ts_beta[a_ts] + (1 - r_ts)
  }
  
  regret_ucb_all[,r] <- cumsum(regret_ucb)
  regret_ts_all[,r] <- cumsum(regret_ts)
}

# Compute average and 95% CI
regret_summary <- tibble(
  t = 1:n_bandit,
  UCB_mean = rowMeans(regret_ucb_all),
  TS_mean = rowMeans(regret_ts_all),
  UCB_sd = apply(regret_ucb_all, 1, sd),
  TS_sd = apply(regret_ts_all, 1, sd)
) %>% 
  mutate(
    UCB_lower = UCB_mean - 1.96 * UCB_sd / sqrt(n_runs),
    UCB_upper = UCB_mean + 1.96 * UCB_sd / sqrt(n_runs),
    TS_lower = TS_mean - 1.96 * TS_sd / sqrt(n_runs),
    TS_upper = TS_mean + 1.96 * TS_sd / sqrt(n_runs)
  )

regret_summary_long <- regret_summary %>%
  select(t, UCB_mean, TS_mean, UCB_lower, UCB_upper, TS_lower, TS_upper) %>%
  pivot_longer(-t, names_to = c("Algorithm", ".value"), names_pattern = "(.*)_([a-z]+)")

ggplot(regret_summary_long, aes(x = t, y = mean, color = Algorithm, fill = Algorithm)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Average Cumulative Regret over 100 Simulations",
       y = "Average Cumulative Regret", x = "Time Step") +
  theme_minimal()
```

# 10. Implementing Sliding Window UCB

Now, we will make the UCB algorithm focus only on recent observations using a **sliding window**. We begin with window sizes of **10**, **20**, and **100**. This can help UCB adapt more quickly to dynamic environments by ignoring outdated information, which is particularly useful when the reward distribution changes due to hidden state switching.

We evaluate the performance of each sliding window version alongside standard UCB and Thompson Sampling.

```{r sliding-window-ucb, warning=FALSE}
run_sliding_ucb <- 
  function(window_size, P_matrix, n_runs = 200, n_bandit = 500) {
    
  regret_ucb_all <- matrix(0, nrow = n_bandit, ncol = n_runs)

  for (r in 1:n_runs) {
    
    states_sim <- numeric(n_bandit)
    states_sim[1] <- sample(1:2, 1, prob = init_prob)
    
    for (t in 2:n_bandit) {
      states_sim[t] <- sample(1:2, 1, prob = P_matrix[states_sim[t-1], ])
    }

    best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))

    # Store past observations
    history <- list(arm = integer(), reward = numeric())
    regret_ucb <- numeric(n_bandit)

    for (t in 1:n_bandit) {
      ucb_values <- numeric(2)
      for (a in 1:2) {
        # Filter sliding window for arm a
        idx <- which(history$arm == a)
        idx <- tail(idx, window_size)
        n_trials <- length(idx)
        successes <- sum(history$reward[idx])

        if (n_trials == 0) {
          ucb_values[a] <- 1  # optimistic initialization
        } else {
          mean_r <- successes / n_trials
          ucb_values[a] <- mean_r + sqrt(2 * log(t) / n_trials)
        }
      }

      a_ucb <- which.max(ucb_values)
      r_ucb <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a_ucb])
      regret_ucb[t] <- best_arm_probs[t] - r_ucb
      history$arm <- c(history$arm, a_ucb)
      history$reward <- c(history$reward, r_ucb)
    }

    regret_ucb_all[,r] <- cumsum(regret_ucb)
  }

  # Summary stats
  tibble(
    t = 1:n_bandit,
    regret_mean = rowMeans(regret_ucb_all),
    regret_sd = apply(regret_ucb_all, 1, sd)
  ) %>%
    mutate(
      lower = regret_mean - 1.96 * regret_sd / sqrt(n_runs),
      upper = regret_mean + 1.96 * regret_sd / sqrt(n_runs),
      Window = paste0("SW-UCB (", window_size, ")")
    )
}

# Run for multiple window sizes
trans_mat <- matrix(c(0.6, 0.4, 0.4, 0.6), nrow = 2, byrow = TRUE)
sw_ucb_10 <- run_sliding_ucb(10, trans_mat)
sw_ucb_20 <- run_sliding_ucb(20, trans_mat)
sw_ucb_100 <- run_sliding_ucb(100, trans_mat)

regret_all_sw <- bind_rows(sw_ucb_10, sw_ucb_20, sw_ucb_100)

ggplot(regret_all_sw, aes(x = t, y = regret_mean, color = Window, fill = Window)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Sliding Window UCB Performance (Low Stickiness)",
       x = "Time Step", y = "Average Cumulative Regret") +
  theme_minimal()
```


# 11. Adding a Third Arm

Next, we add a third arm to increase the complexity of the bandit environment. This allows us to explore how each algorithm handles more challenging exploration-exploitation trade-offs. We'll simulate a 3-arm setting and compare the performance of **standard UCB**, **Thompson Sampling**, and **Sliding Window UCB (window = 20)**. Also, we increased it to 1000 runs, since for 100 we had not as clear results yet.

```{r simulate-3arm-and-compare, warning=FALSE}

set.seed(2043)
n_runs <- 1000
n_bandit <- 500

# Define new success probabilities for 3 arms now we add another column to the matrix
bandit_probs <- matrix(c(0.8, 0.5, 0.2,
              0.2, 0.4, 0.8), nrow = 2, byrow = TRUE)

# Transition matrix (still 2-state HMM determining active best arm)
trans_mat <- matrix(c(0.6, 0.4,
                0.4, 0.6), nrow = 2, byrow = TRUE)

run_bandit_3arm <- function(algorithm = "ucb", window_size = 20) {
  
  regret_mat <- matrix(0, nrow = n_bandit, ncol = n_runs)

  for (r in 1:n_runs) {
    # Simulate state sequence and arm mapping
    
    states_sim <- numeric(n_bandit)
    arms <- numeric(n_bandit)
    rewards <- numeric(n_bandit)
    
    states_sim[1] <- sample(1:2, 1)
    for (t in 2:n_bandit) {
      states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t-1], ])
    }
    
    # Generate optimal arms by state 
    best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))
    
    # Algorithm setup
    if (algorithm == "ucb") {
      success <- rep(0, 3)
      trials <- rep(0, 3)
    } else if (algorithm == "ts") {
      alpha <- rep(1, 3)
      beta <- rep(1, 3)
    } else if (algorithm == "sw_ucb") {
      history <- list(arm = integer(), reward = numeric())
    }

    regrets <- numeric(n_bandit)

    for (t in 1:n_bandit) {
      if (algorithm == "ucb") {
        values <- ifelse(trials == 0, 1,
                         success / trials + sqrt(2 * log(t) / trials))
        a <- which.max(values)
      } else if (algorithm == "ts") {
        values <- rbeta(3, alpha, beta)
        a <- which.max(values)
      } else if (algorithm == "sw_ucb") {
        values <- numeric(3)
        for (i in 1:3) {
          idx <- which(history$arm == i)
          idx <- tail(idx, window_size)
          n_trials <- length(idx)
          s <- sum(history$reward[idx])
          values[i] <- ifelse(n_trials == 0, 1, s/n_trials + sqrt(2 * log(t) / n_trials))
        }
        a <- which.max(values)
      }

      reward <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a]) #get reward using probability from the matrix
      regrets[t] <- best_arm_probs[t] - reward

      if (algorithm == "ucb") {
        success[a] <- success[a] + reward
        trials[a] <- trials[a] + 1
      } else if (algorithm == "ts") {
        alpha[a] <- alpha[a] + reward
        beta[a] <- beta[a] + (1 - reward)
      } else if (algorithm == "sw_ucb") {
        history$arm <- c(history$arm, a)
        history$reward <- c(history$reward, reward)
      }
    }

    regret_mat[, r] <- cumsum(regrets)
  }

  return(regret_mat)
}

regret_ucb_3 <- run_bandit_3arm("ucb")
regret_ts_3 <- run_bandit_3arm("ts")
regret_swucb_3 <- run_bandit_3arm("sw_ucb")

t <- 1:n_bandit
regret_df_3arm <- tibble(
  t = rep(t, 3),
  regret = c(rowMeans(regret_ucb_3), rowMeans(regret_ts_3), rowMeans(regret_swucb_3)),
  sd = c(apply(regret_ucb_3, 1, sd), apply(regret_ts_3, 1, sd), apply(regret_swucb_3, 1, sd)),
  method = rep(c("UCB", "Thompson", "SW-UCB (20)"), each = n_bandit)
) %>%
  mutate(lower = regret - 1.96 * sd / sqrt(n_runs),
         upper = regret + 1.96 * sd / sqrt(n_runs))

ggplot(regret_df_3arm, aes(x = t, y = regret, color = method, fill = method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Cumulative Regret: UCB vs TS vs Sliding UCB (3 Arms, Window = 20)",
       x = "Time Step", y = "Average Cumulative Regret") +
  theme_minimal()
```
In the 3-arm environment, all algorithms demonstrate effective adaptation, with cumulative regret surprisingly decreasing over time. This may indicate that, on average, the algorithms are outperforming the per-step oracle due to consistent identification of a robustly good arm. Thompson Sampling and Sliding Window UCB (window = 20) exhibit slightly lower regret than standard UCB, though all three perform similarly overall.

# 12. Adding a Fourth Arm

To further increase the complexity of the bandit environment, we now add a **fourth arm**. This allows us to evaluate how well each algorithm scales with increased action space.

We define success probabilities for four arms and simulate a dynamic environment as before. We'll compare **UCB**, **Thompson Sampling**, **Sliding Window UCB (window = 20)**, and **Sliding Window UCB (window = 50)** (this we added since we observed that 20 may be too low).

```{r simulate-4arm-and-compare, warning=FALSE}
set.seed(2044)
n_runs <- 1000
n_bandit <- 500

# Define new success probabilities for 4 arms NOW WE ADD ANOTHER COLUMN
bandit_probs <- matrix(c(0.8, 0.5, 0.2, 0.6,
              0.2, 0.4, 0.8,0.6), nrow = 2, byrow = TRUE)

# Transition matrix (2-state HMM switching between optimal arms)
trans_mat <- matrix(c(0.6, 0.4,
                0.4, 0.6), nrow = 2, byrow = TRUE)

run_bandit_4arm <- function(algorithm = "ucb", window_size = 20) {
  
  regret_mat <- matrix(0, nrow = n_bandit, ncol = n_runs)

  for (r in 1:n_runs) {
    states_sim <- numeric(n_bandit)
    states_sim[1] <- sample(1:2, 1)
    for (t in 2:n_bandit) {
      states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t-1], ])
    }

    best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))

    if (algorithm == "ucb") {
      success <- rep(0, 4)
      trials <- rep(0, 4)
    } else if (algorithm == "ts") {
      alpha <- rep(1, 4)
      beta <- rep(1, 4)
    } else if (grepl("sw_ucb", algorithm)) {
      history <- list(arm = integer(), reward = numeric())
    }

    regrets <- numeric(n_bandit)

    for (t in 1:n_bandit) {
      if (algorithm == "ucb") {
        values <- ifelse(trials == 0, 1,
                         success / trials + sqrt(2 * log(t) / trials))
        a <- which.max(values)
      } else if (algorithm == "ts") {
        values <- rbeta(4, alpha, beta)
        a <- which.max(values)
      } else if (grepl("sw_ucb", algorithm)) {
        values <- numeric(4)
        for (i in 1:4) {
          idx <- which(history$arm == i)
          idx <- tail(idx, window_size)
          n_trials <- length(idx)
          s <- sum(history$reward[idx])
          values[i] <- ifelse(n_trials == 0, 1, s/n_trials + sqrt(2 * log(t) / n_trials))
        }
        a <- which.max(values)
      }

      reward <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a])
      regrets[t] <- best_arm_probs[t] - reward

      if (algorithm == "ucb") {
        success[a] <- success[a] + reward
        trials[a] <- trials[a] + 1
      } else if (algorithm == "ts") {
        alpha[a] <- alpha[a] + reward
        beta[a] <- beta[a] + (1 - reward)
      } else if (grepl("sw_ucb", algorithm)) {
        history$arm <- c(history$arm, a)
        history$reward <- c(history$reward, reward)
      }
    }

    regret_mat[, r] <- cumsum(regrets)
  }

  return(regret_mat)
}

regret_ucb_4 <- run_bandit_4arm("ucb")
regret_ts_4 <- run_bandit_4arm("ts")
regret_swucb_20 <- run_bandit_4arm("sw_ucb", window_size = 20)
regret_swucb_50 <- run_bandit_4arm("sw_ucb", window_size = 50)

t <- 1:n_bandit
regret_df_4arm <- tibble(
  t = rep(t, 4),
  regret = c(rowMeans(regret_ucb_4),
             rowMeans(regret_ts_4),
             rowMeans(regret_swucb_20),
             rowMeans(regret_swucb_50)),
  sd = c(apply(regret_ucb_4, 1, sd),
         apply(regret_ts_4, 1, sd),
         apply(regret_swucb_20, 1, sd),
         apply(regret_swucb_50, 1, sd)),
  method = rep(c("UCB", "Thompson", "SW-UCB (20)", "SW-UCB (50)"), each = n_bandit)
) %>%
  mutate(lower = regret - 1.96 * sd / sqrt(n_runs),
         upper = regret + 1.96 * sd / sqrt(n_runs))

ggplot(regret_df_4arm, aes(x = t, y = regret, color = method, fill = method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Cumulative Regret: UCB vs TS vs Sliding UCB (4 Arms)",
       x = "Time Step", y = "Average Cumulative Regret") +
  theme_minimal()
```



# 13. Simulating Abrupt Switches

To test how well the algorithms adapt to rapidly changing environments, we now simulate **abrupt switching dynamics**. This is done by modifying the transition matrix to increase the switching probability.

We use:
$$ P = \begin{bmatrix} 0.3 & 0.7 \\ 0.7 & 0.3 \end{bmatrix} $$

This setup introduces **frequent state changes**, simulating more volatile environments. We compare UCB, Thompson Sampling, and Sliding Window UCB with windows of 20 and 50.

```{r simulate-abrupt-switching, warning=FALSE}
set.seed(2045)
n_runs <- 1000
n_bandit <- 500

# Define new success probabilities for 4 arms
bandit_probs <- matrix(c(0.8, 0.5, 0.2, 0.6,
              0.2, 0.4, 0.8,0.6), nrow = 2, byrow = TRUE)

# Transition matrix (2-state HMM switching between optimal arms)
trans_mat <- matrix(c(0.3, 0.7,
                     0.7, 0.3), nrow = 2, byrow = TRUE)

run_bandit_4arm <- function(algorithm = "ucb", window_size = 20) {
  
  regret_mat <- matrix(0, nrow = n_bandit, ncol = n_runs)

  for (r in 1:n_runs) {
    states_sim <- numeric(n_bandit)
    states_sim[1] <- sample(1:2, 1)
    for (t in 2:n_bandit) {
      states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t-1], ])
    }

    best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))

    if (algorithm == "ucb") {
      success <- rep(0, 4)
      trials <- rep(0, 4)
    } else if (algorithm == "ts") {
      alpha <- rep(1, 4)
      beta <- rep(1, 4)
    } else if (grepl("sw_ucb", algorithm)) {
      history <- list(arm = integer(), reward = numeric())
    }

    regrets <- numeric(n_bandit)

    for (t in 1:n_bandit) {
      if (algorithm == "ucb") {
        values <- ifelse(trials == 0, 1,
                         success / trials + sqrt(2 * log(t) / trials))
        a <- which.max(values)
      } else if (algorithm == "ts") {
        values <- rbeta(4, alpha, beta)
        a <- which.max(values)
      } else if (grepl("sw_ucb", algorithm)) {
        values <- numeric(4)
        for (i in 1:4) {
          idx <- which(history$arm == i)
          idx <- tail(idx, window_size)
          n_trials <- length(idx)
          s <- sum(history$reward[idx])
          values[i] <- ifelse(n_trials == 0, 1, s/n_trials + sqrt(2 * log(t) / n_trials))
        }
        a <- which.max(values)
      }

      reward <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a])
      regrets[t] <- best_arm_probs[t] - reward

      if (algorithm == "ucb") {
        success[a] <- success[a] + reward
        trials[a] <- trials[a] + 1
      } else if (algorithm == "ts") {
        alpha[a] <- alpha[a] + reward
        beta[a] <- beta[a] + (1 - reward)
      } else if (grepl("sw_ucb", algorithm)) {
        history$arm <- c(history$arm, a)
        history$reward <- c(history$reward, reward)
      }
    }

    regret_mat[, r] <- cumsum(regrets)
  }

  return(regret_mat)
}

regret_ucb_4 <- run_bandit_4arm("ucb")
regret_ts_4 <- run_bandit_4arm("ts")
regret_swucb_20 <- run_bandit_4arm("sw_ucb", window_size = 20)
regret_swucb_50 <- run_bandit_4arm("sw_ucb", window_size = 50)

t <- 1:n_bandit
regret_df_4arm <- tibble(
  t = rep(t, 4),
  regret = c(rowMeans(regret_ucb_4),
             rowMeans(regret_ts_4),
             rowMeans(regret_swucb_20),
             rowMeans(regret_swucb_50)),
  sd = c(apply(regret_ucb_4, 1, sd),
         apply(regret_ts_4, 1, sd),
         apply(regret_swucb_20, 1, sd),
         apply(regret_swucb_50, 1, sd)),
  method = rep(c("UCB", "Thompson", "SW-UCB (20)", "SW-UCB (50)"), each = n_bandit)
) %>%
  mutate(lower = regret - 1.96 * sd / sqrt(n_runs),
         upper = regret + 1.96 * sd / sqrt(n_runs))

ggplot(regret_df_4arm, aes(x = t, y = regret, color = method, fill = method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Cumulative Regret: UCB vs TS vs Sliding UCB (4 Arms)",
       x = "Time Step", y = "Average Cumulative Regret") +
  theme_minimal()
```
With more frequent switching between reward states, the landscape becomes much more volatile - and the effect on algorithm performance is clear:

-	Thompson Sampling shows the most stable and adaptive behavior, achieving the lowest cumulative regret. This reflects its inherent flexibility and ability to quickly adjust to changing reward distributions. (Although, note that its preformance is not significantly better.)

- Sliding Window UCB (both 20 and 50) performs comparably to UCB, with window = 50 slightly outperforming window = 20. This suggests that a moderate memory span is sufficient to adapt quickly without overreacting to noise.

-	Standard UCB could be seen as lagging behind all others, struggling to keep up with the fast switching due to its reliance on longer-term averages.

---
In environments with rapid change, Bayesian sampling methods like TS and memory-limited strategies like SW-UCB outperform traditional UCB by staying reactive and localized.
---

# 14. Non-Symmetric Transitions

Next, we explore **non-symmetric transition dynamics** where the environment is **biased toward one state**. This tests whether algorithms can adapt when one arm tends to dominate more persistently.

We use a biased transition matrix:
$$ P = \begin{bmatrix} 0.9 & 0.1 \\ 0.4 & 0.6 \end{bmatrix} $$
This means the environment tends to **remain longer in state 1**, favoring one arm more often.

```{r simulate-non-symmetric, warning=FALSE}
set.seed(2046)
n_runs <- 1000
n_bandit <- 500

# Define new success probabilities for 4 arms
bandit_probs <- matrix(c(0.8, 0.5, 0.2, 0.6,
              0.2, 0.4, 0.8,0.6), nrow = 2, byrow = TRUE)

# Transition matrix (2-state HMM switching between optimal arms)
trans_mat <- matrix(c(0.9, 0.1,
                     0.4, 0.6), nrow = 2, byrow = TRUE)

run_bandit_4arm <- function(algorithm = "ucb", window_size = 20) {
  
  regret_mat <- matrix(0, nrow = n_bandit, ncol = n_runs)

  for (r in 1:n_runs) {
    states_sim <- numeric(n_bandit)
    states_sim[1] <- sample(1:2, 1)
    for (t in 2:n_bandit) {
      states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t-1], ])
    }

    best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))

    if (algorithm == "ucb") {
      success <- rep(0, 4)
      trials <- rep(0, 4)
    } else if (algorithm == "ts") {
      alpha <- rep(1, 4)
      beta <- rep(1, 4)
    } else if (grepl("sw_ucb", algorithm)) {
      history <- list(arm = integer(), reward = numeric())
    }

    regrets <- numeric(n_bandit)

    for (t in 1:n_bandit) {
      if (algorithm == "ucb") {
        values <- ifelse(trials == 0, 1,
                         success / trials + sqrt(2 * log(t) / trials))
        a <- which.max(values)
      } else if (algorithm == "ts") {
        values <- rbeta(4, alpha, beta)
        a <- which.max(values)
      } else if (grepl("sw_ucb", algorithm)) {
        values <- numeric(4)
        for (i in 1:4) {
          idx <- which(history$arm == i)
          idx <- tail(idx, window_size)
          n_trials <- length(idx)
          s <- sum(history$reward[idx])
          values[i] <- ifelse(n_trials == 0, 1, s/n_trials + sqrt(2 * log(t) / n_trials))
        }
        a <- which.max(values)
      }

      reward <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a])
      regrets[t] <- best_arm_probs[t] - reward

      if (algorithm == "ucb") {
        success[a] <- success[a] + reward
        trials[a] <- trials[a] + 1
      } else if (algorithm == "ts") {
        alpha[a] <- alpha[a] + reward
        beta[a] <- beta[a] + (1 - reward)
      } else if (grepl("sw_ucb", algorithm)) {
        history$arm <- c(history$arm, a)
        history$reward <- c(history$reward, reward)
      }
    }

    regret_mat[, r] <- cumsum(regrets)
  }

  return(regret_mat)
}

regret_ucb_4 <- run_bandit_4arm("ucb")
regret_ts_4 <- run_bandit_4arm("ts")
regret_swucb_20 <- run_bandit_4arm("sw_ucb", window_size = 20)
regret_swucb_50 <- run_bandit_4arm("sw_ucb", window_size = 50)

t <- 1:n_bandit
regret_df_4arm <- tibble(
  t = rep(t, 4),
  regret = c(rowMeans(regret_ucb_4),
             rowMeans(regret_ts_4),
             rowMeans(regret_swucb_20),
             rowMeans(regret_swucb_50)),
  sd = c(apply(regret_ucb_4, 1, sd),
         apply(regret_ts_4, 1, sd),
         apply(regret_swucb_20, 1, sd),
         apply(regret_swucb_50, 1, sd)),
  method = rep(c("UCB", "Thompson", "SW-UCB (20)", "SW-UCB (50)"), each = n_bandit)
) %>%
  mutate(lower = regret - 1.96 * sd / sqrt(n_runs),
         upper = regret + 1.96 * sd / sqrt(n_runs))

ggplot(regret_df_4arm, aes(x = t, y = regret, color = method, fill = method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Cumulative Regret: UCB vs TS vs Sliding UCB (4 Arms)",
       x = "Time Step", y = "Average Cumulative Regret") +
  theme_minimal()
```


# 15a. Discounted UCB

We now test a **Discounted UCB** variant, where older observations are **exponentially down-weighted**. This allows the algorithm to stay responsive in dynamic environments without using a hard cutoff like in Sliding Window UCB.

We define the **discount factor** $\gamma \in (0, 1)$. Recent data gets weight $\approx 1$, older data gets weight $\gamma^t$.

```{r discounted-ucb, warning=FALSE}
set.seed(2047)
n_runs <- 1000
n_bandit <- 500

bandit_probs <- matrix(c(0.8, 0.5, 0.2, 0.6,
              0.2, 0.4, 0.8,0.6), nrow = 2, byrow = TRUE)

trans_mat <- matrix(c(0.6, 0.4,
                     0.4, 0.6), nrow = 2, byrow = TRUE)

run_discounted_ucb <- function(gamma = 0.97) {
  
  regret_mat <- matrix(0, nrow = n_bandit, ncol = n_runs)

  for (r in 1:n_runs) {
    states_sim <- numeric(n_bandit)
    states_sim[1] <- sample(1:2, 1)
    for (t in 2:n_bandit) {
      states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t-1], ])
    }

    best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))

    # Initialize discounted stats
    successes <- rep(0, 4)
    weights <- rep(0, 4)

    regrets <- numeric(n_bandit)

    for (t in 1:n_bandit) {
      # Compute UCB with discounting
      values <- ifelse(weights == 0, 1,
                       successes / weights + sqrt(2 * log(t) / weights))
      a <- which.max(values)

      rwd <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a])
      regrets[t] <- best_arm_probs[t] - rwd

      # Discount all past data
      successes <- gamma * successes
      weights <- gamma * weights

      # Update current arm
      successes[a] <- successes[a] + rwd
      weights[a] <- weights[a] + 1
    }

    regret_mat[, r] <- cumsum(regrets)
  }

  return(regret_mat)
}

regret_ucb_std <- run_bandit_4arm("ucb")
regret_ts_std <- run_bandit_4arm("ts")
regret_sw20_std <- run_bandit_4arm("sw_ucb", window_size = 20)
regret_discucb <- run_discounted_ucb(gamma = 0.97)

t <- 1:n_bandit
regret_df_disc <- tibble(
  t = rep(t, 4),
  regret = c(rowMeans(regret_ucb_std),
             rowMeans(regret_ts_std),
             rowMeans(regret_sw20_std),
             rowMeans(regret_discucb)),
  sd = c(apply(regret_ucb_std, 1, sd),
         apply(regret_ts_std, 1, sd),
         apply(regret_sw20_std, 1, sd),
         apply(regret_discucb, 1, sd)),
  method = rep(c("UCB", "Thompson", "SW-UCB (20)", "Discounted-UCB"), each = n_bandit)
) %>%
  mutate(lower = regret - 1.96 * sd / sqrt(n_runs),
         upper = regret + 1.96 * sd / sqrt(n_runs))

ggplot(regret_df_disc, aes(x = t, y = regret, color = method, fill = method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Cumulative Regret: Discounted UCB vs Baselines",
       x = "Time Step", y = "Average Cumulative Regret") +
  theme_minimal()
```

# 15b. Fixing Discounted UCB

Let's revise our implementation of **Discounted UCB** to ensure that recent information is properly weighted without prematurely discounting new observations.

Key adjustments:
- **Discount after update** instead of before.
- Maintain **discounted rewards and counts** separately for stability.
- **Fix regret calculation**: Regret is always \( \text{optimal reward} - \text{observed reward} \)

```{r fixed-discounted-ucb, warning=FALSE}
run_discounted_ucb_fixed <- function(gamma = 0.99) {
  regret_mat <- matrix(0, nrow = n_bandit, ncol = n_runs)

  for (r in 1:n_runs) {
    states_sim <- numeric(n_bandit)
    states_sim[1] <- sample(1:2, 1)
    for (t in 2:n_bandit) {
      states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t-1], ])
    }

    best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))

    rew <- rep(0, 4)
    count <- rep(0, 4)

    regrets <- numeric(n_bandit)

    for (t in 1:n_bandit) {
      ucb <- ifelse(count == 0, 1,
                   rew / count + sqrt(2 * log(t + 1) / count))
      a <- which.max(ucb)

      rwd <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a])

      # FIXED REGRET CALCULATION
      regrets[t] <- best_arm_probs[t] - rwd

      # Discount all arms
      rew <- gamma * rew
      count <- gamma * count

      # Update played arm
      rew[a] <- rew[a] + rwd
      count[a] <- count[a] + 1
    }

    regret_mat[, r] <- cumsum(regrets)
  }

  return(regret_mat)
}

regret_discucb_fix <- run_discounted_ucb_fixed(gamma = 0.99)

regret_df_discfix <- tibble(
  t = rep(t, 4),
  regret = c(rowMeans(regret_ucb_std),
             rowMeans(regret_ts_std),
             rowMeans(regret_sw20_std),
             rowMeans(regret_discucb_fix)),
  sd = c(apply(regret_ucb_std, 1, sd),
         apply(regret_ts_std, 1, sd),
         apply(regret_sw20_std, 1, sd),
         apply(regret_discucb_fix, 1, sd)),
  method = rep(c("UCB", "Thompson", "SW-UCB (20)", "Discounted-UCB (Fixed)"), each = n_bandit)
) %>%
  mutate(lower = regret - 1.96 * sd / sqrt(n_runs),
         upper = regret + 1.96 * sd / sqrt(n_runs))

ggplot(regret_df_discfix, aes(x = t, y = regret, color = method, fill = method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Cumulative Regret: Discounted UCB (Fixed) vs Baselines",
       x = "Time Step", y = "Average Cumulative Regret") +
  theme_minimal()
```


# 15c. Discounted UCB with State-Aligned Optimal Reward

We now revise the regret calculation to explicitly account for the current **latent state**. This ensures we always compare against the **best arm given the state**, rather than against a global best.

```{r discounted-ucb-state-aligned, warning=FALSE}
run_discounted_ucb_state_aligned <- function(gamma = 0.99) {
  regret_mat <- matrix(0, nrow = n_bandit, ncol = n_runs)

  for (r in 1:n_runs) {
    # Simulate Markov states
    states_sim <- numeric(n_bandit)
    states_sim[1] <- sample(1:2, 1)
    for (t in 2:n_bandit) {
      states_sim[t] <- sample(1:2, 1, prob = P_normal[states_sim[t - 1], ])
    }

    # Best arm per state (example: state 1 → arm 4, state 2 → arm 1)
    best_arm_by_state <- list(`1` = 4, `2` = 1)

    rew <- rep(0, 4)
    count <- rep(0, 4)
    regrets <- numeric(n_bandit)

    for (t in 1:n_bandit) {
      s <- states_sim[t]
      best_prob <- theta_vals_4[best_arm_by_state[[as.character(s)]]]

      # Calculate UCB
      ucb <- ifelse(count == 0, 1,
                   rew / count + sqrt(2 * log(t + 1) / count))
      a <- which.max(ucb)

      reward <- rbinom(1, 1, theta_vals_4[a])
      regrets[t] <- best_prob - theta_vals_4[a]

      # Discount all stats
      rew <- gamma * rew
      count <- gamma * count

      # Update played arm
      rew[a] <- rew[a] + reward
      count[a] <- count[a] + 1
    }

    regret_mat[, r] <- cumsum(regrets)
  }

  return(regret_mat)
}

regret_discucb_state <- run_discounted_ucb_state_aligned(gamma = 0.99)

regret_df_discstate <- tibble(
  t = rep(t, 5),
  regret = c(rowMeans(regret_ucb_std),
             rowMeans(regret_ts_std),
             rowMeans(regret_sw20_std),
             rowMeans(regret_discucb_fix),
             rowMeans(regret_discucb_state)),
  sd = c(apply(regret_ucb_std, 1, sd),
         apply(regret_ts_std, 1, sd),
         apply(regret_sw20_std, 1, sd),
         apply(regret_discucb_fix, 1, sd),
         apply(regret_discucb_state, 1, sd)),
  method = rep(c("UCB", "Thompson", "SW-UCB (20)", "Discounted-UCB (Fixed)", "Discounted-UCB (StateAligned)"), each = n_bandit)
) %>%
  mutate(lower = regret - 1.96 * sd / sqrt(n_runs),
         upper = regret + 1.96 * sd / sqrt(n_runs))

ggplot(regret_df_discstate, aes(x = t, y = regret, color = method, fill = method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Cumulative Regret: Discounted UCB (State-Aligned) vs Baselines",
       x = "Time Step", y = "Average Cumulative Regret") +
  theme_minimal()
```
Discounted-UCB (StateAligned) now behaves logically — it starts with higher regret (due to exploration) but eventually adapts and drives cumulative regret sharply downward, even below zero (more on that below).

The standard baselines (UCB, TS, SW-UCB) remain flat at zero, as expected with consistent state-aware regret.

Interestingly, both Discounted-UCB versions overtake the oracle on average later in time — this could mean they:

-	Lock into a good arm more aggressively than the state-based switching optimal arm.

-	Or, more likely: there’s a mismatch between the simulation’s latent state and what we assume to be the “best” at each time.


# 16. Different Reward Factors

Now we explore how different reward gaps between arms affect algorithm performance. We simulate a 4-arm environment with more subtle differences:

```{r simulate-small-gap-rewards, warning=FALSE}
# Define smaller reward gap between arms
bandit_probs <- matrix(c(0.6, 0.5, 0.55, 0.45,
              0.5, 0.4, 0.8,0.6), nrow = 2, byrow = TRUE)

# Simulate 1000 runs and store regret curves
run_bandits_with_gap <- function(reward_probs) {
  regret_ucb <- matrix(0, n_bandit, n_runs)
  regret_ts <- matrix(0, n_bandit, n_runs)
  regret_sw20 <- matrix(0, n_bandit, n_runs)

  for (r in 1:n_runs) {
    # Simulate latent states
    states_sim <- numeric(n_bandit)
    states_sim[1] <- sample(1:2, 1)
    for (t in 2:n_bandit) {
      states_sim[t] <- sample(1:2, 1, prob = trans_mat[states_sim[t - 1], ])
    }

    best_arm_probs <- sapply(states_sim, function(i) max(bandit_probs[i, ]))

    simulate_algos <- function(strategy_fun) {
      rew <- rep(0, 4)
      count <- rep(0, 4)
      regrets <- numeric(n_bandit)
      for (t in 1:n_bandit) {
        a <- strategy_fun(rew, count, t)
        reward <- rbinom(1, 1, prob = bandit_probs[states_sim[t],a])
        regrets[t] <- best_arm_probs[t] - reward
        rew[a] <- rew[a] + reward
        count[a] <- count[a] + 1
      }
      cumsum(regrets)
    }

    regret_ucb[, r] <- simulate_algos(function(r, c, t) {
      ucb <- ifelse(c == 0, 1, r / c + sqrt(2 * log(t + 1) / c))
      which.max(ucb)
    })

    regret_ts[, r] <- simulate_algos(function(r, c, t) {
      alpha <- 1 + r
      beta <- 1 + c - r
      which.max(rbeta(4, alpha, beta))
    })

    regret_sw20[, r] <- simulate_algos(function(r, c, t) {
      recent <- max(1, t - 20 + 1):t
      arm_rewards <- sapply(1:4, function(a) {
        rew <- 0; n <- 0
        for (i in recent) {
          if (a == which.max(rbeta(4, 1 + r, 1 + c - r))) {
            rew <- rew + 1; n <- n + 1
          }
        }
        if (n == 0) return(1) else return(rew / n + sqrt(2 * log(t + 1) / n))
      })
      which.max(arm_rewards)
    })
  }

  list(ucb = regret_ucb, ts = regret_ts, sw = regret_sw20)
}

regrets_small_gap <- run_bandits_with_gap(bandits_probs)

regret_df_gap <- tibble(
  t = rep(t, 3),
  regret = c(rowMeans(regrets_small_gap$ts),
             rowMeans(regrets_small_gap$ucb),
             rowMeans(regrets_small_gap$sw)),
  sd = c(apply(regrets_small_gap$ts, 1, sd),
         apply(regrets_small_gap$ucb, 1, sd),
         apply(regrets_small_gap$sw, 1, sd)),
  method = rep(c("Thompson", "UCB", "SW-UCB (20)"), each = n_bandit)
) %>%
  mutate(lower = regret - 1.96 * sd / sqrt(n_runs),
         upper = regret + 1.96 * sd / sqrt(n_runs))

ggplot(regret_df_gap, aes(x = t, y = regret, color = method, fill = method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Cumulative Regret under Smaller Reward Gaps",
       x = "Time Step", y = "Average Cumulative Regret") +
  theme_minimal()
```
This experiment simulates a 4-armed bandit with narrow gaps between arm success probabilities:
$[0.55,\ 0.5,\ 0.45,\ 0.4]$

In such settings:

Thompson Sampling (green) performs significantly better, with much lower cumulative regret across time. It benefits from its Bayesian exploration-exploitation tradeoff, allowing it to resolve subtle differences between arms more efficiently.

UCB (blue) and Sliding Window UCB (red) show higher regret, especially in early steps.

-	This is because UCB’s confidence bounds are more optimistic and less adaptive when differences are small.

-	Sliding Window UCB is slightly better early on (quicker adaptation), but ultimately converges to similar performance as UCB, indicating its limitation in this tighter reward environment.

---

- Narrow reward gaps are hard: Distinguishing between 0.55 and 0.5 takes many more samples, and strategies that rely heavily on confidence intervals (like UCB) may struggle.

-	Thompson Sampling wins in low signal-to-noise environments because it naturally incorporates uncertainty.

---


# 17. Reward Gap Shrinkage

Now, we try even smaller gaps: `[0.51, 0.5, 0.49, 0.48]` to examine how algorithms perform when the arms are nearly indistinguishable.

```{r simulate-tiny-gap-rewards, warning=FALSE}
theta_vals_tiny_gap <- c(0.51, 0.5, 0.49, 0.48)

regrets_tiny_gap <- run_bandits_with_gap(theta_vals_tiny_gap)

regret_df_tiny <- tibble(
  t = rep(t, 3),
  regret = c(rowMeans(regrets_tiny_gap$ts),
             rowMeans(regrets_tiny_gap$ucb),
             rowMeans(regrets_tiny_gap$sw)),
  sd = c(apply(regrets_tiny_gap$ts, 1, sd),
         apply(regrets_tiny_gap$ucb, 1, sd),
         apply(regrets_tiny_gap$sw, 1, sd)),
  method = rep(c("Thompson", "UCB", "SW-UCB (20)"), each = n_bandit)
) %>%
  mutate(lower = regret - 1.96 * sd / sqrt(n_runs),
         upper = regret + 1.96 * sd / sqrt(n_runs))

ggplot(regret_df_tiny, aes(x = t, y = regret, color = method, fill = method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +
  labs(title = "Cumulative Regret under Extremely Small Reward Gaps",
       x = "Time Step", y = "Average Cumulative Regret") +
  theme_minimal()
```
All algorithms accumulate regret more slowly than before, but the differences between arms are so small that distinguishing the best one becomes very difficult, especially early on.

UCB performs worst in this setting. It explores longer due to high uncertainty, which leads to more regret accumulation over time.

Thompson Sampling performs better, adapting faster to subtle differences between arms. It maintains a consistently lower regret than UCB.

Sliding Window UCB (window = 20) surprisingly performs best.

-	This suggests that focusing only on recent rewards helps in environments where gaps are minimal and noisy exploration would otherwise dominate.

-	It avoids overcommitting to long-term averages that don’t help in detecting small variations quickly.



# 18. Time-Varying Within Arm-Rewards

```{r}
# 17 Hamilton Markov‑Switching Autoregressive based on the James Douglas book. Formulations :
# — Stationary case: fixed p1,p2
set.seed(42)
n_bandit <- 500
n_runs   <- 200
p1 <- 0.7
p2 <- 0.3
best_prob <- pmax(p1,p2)

# storage
reg_ucb_stat <- reg_ts_stat <- reg_sw_stat <- 
  array(0, c(n_bandit, n_runs))
```

```{r}
for(r in 1:n_runs){
  # initialize stats
  succ_ucb <- tr_ucb <- c(0,0)
  alpha_ts <- beta_ts <- c(1,1)
  history  <- list(arm=integer(), reward=numeric())
  cum_ucb <- cum_ts <- cum_sw <- 0
  regs_ucb <- regs_ts <- regs_sw <- numeric(n_bandit)
  
  for(t in 1:n_bandit){
    probs <- c(p1, p2)
    
    # UCB
    ucbv <- ifelse(tr_ucb==0, 1,
                   succ_ucb/tr_ucb + sqrt(2*log(t)/tr_ucb))
    a_ucb <- which.max(ucbv)
    r_ucb <- rbinom(1,1,probs[a_ucb])
    tr_ucb[a_ucb]   <- tr_ucb[a_ucb]   + 1
    succ_ucb[a_ucb] <- succ_ucb[a_ucb] + r_ucb
    cum_ucb <- cum_ucb + (best_prob - probs[a_ucb])
    regs_ucb[t] <- cum_ucb
    
    # Thompson
    draw_ts <- rbeta(2, alpha_ts, beta_ts)
    a_ts    <- which.max(draw_ts)
    r_ts    <- rbinom(1,1,probs[a_ts])
    alpha_ts[a_ts] <- alpha_ts[a_ts] + r_ts
    beta_ts[a_ts]  <- beta_ts[a_ts]  + (1 - r_ts)
    cum_ts <- cum_ts + (best_prob - probs[a_ts])
    regs_ts[t] <- cum_ts
    
    # Sliding‑Window UCB (W=50)
    W <- 50
    swv <- numeric(2)
    for(a in 1:2){
      idx <- tail(which(history$arm==a), W)
      ntr <- length(idx); nsu <- sum(history$reward[idx])
      swv[a] <- if(ntr==0) 1 else nsu/ntr + sqrt(2*log(t)/ntr)
    }
    a_sw   <- which.max(swv)
    r_sw   <- rbinom(1,1,probs[a_sw])
    history$arm    <- c(history$arm, a_sw)
    history$reward <- c(history$reward, r_sw)
    cum_sw <- cum_sw + (best_prob - probs[a_sw])
    regs_sw[t] <- cum_sw
  }
  
  reg_ucb_stat[,r] <- regs_ucb
  reg_ts_stat[,r]  <- regs_ts
  reg_sw_stat[,r]  <- regs_sw
}

# summarize & plot
df_stat <- tibble(
  t       = 1:n_bandit,
  UCB     = rowMeans(reg_ucb_stat),
  Thompson= rowMeans(reg_ts_stat),
  SW_UCB  = rowMeans(reg_sw_stat)
) %>% pivot_longer(-t, names_to="Method", values_to="CumulativeRegret")

ggplot(df_stat, aes(t, CumulativeRegret, color=Method)) +
  geom_line() +
  labs(
    title="Stationary Case: Avg. Cumulative Regret",
    x="Time", y="Cumulative Regret"
  ) +
  theme_minimal()
```

```{r}
# Here im only doing for 2 arms, maybe i should be running this for more arms like 4. We can always change easily
sim <- simulate_p()      
arm1 <- sim$p 
arm2 <- simulate_p()$p
best <- ifelse(arm1>arm2,1,2)

# run one bandit experiment, storing decisions
n <- length(arm1)
ucb_choice <- ts_choice <- sw_choice <- integer(n)
ucb_reward <- ts_reward <- sw_reward <- integer(n)

# initialize...
succ_ucb <- tr_ucb <- c(0,0)
alpha_ts <- beta_ts <- c(1,1)
history <- list(arm=integer(), reward=numeric())

for(t in 1:n){
  probs <- c(arm1[t],arm2[t])
  best[t] <- which.max(probs)
  # UCB
  vals <- ifelse(tr_ucb==0,1,succ_ucb/tr_ucb+sqrt(2*log(t)/tr_ucb))
  a <- which.max(vals); ucb_choice[t]<-a
  r <- rbinom(1,1,probs[a]); ucb_reward[t]<-r
  succ_ucb[a]<-succ_ucb[a]+r; tr_ucb[a]<-tr_ucb[a]+1
  # TS
  d <- rbeta(2,alpha_ts,beta_ts); a<-which.max(d); ts_choice[t]<-a
  r <- rbinom(1,1,probs[a]); ts_reward[t]<-r
  alpha_ts[a]<-alpha_ts[a]+r; beta_ts[a]<-beta_ts[a]+(1-r)
  # SW‑UCB(50)
  W<-50; swv<-numeric(2)
  for(a in 1:2){
    idx <- tail(which(history$arm==a),W); ntr<-length(idx)
    swv[a]<- if(ntr==0) 1 else sum(history$reward[idx])/ntr+sqrt(2*log(t)/ntr)
  }
  a<-which.max(swv); sw_choice[t]<-a
  r<-rbinom(1,1,probs[a]); sw_reward[t]<-r
  history$arm    <- c(history$arm,a)
  history$reward <- c(history$reward,r)
}

df_steps <- data.frame(
  t       = 1:n,
  p1      = round(arm1,3),
  p2      = round(arm2,3),
  best    = best,
  UCB_arm = ucb_choice, UCB_r = ucb_reward,
  TS_arm  = ts_choice,  TS_r = ts_reward,
  SW_arm  = sw_choice,  SW_r = sw_reward
)
kable(head(df_steps,10), caption="First 10 Decisions of Each Algorithm")
```

```{r}
df_plot <- data.frame(
  t=1:n,
  p1=arm1, p2=arm2,
  UCB=ucb_choice,
  TS=ts_choice,
  SW=sw_choice
)

ggplot(df_plot, aes(t)) +
  geom_line(aes(y=p1), color="blue") +
  geom_line(aes(y=p2), color="red") +
  geom_point(aes(y=0.05, color=factor(UCB)), shape=15, size=1, alpha=0.6) +
  labs(y="p", color="UCB choice") +
  theme_minimal()
```
```{r}
acc_ucb <- cumsum(ucb_choice==best)/(1:n)
acc_ts  <- cumsum(ts_choice==best)/(1:n)
acc_sw  <- cumsum(sw_choice==best)/(1:n)
df_acc  <- data.frame(t=1:n, UCB=acc_ucb, TS=acc_ts, SW=acc_sw)
df_acc_long <- reshape2::melt(df_acc, "t")

ggplot(df_acc_long, aes(t, value, color=variable)) +
  geom_line() +
  labs(y="Cumulative Accuracy", title="How Often Each Algorithm Picks the Best Arm") +
  theme_minimal()
```



# 19. Adding a Fifth Arm




# 20. Non-Stationary Rewards 

Here I'm doing Non‑Stationary Rewards driven by James Hamilton’s Markov‑Switching Autoregressive thingie in the book.

Initializing these params do we can use the Markov‑Switching Autoregressive model.
```{r}
set.seed(2051)
T  <- 500    
R  <- 200    
W  <- 50    

# Markov‑Switching Autoregressive params for each of 4 arms
P_list <- list(
  matrix(c(0.9,0.1,0.1,0.9),   2,2),
  matrix(c(0.85,0.15,0.15,0.85),2,2),
  matrix(c(0.8,0.2,0.2,0.8),   2,2),
  matrix(c(0.95,0.05,0.05,0.95),2,2)
)
mu_list    <- list(c(-1,1), c(0,2), c(-0.5,1.5), c(0.5,-1))
phi_list   <- c(0.8, 0.9, 0.85, 0.7)
sigma_list <- c(0.3, 0.2, 0.25, 0.4)

simulate_arm <- function(P, mu, phi, sigma){
  s <- numeric(T); z <- numeric(T)
  s[1] <- sample(1:2,1)
  z[1] <- rnorm(1, mu[s[1]], sigma/sqrt(1-phi^2))
  for(t in 2:T){
    s[t] <- sample(1:2,1, prob=P[s[t-1],])
    z[t] <- mu[s[t]] + phi*(z[t-1]-mu[s[t]]) + sigma*rnorm(1)
  }
  plogis(z)
}

```



```{r}
# cumulative regret
reg_ucb <- reg_ts <- reg_sw <- array(0, c(T, R))
# choices (arm index) and best arm
ucb_choice <- ts_choice <- sw_choice <- matrix(NA, nrow=T, ncol=R)
best_arm    <- matrix(NA, nrow=T, ncol=R)

for(r in seq_len(R)){
  # simulate arms
  p_mat <- sapply(1:4, function(i)
    simulate_arm(P_list[[i]], mu_list[[i]],
                 phi_list[i], sigma_list[i])
  )
  best_arm[,r] <- apply(p_mat, 1, which.max)
  
  # init
  succ_ucb <- tr_ucb <- numeric(4)
  alpha_ts <- beta_ts <- rep(1,4)
  history <- list(arm=integer(), reward=numeric())
  cum_ucb <- cum_ts <- cum_sw <- 0
  
  for(t in seq_len(T)){
    probs <- p_mat[t,]
    ucbv <- ifelse(tr_ucb==0, 1,
                   succ_ucb/tr_ucb + sqrt(2*log(t)/tr_ucb))
    a_ucb <- which.max(ucbv)
    ucb_choice[t,r] <- a_ucb
    rwd <- rbinom(1,1,probs[a_ucb])
    tr_ucb[a_ucb]   <- tr_ucb[a_ucb] + 1
    succ_ucb[a_ucb] <- succ_ucb[a_ucb] + rwd
    cum_ucb <- cum_ucb + (probs[best_arm[t,r]] - probs[a_ucb])
    reg_ucb[t,r] <- cum_ucb
  
    draw_ts <- rbeta(4, alpha_ts, beta_ts)
    a_ts <- which.max(draw_ts)
    ts_choice[t,r] <- a_ts
    rwd <- rbinom(1,1,probs[a_ts])
    alpha_ts[a_ts] <- alpha_ts[a_ts] + rwd
    beta_ts[a_ts]  <- beta_ts[a_ts] + (1-rwd)
    cum_ts <- cum_ts + (probs[best_arm[t,r]] - probs[a_ts])
    reg_ts[t,r] <- cum_ts
    
    swv <- numeric(4)
    for(a in 1:4){
      idx <- tail(which(history$arm==a), W)
      ntr <- length(idx); nsu <- sum(history$reward[idx])
      # use if-else syntax:
      swv[a] <- if(ntr==0) 1 else nsu/ntr + sqrt(2*log(t)/ntr)
    }
    a_sw <- which.max(swv)
    sw_choice[t,r] <- a_sw
    rwd <- rbinom(1,1,probs[a_sw])
    history$arm    <- c(history$arm, a_sw)
    history$reward <- c(history$reward, rwd)
    cum_sw <- cum_sw + (probs[best_arm[t,r]] - probs[a_sw])
    reg_sw[t,r] <- cum_sw
  }
}
```

```{r}
df_reg <- tibble(
  time     = 1:T,
  UCB      = rowMeans(reg_ucb),
  Thompson = rowMeans(reg_ts),
  SW_UCB   = rowMeans(reg_sw)
) %>% pivot_longer(-time, names_to="Method", values_to="CumulativeRegret")

p1 <- ggplot(df_reg, aes(time, CumulativeRegret, color=Method)) +
  geom_line(size=1) +
  labs(
    title="Avg. CumulativeRegret under Markov‑Switching Autoregressive (4 Arms)",
    x="Time", y="CumulativeRegret"
  ) +
  theme_minimal()

acc_df <- tibble(
  time     = 1:T,
  UCB      = rowMeans(ucb_choice == best_arm),
  Thompson = rowMeans(ts_choice  == best_arm),
  SW_UCB   = rowMeans(sw_choice  == best_arm)
) %>% pivot_longer(-time, names_to="Method", values_to="Accuracy")

p2 <- ggplot(acc_df, aes(time, Accuracy, color=Method)) +
  geom_line(size=1) +
  labs(
    title="Per‑Step Optimal‑Arm Accuracy",
    x="Time", y="Accuracy"
  ) +
  theme_minimal()

print(p1)
```
```{r}
print(p2)
```

The algorithm whose accuracy dips the least (and recovers fastest) is the most responsive to change. this being ubc makes a lot of sense.
###these need further commenting.





