---
title: "Bandit Algorithms in a Markov Switching Environment"
author: "Marvin Ernst"
date: "2025-04-13"
output: html_document
editor_options: 
  chunk_output_type: inline
---

Clear environment:
```{r}
rm(list = ls())
```

Install from GitHub - devtools:
```{r}
#install.packages("devtools")
#devtools::install_github("stan-dev/cmdstanr", dependencies = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

# 1. Simulate Markov Switching Model (MSM)

We simulate a 2-state MSM where:
- State 1: $\pi_1 = 0.8$ (high success)
- State 2: $\pi_2 = 0.2$ (low success)
- Transition matrix:
  $\begin{bmatrix} 0.9 & 0.1 \\ 0.1 & 0.9 \end{bmatrix}$

```{r simulate-msm}
set.seed(42)
n <- 500
states <- numeric(n)
y <- numeric(n)

# Transition matrix
P <- matrix(c(0.9, 0.1,
              0.1, 0.9), nrow = 2, byrow = TRUE)

# State-dependent success probabilities
pi_vals <- c(0.8, 0.2)
states[1] <- 1
for (t in 2:n) {
  states[t] <- sample(1:2, 1, prob = P[states[t-1], ])
}
y <- rbinom(n, size = 1, prob = pi_vals[states])

data_msm <- tibble(t = 1:n, state = states, y = y)
stan_data <- list(N = n, y = y)
```

# 2. Estimate Parameters Using Stan

```{r}
fit <- stan(file = "msm_bernoulli.stan",
            data = stan_data,
            iter = 2000,
            warmup = 1000,
            chains = 4,
            seed = 123)
```

```{r}
print(fit, pars = c("pi", "p"))
```

pi[1] and pi[2] are the mixture proportions, i.e., how often the model believes the system is in state 1 vs state 2.

Our model is unsure about which state dominates – it splits the weight evenly. This is expected since:

- We gave it no state labels.

- The success probabilities p[1] and p[2] are very similar. 

The model couldn’t clearly separate the states, and estimates both states with similar success probabilities (~0.55), centered between 0.8 and 0.2.

**Why didn’t the model recover the true parameters?**

Possible Reasons:

1.	Label switching: The model doesn’t know which state is “state 1” vs “state 2” — it’s symmetric. You just get a mix of the two.
	
2.	No time structure: The Stan model was a mixture model, not a full Markov switching model. So it didn’t learn the transition matrix or sequence of states.
	
3.	Identifiability: Without constraints or state information, the model can’t confidently tell apart the two distributions.
	
	
Saving the model (to avoid recompiling every time):
```{r}
saveRDS(fit, "fit_msm_model.rds")
fit <- readRDS("fit_msm_model.rds")
```

# 3. Simulate Hidden Markov Model (HMM)

```{r simulate-hmm}
data_hmm <- tibble(t = 1:n, state = states, y = y)
```

Fit HMM with rstan:

```{r fit-hmm-stan, eval=FALSE}
fit <- stan(file = "hmm_bernoulli.stan",
            data = stan_data,
            iter = 2000,
            warmup = 1000,
            chains = 4,
            seed = 123,
            control = list(adapt_delta = 0.99, 
                           max_treedepth = 15))
```

## Interpret Parameters

- `init`: estimated initial state probabilities.

- `trans`: estimated transition matrix between states.

- `theta`: estimated success probabilities in each state.

```{r}
print(fit, pars = c("init", "trans", "theta"))
```

### Third Version - this is what we currently observe:

This version shows that we have significantly improved the model’s performance and stability compared to previous attempts. By reparameterizing the emission probabilities using a logit transformation (theta[1] = inv_logit(alpha), theta[2] = inv_logit(alpha + delta)) and adding informative priors on the transition matrix and emission separation, the sampler is now able to:

- Converge cleanly across all chains (Rhat = 1.00)

-	Avoid divergent transitions entirely

-	Accurately recover the true transition probabilities and emission values used in the simulation:

-	$\theta_1 \approx 0.2$, $\theta_2 \approx 0.8$

-	$P_{11} \approx 0.9$, $P_{22} \approx 0.9$

In contrast to earlier versions where the sampler was unstable (high Rhat, low n_eff, divergences), this version demonstrates that reparameterization and identifiability constraints are crucial when working with Hidden Markov Models in Stan.

This model can now serve as a stable basis for simulating environments and evaluating bandit algorithms in non-stationary, partially observed settings.

### Second Version - after doing some improvements:

Right now, Stan is struggling because:

- It’s unsure how to separate the emission probabilities.

-	There’s still symmetry in the model.

So let’s do two concrete things that will almost always help:

1. Add Informative Priors to Help Identify States

2. Add Priors on Transition Probabilities

### First Version - this is what we observed before doing some improvements:

These transition probabilities correctly recover the transition matrix we simulated!

Issues: We have a very low n_eff and High Rhat!

So, our chains did not mix well. The sampler is very uncertain about what the success probabilities should be. It’s likely stuck in different modes or swapping labels (more below).

Our model likely suffers from label switching — the sampler can’t tell whether:

-	State 1 is the “high success” state, or

- State 2 is the “high success” state

It keeps switching during sampling, making the average estimates look like:

$$
theta[1] ≈ 0.47 \\

theta[2] ≈ 0.47
$$
even though the true values were 0.8 and 0.2.

1. We will now orce the model to order the theta values so it breaks the symmetry. 

This ensures:
$$\theta[1] < \theta[2]$$
or vice versa, depending on your interpretation.

This does not affect inference, but ensures consistent labeling across chains.

2. We change the sampler by adding:

control = list(adapt_delta = 0.99, max_treedepth = 15)

# Below is NOT up to date!

# 4. Bandit Algorithms (UCB, UCB-Tuned, Thompson Sampling)

```{r bandit-algorithms}
K <- 1  # Only one arm in this simulation
t_horizon <- n

# Thompson Sampling
alpha <- 1; beta <- 1

# UCB parameters
ucb_counts <- 0
ucb_sum <- 0

# Tracking
ts_regret <- ucb_regret <- rep(0, t_horizon)

for (t in 1:t_horizon) {
  # True reward
  p_true <- pi_vals[states[t]]

  # Thompson Sampling action
  ts_draw <- rbeta(1, alpha, beta)
  ts_reward <- y[t]
  alpha <- alpha + ts_reward
  beta <- beta + (1 - ts_reward)
  ts_regret[t] <- abs(p_true - ts_draw)

  # UCB action
  if (ucb_counts == 0) {
    ucb_val <- 1
  } else {
    ucb_val <- (ucb_sum / ucb_counts) + 
                sqrt(2 * log(t) / ucb_counts)
  }
  ucb_reward <- y[t]
  ucb_counts <- ucb_counts + 1
  ucb_sum <- ucb_sum + ucb_reward
  ucb_regret[t] <- abs(p_true - ucb_val)
}
```

# 5. Modifications to UCB: Sliding Window and Discounted

```{r modified-ucb}
window_size <- 20
ucb_window_regret <- ucb_discount_regret <- rep(0, t_horizon)

gamma <- 0.95  # discount factor
ucb_sum_d <- 0
ucb_counts_d <- 0

for (t in 1:t_horizon) {
  p_true <- pi_vals[states[t]]

  # Sliding window UCB
  start <- max(1, t - window_size + 1)
  window_rewards <- y[start:t]
  n_window <- length(window_rewards)
  mean_r <- mean(window_rewards)
  ucb_val <- mean_r + sqrt(2 * log(t) / n_window)
  ucb_window_regret[t] <- abs(p_true - ucb_val)

  # Discounted UCB
  ucb_sum_d <- gamma * ucb_sum_d + y[t]
  ucb_counts_d <- gamma * ucb_counts_d + 1
  mean_d <- ucb_sum_d / ucb_counts_d
  ucb_val_d <- mean_d + sqrt(2 * log(t) / ucb_counts_d)
  ucb_discount_regret[t] <- abs(p_true - ucb_val_d)
}
```

# 6. Plot Regret Comparison

```{r plot-regret}
tibble(
  t = 1:n,
  Thompson = cumsum(ts_regret),
  UCB = cumsum(ucb_regret),
  UCB_Window = cumsum(ucb_window_regret),
  UCB_Discount = cumsum(ucb_discount_regret)
) %>%
  pivot_longer(-t) %>%
  ggplot(aes(x = t, y = value, color = name)) +
  geom_line() +
  labs(title = "Cumulative Regret Over Time",
       y = "Cumulative Regret", x = "Time Step") +
  theme_minimal()
```



