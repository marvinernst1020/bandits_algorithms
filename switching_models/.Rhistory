a_t <- which.max(sampled_values)
selected_arms[t] <- a_t
r_t <- y_true[a_t, t]
received_rewards[t] <- r_t
regret[t] <- y_true[oracle_arm, t] - r_t
observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")
plot(cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
View(observed_rewards)
y_true
set.seed(456)
K <- 3
N <- 100
theta <- matrix(c(0.2, 0.8,
0.5, 0.7,
0.3, 0.9),
nrow = K, byrow = TRUE)
pi_indiv <- list(
matrix(c(0.9, 0.1,
0.1, 0.9), 2, 2, byrow = TRUE),
matrix(c(0.85, 0.15,
0.2, 0.8), 2, 2, byrow = TRUE),
matrix(c(0.95, 0.05,
0.3, 0.7), 2, 2, byrow = TRUE)
)
z_true <- matrix(0, nrow = K, ncol = N)
y_true <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
z_true[i, 1] <- rbinom(1, 1, 0.5)
y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
for (t in 2:N) {
z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
}
}
View(z_true)
set.seed(456)
K <- 3
N <- 100
theta <- matrix(c(0.2, 0.8,
0.5, 0.7,
0.3, 0.9),
nrow = K, byrow = TRUE)
pi_indiv <- list(
matrix(c(0.9, 0.1,
0.1, 0.9), 2, 2, byrow = TRUE),
matrix(c(0.85, 0.15,
0.2, 0.8), 2, 2, byrow = TRUE),
matrix(c(0.95, 0.05,
0.3, 0.7), 2, 2, byrow = TRUE)
)
z_true <- matrix(0, nrow = K, ncol = N)
y_true <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
z_true[i, 1] <- rbinom(1, 1, 0.5)
y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
for (t in 2:N) {
z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
}
}
View(y_true)
View(y_true)
set.seed(456)
K <- 3
N <- 100
theta <- matrix(c(0.2, 0.8,
0.5, 0.7,
0.3, 0.9),
nrow = K, byrow = TRUE)
pi_indiv <- list(
matrix(c(0.9, 0.1,
0.1, 0.9), 2, 2, byrow = TRUE),
matrix(c(0.85, 0.15,
0.2, 0.8), 2, 2, byrow = TRUE),
matrix(c(0.95, 0.05,
0.3, 0.7), 2, 2, byrow = TRUE)
)
z_true <- matrix(0, nrow = K, ncol = N)
y_true <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
z_true[i, 1] <- rbinom(1, 1, 0.5)
y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
for (t in 2:N) {
z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
}
}
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
oracle_rewards <- apply(y_true, 1, sum)
oracle_arm <- which.max(oracle_rewards)
for (t in 1:N) {
sampled_values <- numeric(K)
for (i in 1:K) {
if (length(observed_rewards[[i]]) >= 2) {
data_list <- list(y = observed_rewards[[i]], N = length(observed_rewards[[i]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 100)
post <- coda.samples(model, c("theta0", "theta1", "pi"), n.iter = 100)
post_matrix <- as.matrix(post)
idx <- sample(1:nrow(post_matrix), 1)
theta0 <- post_matrix[idx, "theta0"]
theta1 <- post_matrix[idx, "theta1"]
pi1 <- post_matrix[idx, "pi[1]"]
pi2 <- post_matrix[idx, "pi[2]"]
pz0 <- 0.5
pz1 <- 0.5
for (s in 1:5) {
new_pz0 <- pz0 * (1 - pi1) + pz1 * pi2
new_pz1 <- pz0 * pi1 + pz1 * (1 - pi2)
norm <- new_pz0 + new_pz1
pz0 <- new_pz0 / norm
pz1 <- new_pz1 / norm
}
sampled_values[i] <- pz0 * theta0 + pz1 * theta1
} else {
sampled_values[i] <- runif(1)
}
}
a_t <- which.max(sampled_values)
selected_arms[t] <- a_t
r_t <- y_true[a_t, t]
received_rewards[t] <- r_t
regret[t] <- y_true[oracle_arm, t] - r_t
observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")
plot(cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
theta_selected <- matrix(0, nrow = K, ncol = T)
for (i in 1:K) {
for (t in 1:T) {
theta_selected[i, t] <- theta[i, z[i, t] + 1]
}
}
theta_selected <- matrix(0, nrow = K, ncol = T)
for (i in 1:K) {
for (t in 1:T) {
theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
}
}
View(theta_selected)
theta_selected <- matrix(0, nrow = K, ncol = T)
for (i in 1:K) {
for (t in 1:N) {
theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
}
}
View(z_true)
theta_selected <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
}
}
View(theta_selected)
theta_selected <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
}
}
oracle_arm <- apply(theta_selected, 1, which.max)
theta_selected <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
}
}
oracle_arm <- apply(theta_selected, 0, which.max)
theta_selected <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
}
}
oracle_arm <- apply(theta_selected, 2, which.max)
View(theta_selected)
View(observed_rewards)
View(post)
View(post_matrix)
View(model)
View(post_matrix)
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
oracle_rewards <- apply(y_true, 1, sum)
oracle_arm <- which.max(oracle_rewards)
for (t in 1:N) {
sampled_values <- numeric(K)
for (i in 1:K) {
if (length(observed_rewards[[i]]) >= 2) {
data_list <- list(y = observed_rewards[[i]], N = length(observed_rewards[[i]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 100)
post <- coda.samples(model, n.iter = 100)
post_matrix <- as.matrix(post)
idx <- sample(1:nrow(post_matrix), 1)
theta0 <- post_matrix[idx, "theta0"]
theta1 <- post_matrix[idx, "theta1"]
pi1 <- post_matrix[idx, "pi[1]"]
pi2 <- post_matrix[idx, "pi[2]"]
pz0 <- 0.5
pz1 <- 0.5
for (s in 1:5) {
new_pz0 <- pz0 * (1 - pi1) + pz1 * pi2
new_pz1 <- pz0 * pi1 + pz1 * (1 - pi2)
norm <- new_pz0 + new_pz1
pz0 <- new_pz0 / norm
pz1 <- new_pz1 / norm
}
sampled_values[i] <- pz0 * theta0 + pz1 * theta1
} else {
sampled_values[i] <- runif(1)
}
}
a_t <- which.max(sampled_values)
selected_arms[t] <- a_t
r_t <- y_true[a_t, t]
received_rewards[t] <- r_t
regret[t] <- y_true[oracle_arm, t] - r_t
observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}
View(post)
View(model)
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
oracle_rewards <- apply(y_true, 1, sum)
oracle_arm <- which.max(oracle_rewards)
for (t in 1:N) {
sampled_values <- numeric(K)
for (i in 1:K) {
if (length(observed_rewards[[i]]) >= 2) {
data_list <- list(y = observed_rewards[[i]], N = length(observed_rewards[[i]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 100)
post <- coda.samples(model, c("theta0", "theta1", "pi"), n.iter = 100)
post_matrix <- as.matrix(post)
idx <- sample(1:nrow(post_matrix), 1)
theta0 <- post_matrix[idx, "theta0"]
theta1 <- post_matrix[idx, "theta1"]
pi1 <- post_matrix[idx, "pi[1]"]
pi2 <- post_matrix[idx, "pi[2]"]
pz0 <- 0.5
pz1 <- 0.5
for (s in 1:5) {
new_pz0 <- pz0 * (1 - pi1) + pz1 * pi2
new_pz1 <- pz0 * pi1 + pz1 * (1 - pi2)
norm <- new_pz0 + new_pz1
pz0 <- new_pz0 / norm
pz1 <- new_pz1 / norm
}
sampled_values[i] <- pz0 * theta0 + pz1 * theta1
} else {
sampled_values[i] <- runif(1)
}
}
a_t <- which.max(sampled_values)
selected_arms[t] <- a_t
r_t <- y_true[a_t, t]
received_rewards[t] <- r_t
regret[t] <- y_true[oracle_arm, t] - r_t
observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")
plot(cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
coda.samples()
coda.samples(model)
z_names <- paste("z", 1:N, sep="")
coda.samples(model, c("theta0", "theta1", "pi", z_names), n.iter = 100)
z_names <- paste0("z[", 1:100, "]")
# Sample including all z[t]
post <- coda.samples(model, c("theta0", "theta1", "pi", z_names), n.iter = 100)
z_names <- paste0("z[", 1:12, "]")
# Sample including all z[t]
post <- coda.samples(model, c("theta0", "theta1", "pi", z_names), n.iter = 100)
# Sample including all z[t]
post <- coda.samples(model, c("theta0", "theta1", "pi", z_names), n.iter = 100)
data_list <- list(y = observed_rewards[[1]], N = length(observed_rewards[[1]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 100)
post <- coda.samples(model, c("theta0", "theta1", "pi","z[1]"), n.iter = 100)
post_matrix <- as.matrix(post)
View(post_matrix)
post <- coda.samples(model, c("theta0", "theta1", "pi","z[1]","z[2]"), n.iter = 100)
post_matrix <- as.matrix(post)
View(post_matrix)
post <- coda.samples(model, c("theta0", "theta1", "pi","z[-1]"), n.iter = 100)
View(data_list)
post <- coda.samples(model, c("theta0", "theta1", "pi","z[26]"), n.iter = 100)
post_matrix <- as.matrix(post)
View(post)
View(post_matrix)
post <- coda.samples(model, c("theta0", "theta1", "pi","z[27]"), n.iter = 100)
post_matrix <- as.matrix(post)
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
oracle_rewards <- apply(y_true, 1, sum)
oracle_arm <- which.max(oracle_rewards)
for (t in 1:N) {
sampled_values <- numeric(K)
for (i in 1:K) {
if (length(observed_rewards[[i]]) >= 2) {
data_list <- list(y = observed_rewards[[i]], N = length(observed_rewards[[i]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 100)
post <- coda.samples(model, c("theta0", "theta1", "pi", paste0("z[", length(observed_rewards[[i]], "]")), n.iter = 100)
post_matrix <- as.matrix(post)
post <- coda.samples(model, c("theta0", "theta1", "pi", paste0("z[", length(observed_rewards[[i]]), "]")), n.iter = 100)
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
oracle_rewards <- apply(y_true, 1, sum)
oracle_arm <- which.max(oracle_rewards)
for (t in 1:N) {
sampled_values <- numeric(K)
for (i in 1:K) {
if (length(observed_rewards[[i]]) >= 2) {
data_list <- list(y = observed_rewards[[i]], N = length(observed_rewards[[i]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 100)
post <- coda.samples(model, c("theta0", "theta1", "pi", paste0("z[", length(observed_rewards[[i]]), "]")), n.iter = 100)
post_matrix <- as.matrix(post)
idx <- sample(1:nrow(post_matrix), 1)
theta0 <- post_matrix[idx, "theta0"]
theta1 <- post_matrix[idx, "theta1"]
pi1 <- post_matrix[idx, "pi[1]"]
pi2 <- post_matrix[idx, "pi[2]"]
pz0 <- 0.5
pz1 <- 0.5
for (s in 1:5) {
new_pz0 <- pz0 * (1 - pi1) + pz1 * pi2
new_pz1 <- pz0 * pi1 + pz1 * (1 - pi2)
norm <- new_pz0 + new_pz1
pz0 <- new_pz0 / norm
pz1 <- new_pz1 / norm
}
sampled_values[i] <- pz0 * theta0 + pz1 * theta1
} else {
sampled_values[i] <- runif(1)
}
}
a_t <- which.max(sampled_values)
selected_arms[t] <- a_t
r_t <- y_true[a_t, t]
received_rewards[t] <- r_t
regret[t] <- y_true[oracle_arm, t] - r_t
observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")
plot(cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
z_t <- post_matrix[idx, paste0("z[", length(observed_rewards[[i]]), "]"))]
z_t <- post_matrix[idx, paste0("z[", length(observed_rewards[[i]]), "]"))]
z_t <- post_matrix[idx, paste0("z[", length(observed_rewards[[i]]), "]")]
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
oracle_rewards <- apply(y_true, 1, sum)
oracle_arm <- which.max(oracle_rewards)
for (t in 1:N) {
sampled_values <- numeric(K)
for (i in 1:K) {
if (length(observed_rewards[[i]]) >= 2) {
data_list <- list(y = observed_rewards[[i]], N = length(observed_rewards[[i]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 100)
post <- coda.samples(model, c("theta0", "theta1", "pi", paste0("z[", length(observed_rewards[[i]]), "]")), n.iter = 100)
post_matrix <- as.matrix(post)
idx <- sample(1:nrow(post_matrix), 1)
theta0 <- post_matrix[idx, "theta0"]
theta1 <- post_matrix[idx, "theta1"]
pi1 <- post_matrix[idx, "pi[1]"]
pi2 <- post_matrix[idx, "pi[2]"]
z_t <- post_matrix[idx, paste0("z[", length(observed_rewards[[i]]), "]")]
sampled_values[i] <- (1-z_t)* theta0 + z_t * theta1
} else {
sampled_values[i] <- runif(1)
}
}
a_t <- which.max(sampled_values)
selected_arms[t] <- a_t
r_t <- y_true[a_t, t]
received_rewards[t] <- r_t
regret[t] <- y_true[oracle_arm, t] - r_t
observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")
plot(cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
oracle_rewards <- apply(y_true, 1, sum)
oracle_arm <- which.max(oracle_rewards)
for (t in 1:N) {
sampled_values <- numeric(K)
for (i in 1:K) {
if (length(observed_rewards[[i]]) >= 2) {
data_list <- list(y = observed_rewards[[i]], N = length(observed_rewards[[i]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 100)
post <- coda.samples(model, c("theta0", "theta1", "pi"), n.iter = 100)
post_matrix <- as.matrix(post)
idx <- sample(1:nrow(post_matrix), 1)
theta0 <- post_matrix[idx, "theta0"]
theta1 <- post_matrix[idx, "theta1"]
pi1 <- post_matrix[idx, "pi[1]"]
pi2 <- post_matrix[idx, "pi[2]"]
pz0 <- 0.5
pz1 <- 0.5
for (s in 1:5) {
new_pz0 <- pz0 * (1 - pi1) + pz1 * pi2
new_pz1 <- pz0 * pi1 + pz1 * (1 - pi2)
norm <- new_pz0 + new_pz1
pz0 <- new_pz0 / norm
pz1 <- new_pz1 / norm
}
sampled_values[i] <- pz0 * theta0 + pz1 * theta1
} else {
sampled_values[i] <- runif(1)
}
}
a_t <- which.max(sampled_values)
selected_arms[t] <- a_t
r_t <- y_true[a_t, t]
received_rewards[t] <- r_t
regret[t] <- y_true[oracle_arm, t] - r_t
observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")
plot(cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
oracle_rewards <- apply(y_true, 1, sum)
oracle_arm <- which.max(oracle_rewards)
for (t in 1:N) {
sampled_values <- numeric(K)
for (i in 1:K) {
if (length(observed_rewards[[i]]) >= 2) {
data_list <- list(y = observed_rewards[[i]], N = length(observed_rewards[[i]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 100)
post <- coda.samples(model, c("theta0", "theta1", "pi", paste0("z[", length(observed_rewards[[i]]), "]")), n.iter = 100)
post_matrix <- as.matrix(post)
idx <- sample(1:nrow(post_matrix), 1)
theta0 <- post_matrix[idx, "theta0"]
theta1 <- post_matrix[idx, "theta1"]
pi1 <- post_matrix[idx, "pi[1]"]
pi2 <- post_matrix[idx, "pi[2]"]
z_t <- post_matrix[idx, paste0("z[", length(observed_rewards[[i]]), "]")]
sampled_values[i] <- (1-z_t)* theta0 + z_t * theta1
} else {
sampled_values[i] <- runif(1)
}
}
a_t <- which.max(sampled_values)
selected_arms[t] <- a_t
r_t <- y_true[a_t, t]
received_rewards[t] <- r_t
regret[t] <- y_true[oracle_arm, t] - r_t
observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")
plot(cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
oracle_rewards <- apply(y_true, 1, sum)
theta_selected <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
}
}
oracle_arm <- apply(theta_selected, 2, which.max)
