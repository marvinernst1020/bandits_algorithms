knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
set.seed(123)
K <- 3 # number of arms
N <- 100 # time steps
theta <- matrix(c(0.2, 0.8,
0.5, 0.7,
0.3, 0.9),
nrow = K, byrow = TRUE)
pi_global <- matrix(c(0.9, 0.1,
0.1, 0.9), nrow = 2, byrow = TRUE)
z_global <- numeric(N)
z_global[1] <- rbinom(1, 1, 0.5)
for (t in 2:N) {
z_global[t] <- rbinom(1, 1, pi_global[z_global[t - 1] + 1, 2])
}
y_global <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
for (t in 1:N) {
y_global[i, t] <- rbinom(1, 1, theta[i, z_global[t] + 1])
}
}
set.seed(456)
K <- 3
N <- 500
theta <- matrix(c(0.2, 0.8,
0.5, 0.7,
0.3, 0.9),
nrow = K, byrow = TRUE)
pi_indiv <- list(
matrix(c(0.9, 0.1,
0.1, 0.9), 2, 2, byrow = TRUE),
matrix(c(0.85, 0.15,
0.2, 0.8), 2, 2, byrow = TRUE),
matrix(c(0.95, 0.05,
0.3, 0.7), 2, 2, byrow = TRUE)
)
z_true <- matrix(0, nrow = K, ncol = N)
y_true <- matrix(0, nrow = K, ncol = N)
for (i in 1:K) {
z_true[i, 1] <- rbinom(1, 1, 0.5)
y_true[i, 1] <- rbinom(1, 1, theta[i, z_true[i, 1] + 1])
for (t in 2:N) {
z_true[i, t] <- rbinom(1, 1, pi_indiv[[i]][z_true[i, t - 1] + 1, 2])
y_true[i, t] <- rbinom(1, 1, theta[i, z_true[i, t] + 1])
}
}
model_file <- "poor_model.jags"
observed_rewards <- vector("list", K)
selected_arms <- numeric(N)
received_rewards <- numeric(N)
regret <- numeric(N)
# This matrix contains the probability of each arm at each time step:
theta_selected <- matrix(0, nrow = K, ncol = N)
# We are computing the arm that has the highest expected reward at each time, as the oracle:
for (i in 1:K) {
for (t in 1:N) {
theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
}
}
oracle_arm <- apply(theta_selected, 2, which.max)
posterior_matrices <- rep(list(1), K)
last_states <- list()
a_t <- sample(1:K, 1) # pick first arm to pull
for (t in 1:N) {
sampled_values <- numeric(K)
if (length(observed_rewards[[a_t]]) >= 2) {
data_list <- list(y = observed_rewards[[a_t]], N = length(observed_rewards[[a_t]]))
model <- jags.model(model_file, data = data_list, n.chains = 1, quiet = TRUE)
update(model, 1000)
post <- coda.samples(model, c("theta0", "theta1", "pi", paste0("z[", length(observed_rewards[[a_t]]), "]")), n.iter = 100)
post_matrix <- as.matrix(post)
posterior_matrices[[a_t]] <- post_matrix
last_states[[a_t]] <-length(observed_rewards[[a_t]])
}
for (l in 1:K){
if (!is.null(nrow(posterior_matrices[[l]])) > 0){
idx <- sample(1:nrow(posterior_matrices[[l]]), 1)
theta0 <- posterior_matrices[[l]][idx, "theta0"]
theta1 <- posterior_matrices[[l]][idx, "theta1"]
pi1 <- posterior_matrices[[l]][idx, "pi[1]"]
pi2 <- posterior_matrices[[l]][idx, "pi[2]"]
z_t <- posterior_matrices[[l]][idx, paste0("z[", last_states[[l]], "]")]
sampled_values[l] <- (1-z_t)* theta0 + z_t * theta1
}
else {
sampled_values[l] <- runif(1)
}
}
a_t <- which.max(sampled_values)
selected_arms[t] <- a_t
r_t <- y_true[a_t, t]
received_rewards[t] <- r_t
# we use the expected regret:
regret[t] <- theta_selected[oracle_arm[t], t] - theta[selected_arms[t], z_true[selected_arms[t], t] + 1]
observed_rewards[[a_t]] <- c(observed_rewards[[a_t]], r_t)
}
install.packages("rjags")
cumulative_reward <- cumsum(received_rewards)
cumulative_regret <- cumsum(regret)
plot(cumulative_reward, type = "l", col = "darkgreen", lwd = 2,
xlab = "Time", ylab = "Cumulative Reward", main = "Thompson Sampling")
plot(cumulative_regret, type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Cumulative Regret", main = "Regret vs Oracle")
set.seed(999)
N_long <- 10000
theta_true <- c(0.3, 0.9)
pi_true <- c(0.10, 0.95)  # pi[1] = P(0 to 1), pi[2] = P(1 to 1)
# Simulate z and y
z_check <- numeric(N_long)
y_check <- numeric(N_long)
z_check[1] <- rbinom(1, 1, pi_true[1] / (pi_true[1] + pi_true[2]))  # start in stationary dist
y_check[1] <- rbinom(1, 1, ifelse(z_check[1] == 0, theta_true[1], theta_true[2]))
for (t in 2:N_long) {
z_check[t] <- rbinom(1, 1, ifelse(z_check[t-1] == 0, pi_true[1], pi_true[2]))
y_check[t] <- rbinom(1, 1, ifelse(z_check[t] == 0, theta_true[1], theta_true[2]))
}
# Run JAGS
data_check <- list(y = y_check, N = N_long)
model_check <- jags.model("poor_model.jags", data = data_check, n.chains = 1, quiet = TRUE)
update(model_check, 1000)
post_check <- coda.samples(model_check, c("theta0", "theta1", "pi"), n.iter = 1000)
summary(post_check)
set.seed(456)
K <- 3
N <- 500
# true emission probabilities for state 0 & 1, per arm
theta <- matrix(c(0.2, 0.8,
0.5, 0.7,
0.3, 0.9),
nrow = K, byrow = TRUE)
# true transition matrices per arm
pi_indiv <- list(
matrix(c(0.9, 0.1,
0.1, 0.9), 2, 2, byrow = TRUE),
matrix(c(0.85, 0.15,
0.2, 0.8), 2, 2, byrow = TRUE),
matrix(c(0.95, 0.05,
0.3, 0.7), 2, 2, byrow = TRUE)
)
# simulate latent states z_true and observed rewards y_true
z_true <- matrix(0, K, N)
y_true <- matrix(0, K, N)
for(i in 1:K){
z_true[i,1] <- rbinom(1,1,0.5)
y_true[i,1] <- rbinom(1,1,theta[i, z_true[i,1]+1])
for(t in 2:N){
z_true[i,t] <- rbinom(1,1, pi_indiv[[i]][ z_true[i,t-1]+1 , 2 ])
y_true[i,t] <- rbinom(1,1, theta[i, z_true[i,t]+1 ])
}
}
# Precompute oracle for regret
theta_selected <- apply(z_true, c(1,2),
function(z,i) theta[i,z+1],
i = rep(1:K, each=N))
oracle_arm <- apply(theta_selected, 2, which.max)
# ----- 2. UCB algorithm -----
counts       <- rep(0, K)   # number of times each arm has been pulled
sum_rewards  <- rep(0, K)   # cumulative reward per arm
selected_arms_ucb <- integer(N)
received_rewards_ucb <- numeric(N)
regret_ucb   <- numeric(N)
# 2a) Initialization: play each arm once
for(t in 1:K){
arm <- t
r   <- y_true[arm, t]
counts[arm]      <- 1
sum_rewards[arm] <- r
selected_arms_ucb[t] <- arm
received_rewards_ucb[t] <- r
regret_ucb[t] <- theta_selected[oracle_arm[t], t] -
theta_selected[arm, t]
}
set.seed(456)
K <- 3
N <- 500
# true emission probabilities for state 0 & 1, per arm
theta <- matrix(c(0.2, 0.8,
0.5, 0.7,
0.3, 0.9),
nrow = K, byrow = TRUE)
# true transition matrices per arm
pi_indiv <- list(
matrix(c(0.9, 0.1,
0.1, 0.9), 2, 2, byrow = TRUE),
matrix(c(0.85, 0.15,
0.2, 0.8), 2, 2, byrow = TRUE),
matrix(c(0.95, 0.05,
0.3, 0.7), 2, 2, byrow = TRUE)
)
# simulate latent states z_true and observed rewards y_true
z_true <- matrix(0, K, N)
y_true <- matrix(0, K, N)
for(i in 1:K){
z_true[i,1] <- rbinom(1,1,0.5)
y_true[i,1] <- rbinom(1,1, theta[i, z_true[i,1]+1])
for(t in 2:N){
z_true[i,t] <- rbinom(1,1, pi_indiv[[i]][ z_true[i,t-1]+1 , 2 ])
y_true[i,t] <- rbinom(1,1, theta[i, z_true[i,t]+1 ])
}
}
# —— FIX: build theta_selected as a real K×N matrix ——
theta_selected <- matrix(0, nrow = K, ncol = N)
for(i in 1:K){
for(t in 1:N){
theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
}
}
oracle_arm <- apply(theta_selected, 2, which.max)
# ----- 2. UCB algorithm -----
counts       <- rep(0, K)   # number of times each arm has been pulled
sum_rewards  <- rep(0, K)   # cumulative reward per arm
selected_arms_ucb    <- integer(N)
received_rewards_ucb <- numeric(N)
regret_ucb           <- numeric(N)
# 2a) Initialization: play each arm once
for(t in 1:K){
arm <- t
r   <- y_true[arm, t]
counts[arm]      <- 1
sum_rewards[arm] <- r
selected_arms_ucb[t]    <- arm
received_rewards_ucb[t] <- r
regret_ucb[t] <- theta_selected[oracle_arm[t], t] -
theta_selected[arm, t]
}
# 2b) Main loop
for(t in (K+1):N){
ucb_values <- numeric(K)
for(i in 1:K){
mu_hat <- sum_rewards[i] / counts[i]
bonus  <- sqrt(2 * log(t) / counts[i])
ucb_values[i] <- mu_hat + bonus
}
arm <- which.max(ucb_values)
r   <- y_true[arm, t]
counts[arm]      <- counts[arm] + 1
sum_rewards[arm] <- sum_rewards[arm] + r
selected_arms_ucb[t]    <- arm
received_rewards_ucb[t] <- r
regret_ucb[t] <- theta_selected[oracle_arm[t], t] -
theta_selected[arm, t]
}
# ----- 3. Evaluate -----
cumu_reward_ucb <- cumsum(received_rewards_ucb)
cumu_regret_ucb <- cumsum(regret_ucb)
# Plotting
par(mfrow=c(1,2))
plot(cumu_reward_ucb, type="l", lwd=2,
xlab="Time", ylab="Cumulative Reward",
main="UCB: Cumulative Reward")
plot(cumu_regret_ucb, type="l", lwd=2, col="red",
xlab="Time", ylab="Cumulative Regret",
main="UCB: Regret vs Oracle")
set.seed(2025)
N_long <- 5000
# True parameters:
theta_true <- c(0.3, 0.9)       # theta0, theta1
pi_true    <- c(0.10, 0.95)     # pi[1]=P(0->1), pi[2]=P(1->1)
# Pre-allocate
z_long <- numeric(N_long)
y_long <- numeric(N_long)
# Stationary initial draw:
rho0 <- pi_true[2] / sum(pi_true)   # P(z=0)
z_long[1] <- rbinom(1,1, 1 - rho0)   # P(z=1)=rho1
y_long[1] <- rbinom(1,1, theta_true[z_long[1]+1])
# Forward simulate
for(t in 2:N_long) {
p_z1 <- ifelse(z_long[t-1]==0,
pi_true[1],        # 0->1
pi_true[2])        # 1->1
z_long[t] <- rbinom(1,1, p_z1)
y_long[t] <- rbinom(1,1, theta_true[z_long[t]+1])
}
#----------------------------------------
# 2) FIT IN JAGS USING YOUR poor_model.jags
#----------------------------------------
library(rjags)
library(coda)
data_long <- list(
y = y_long,
N = N_long
)
# Initialize and burn in
model_long <- jags.model(
file = "poor_model.jags",
data = data_long,
n.chains = 2,
quiet = TRUE
)
update(model_long, 2000)    # burn-in
# Sample the key parameters
post_long <- coda.samples(
model = model_long,
variable.names = c("theta0","theta1","pi[1]","pi[2]"),
n.iter = 5000
)
print( summary(post_long) )
set.seed(2025)
N_long <- 5000
# True parameters:
theta_true <- c(0.3, 0.9)       # theta0, theta1
pi_true    <- c(0.10, 0.95)     # pi[1]=P(0->1), pi[2]=P(1->1)
# Pre-allocate
z_long <- numeric(N_long)
y_long <- numeric(N_long)
# Stationary initial draw:
rho0 <- pi_true[2] / sum(pi_true)   # P(z=0)
z_long[1] <- rbinom(1,1, 1 - rho0)   # P(z=1)=rho1
y_long[1] <- rbinom(1,1, theta_true[z_long[1]+1])
# Forward simulate
for(t in 2:N_long) {
p_z1 <- ifelse(z_long[t-1]==0,
pi_true[1],        # 0->1
pi_true[2])        # 1->1
z_long[t] <- rbinom(1,1, p_z1)
y_long[t] <- rbinom(1,1, theta_true[z_long[t]+1])
}
#----------------------------------------
# 2) FIT IN JAGS USING YOUR poor_model.jags
#----------------------------------------
library(rjags)
library(coda)
data_long <- list(
y = y_long,
N = N_long
)
# Initialize and burn in
model_long <- jags.model(
file = "poor_model.jags",
data = data_long,
n.chains = 2,
quiet = TRUE
)
update(model_long, 1000)    # burn-in
# Sample the key parameters
post_long <- coda.samples(
model = model_long,
variable.names = c("theta0","theta1","pi[1]","pi[2]"),
n.iter = 1000
)
print( summary(post_long) )
# Optionally: traceplots & density
plot(post_long)
# Initializing algorithm structure
counts       <- rep(0, K)   # storing number of times each arm has been pulled
sum_rewards  <- rep(0, K)   # cumulative reward per arm
selected_arms_ucb    <- integer(N) # which arm is picked at time t
received_rewards_ucb <- numeric(N) # reward at time t
regret_ucb           <- numeric(N) # regret at time t
# Initialization: play each arm once
for(t in 1:K){
arm <- t
r   <- y_true[arm, t]
counts[arm]      <- 1
sum_rewards[arm] <- r
selected_arms_ucb[t]    <- arm
received_rewards_ucb[t] <- r
regret_ucb[t] <- theta_selected[oracle_arm[t], t] -
theta_selected[arm, t] # best arm - selected arm
}
# to be repeaated 500 tims
for(t in (K+1):N){
ucb_values <- numeric(K)
for(i in 1:K){
mu_hat <- sum_rewards[i] / counts[i] #nvr /0 bcs we initialized as 1
bonus  <- sqrt(2 * log(t) / counts[i])
ucb_values[i] <- mu_hat + bonus
}
arm <- which.max(ucb_values)
r   <- y_true[arm, t]
counts[arm]      <- counts[arm] + 1 #adding count after the arm chosen again
sum_rewards[arm] <- sum_rewards[arm] + r #ading reward after arm chosen again
selected_arms_ucb[t]    <- arm
received_rewards_ucb[t] <- r
regret_ucb[t] <- theta_selected[oracle_arm[t], t] -
theta_selected[arm, t]
}
#Evaluate
cumu_reward_ucb <- cumsum(received_rewards_ucb)
cumu_regret_ucb <- cumsum(regret_ucb)
par(mfrow=c(1,2))
plot(cumu_reward_ucb, type="l", lwd=2,
xlab="Time", ylab="Cumulative Reward",
main="UCB: Cumulative Reward")
plot(cumu_regret_ucb, type="l", lwd=2, col="red",
xlab="Time", ylab="Cumulative Regret",
main="UCB: Regret vs Oracle")
set.seed(2025)
N_long <- 5000
theta_true <- c(0.3, 0.9)       # theta0, theta1
pi_true    <- c(0.10, 0.95)     # pi[1]=P(0 to1), pi[2]=P(1 to 1)
#init
z_long <- numeric(N_long)
y_long <- numeric(N_long)
# initial draw:
rho0 <- pi_true[2] / sum(pi_true)   # P(z=0)
z_long[1] <- rbinom(1,1, 1 - rho0)   # P(z=1)=rho1
y_long[1] <- rbinom(1,1, theta_true[z_long[1]+1])
# simulate
for(t in 2:N_long) {
# Choose the correct transition probability into state 1, depending on whether z_{t-1} was 0 or 1
p_z1 <- ifelse(z_long[t-1]==0,
pi_true[1],        # 0->1
pi_true[2])        # 1->1
z_long[t] <- rbinom(1,1, p_z1)
y_long[t] <- rbinom(1,1, theta_true[z_long[t]+1])
}
# FIT IN JAGS USING poor_model.jags
data_long <- list(
y = y_long,
N = N_long
)
model_long <- jags.model(
file = "poor_model.jags",
data = data_long,
n.chains = 2,
quiet = TRUE
)
update(model_long, 1000)
post_long <- coda.samples(
model = model_long,
variable.names = c("theta0","theta1","pi[1]","pi[2]"),
n.iter = 1000
)
print( summary(post_long) )
cat(
'
model {
logit_mu[1] <- logit(mu1)
p[1] <- mu1
y[1] ~ dbern(p[1])
for (t in 2:N) {
logit_mu[t] ~ dnorm(phi * logit_mu[t-1], tau)
p[t]<- ilogit(logit_mu[t])
y[t] ~ dbern(p[t])
}
#––– Priors on AR parameters –––
mu1        ~ dbeta(1, 1)
phi    ~ dnorm(0, 1 / pow(0.5,2)) T(-1,1)
sigma2 ~ dinvgamma(0.001, 0.001)
tau     <- 1 / sigma2
}
',
file = "models/ar_model.jags"
)
# set the correct working directory:
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
cat(
'model {
y[1] ~ dbern(p[1])
z[1] ~ dbern(0.5)
p[1] <- z[1]*mu1 + (1 - z[1])*mu0
for (t in 2:N) {
z[t] ~ dbern(z[t-1]*pi[2] + (1 - z[t-1])*pi[1])
p[t] <- z[t]*mu1 + (1 - z[t])*mu0
y[t] ~ dbern(p[t])
}
mu0 ~ dbeta(1,1)
mu1 ~ dbeta(1,1)
pi[1] ~ dbeta(1,1)
pi[2] ~ dbeta(1,1)
}
', file = "models/poor_model.jags")
cat(
'model {
for (i in 1:K) {
for (t in 1:N) {
p_arm[i,t] <- z[t] * mu[i,2] + (1 - z[t]) * mu[i,1]
}
}
z[1] ~ dbern(0.5)
for (i in 1:K) {
y_obs[i,1] ~ dbern(p_arm[i,1])
}
for (t in 2:N) {
z[t] ~ dbern(z[t-1] * pi[2] + (1 - z[t-1]) * pi[1])
for (i in 1:K) {
y_obs[i,t] ~ dbern(p_arm[i,t])
}
}
pi[1] ~ dbeta(1,1)
pi[2] ~ dbeta(1,1)
for (i in 1:K) {
for (s in 1:2) {
mu[i,s] ~ dbeta(1,1)
}
}
}
', file = "models/advanced_model.jags")
cat(
'
model {
logit_mu[1] <- logit(mu1)
p[1] <- mu1
y[1] ~ dbern(p[1])
for (t in 2:N) {
logit_mu[t] ~ dnorm(phi * logit_mu[t-1], tau)
p[t]<- ilogit(logit_mu[t])
y[t] ~ dbern(p[t])
}
#––– Priors on AR parameters –––
mu1        ~ dbeta(1, 1)
phi    ~ dnorm(0, 1 / pow(0.5,2)) T(-1,1)
sigma2 ~ dinvgamma(0.001, 0.001)
tau     <- 1 / sigma2
}
',
file = "models/ar_model.jags"
)
