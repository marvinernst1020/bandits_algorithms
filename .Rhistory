# state-dependent probabilities for each arm
arm1_probs <- c(0.8, 0.3)
arm2_probs <- c(0.4, 0.7)
reg_ucb_stat <- reg_ts_stat <- reg_sw_stat <- array(0, c(n_bandit, n_runs))
for(r in 1:n_runs){
# atent states
states <- numeric(n_bandit)
states[1] <- sample(1:2, 1)
for(t in 2:n_bandit) {
states[t] <- sample(1:2, 1, prob = trans_mat[states[t - 1], ])
}
# probs based on state
arm1 <- arm1_probs[states]
arm2 <- arm2_probs[states]
best <- ifelse(arm1 > arm2, 1, 2)
best_prob <- pmax(arm1, arm2)
#initialization
succ_ucb <- tr_ucb <- c(0, 0)
alpha_ts <- beta_ts <- c(1, 1)
history <- list(arm = integer(), reward = numeric())
cum_ucb <- cum_ts <- cum_sw <- 0
regs_ucb <- regs_ts <- regs_sw <- numeric(n_bandit)
for(t in 1:n_bandit){
probs <- c(arm1[t], arm2[t])
# UCB
ucbv <- ifelse(tr_ucb == 0, 1, succ_ucb / tr_ucb + sqrt(2 * log(t) / tr_ucb))
a_ucb <- which.max(ucbv)
r_ucb <- rbinom(1, 1, probs[a_ucb])
succ_ucb[a_ucb] <- succ_ucb[a_ucb] + r_ucb
tr_ucb[a_ucb] <- tr_ucb[a_ucb] + 1
cum_ucb <- cum_ucb + (best_prob[t] - probs[a_ucb])
regs_ucb[t] <- cum_ucb
# Thompson Sampling
draw_ts <- rbeta(2, alpha_ts, beta_ts)
a_ts <- which.max(draw_ts)
r_ts <- rbinom(1, 1, probs[a_ts])
alpha_ts[a_ts] <- alpha_ts[a_ts] + r_ts
beta_ts[a_ts] <- beta_ts[a_ts] + (1 - r_ts)
cum_ts <- cum_ts + (best_prob[t] - probs[a_ts])
regs_ts[t] <- cum_ts
# Sliding Window UCB (W = 50)
W <- 50
swv <- numeric(2)
for(a in 1:2){
idx <- tail(which(history$arm == a), W)
ntr <- length(idx); nsu <- sum(history$reward[idx])
swv[a] <- if(ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a_sw <- which.max(swv)
r_sw <- rbinom(1, 1, probs[a_sw])
history$arm <- c(history$arm, a_sw)
history$reward <- c(history$reward, r_sw)
cum_sw <- cum_sw + (best_prob[t] - probs[a_sw])
regs_sw[t] <- cum_sw
}
reg_ucb_stat[,r] <- regs_ucb
reg_ts_stat[,r]  <- regs_ts
reg_sw_stat[,r]  <- regs_sw
}
df_stat <- tibble(
t = 1:n_bandit,
UCB = rowMeans(reg_ucb_stat),
Thompson = rowMeans(reg_ts_stat),
SW_UCB = rowMeans(reg_sw_stat)
) %>% pivot_longer(-t, names_to = "Method", values_to = "CumulativeRegret")
ggplot(df_stat, aes(t, CumulativeRegret, color = Method)) +
geom_line(size = 1) +
labs(
title = "Cumulative Regret: Discrete Time-Varying Arm Rewards (Markov Switching)",
x = "Time Step", y = "Cumulative Regret"
) +
theme_minimal()
acc_ucb <- cumsum(ucb_choice==best)/(1:n)
acc_ts  <- cumsum(ts_choice==best)/(1:n)
acc_sw  <- cumsum(sw_choice==best)/(1:n)
df_acc  <- data.frame(t=1:n, UCB=acc_ucb, TS=acc_ts, SW=acc_sw)
df_acc_long <- reshape2::melt(df_acc, "t")
ggplot(df_acc_long, aes(t, value, color=variable)) +
geom_line() +
labs(y="Cumulative Accuracy", title="How Often Each Algorithm Picks the Best Arm") +
theme_minimal()
seed(1234)
set.seed(1234)
trans_mat <- matrix(c(0.6, 0.4, 0.4, 0.6), 2, 2)
# state-dependent probabilities for each arm
arm1_probs <- c(0.8, 0.3)
arm2_probs <- c(0.4, 0.7)
reg_ucb_stat <- reg_ts_stat <- reg_sw_stat <- array(0, c(n_bandit, n_runs))
for(r in 1:n_runs){
# atent states
states <- numeric(n_bandit)
states[1] <- sample(1:2, 1)
for(t in 2:n_bandit) {
states[t] <- sample(1:2, 1, prob = trans_mat[states[t - 1], ])
}
# probs based on state
arm1 <- arm1_probs[states]
arm2 <- arm2_probs[states]
best <- ifelse(arm1 > arm2, 1, 2)
best_prob <- pmax(arm1, arm2)
#initialization
succ_ucb <- tr_ucb <- c(0, 0)
alpha_ts <- beta_ts <- c(1, 1)
history <- list(arm = integer(), reward = numeric())
cum_ucb <- cum_ts <- cum_sw <- 0
regs_ucb <- regs_ts <- regs_sw <- numeric(n_bandit)
for(t in 1:n_bandit){
probs <- c(arm1[t], arm2[t])
# UCB
ucbv <- ifelse(tr_ucb == 0, 1, succ_ucb / tr_ucb + sqrt(2 * log(t) / tr_ucb))
a_ucb <- which.max(ucbv)
r_ucb <- rbinom(1, 1, probs[a_ucb])
succ_ucb[a_ucb] <- succ_ucb[a_ucb] + r_ucb
tr_ucb[a_ucb] <- tr_ucb[a_ucb] + 1
cum_ucb <- cum_ucb + (best_prob[t] - probs[a_ucb])
regs_ucb[t] <- cum_ucb
# Thompson Sampling
draw_ts <- rbeta(2, alpha_ts, beta_ts)
a_ts <- which.max(draw_ts)
r_ts <- rbinom(1, 1, probs[a_ts])
alpha_ts[a_ts] <- alpha_ts[a_ts] + r_ts
beta_ts[a_ts] <- beta_ts[a_ts] + (1 - r_ts)
cum_ts <- cum_ts + (best_prob[t] - probs[a_ts])
regs_ts[t] <- cum_ts
# Sliding Window UCB (W = 50)
W <- 50
swv <- numeric(2)
for(a in 1:2){
idx <- tail(which(history$arm == a), W)
ntr <- length(idx); nsu <- sum(history$reward[idx])
swv[a] <- if(ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a_sw <- which.max(swv)
r_sw <- rbinom(1, 1, probs[a_sw])
history$arm <- c(history$arm, a_sw)
history$reward <- c(history$reward, r_sw)
cum_sw <- cum_sw + (best_prob[t] - probs[a_sw])
regs_sw[t] <- cum_sw
}
reg_ucb_stat[,r] <- regs_ucb
reg_ts_stat[,r]  <- regs_ts
reg_sw_stat[,r]  <- regs_sw
}
df_stat <- tibble(
t = 1:n_bandit,
UCB = rowMeans(reg_ucb_stat),
Thompson = rowMeans(reg_ts_stat),
SW_UCB = rowMeans(reg_sw_stat)
) %>% pivot_longer(-t, names_to = "Method", values_to = "CumulativeRegret")
ggplot(df_stat, aes(t, CumulativeRegret, color = Method)) +
geom_line(size = 1) +
labs(
title = "Cumulative Regret: Discrete Time-Varying Arm Rewards (Markov Switching)",
x = "Time Step", y = "Cumulative Regret"
) +
theme_minimal()
acc_ucb <- cumsum(ucb_choice==best)/(1:n)
acc_ts  <- cumsum(ts_choice==best)/(1:n)
acc_sw  <- cumsum(sw_choice==best)/(1:n)
df_acc  <- data.frame(t=1:n, UCB=acc_ucb, TS=acc_ts, SW=acc_sw)
df_acc_long <- reshape2::melt(df_acc, "t")
ggplot(df_acc_long, aes(t, value, color=variable)) +
geom_line() +
labs(y="Cumulative Accuracy", title="How Often Each Algorithm Picks the Best Arm") +
theme_minimal()
set.seed(124)
trans_mat <- matrix(c(0.6, 0.4, 0.4, 0.6), 2, 2)
# state-dependent probabilities for each arm
arm1_probs <- c(0.8, 0.3)
arm2_probs <- c(0.4, 0.7)
reg_ucb_stat <- reg_ts_stat <- reg_sw_stat <- array(0, c(n_bandit, n_runs))
for(r in 1:n_runs){
# atent states
states <- numeric(n_bandit)
states[1] <- sample(1:2, 1)
for(t in 2:n_bandit) {
states[t] <- sample(1:2, 1, prob = trans_mat[states[t - 1], ])
}
# probs based on state
arm1 <- arm1_probs[states]
arm2 <- arm2_probs[states]
best <- ifelse(arm1 > arm2, 1, 2)
best_prob <- pmax(arm1, arm2)
#initialization
succ_ucb <- tr_ucb <- c(0, 0)
alpha_ts <- beta_ts <- c(1, 1)
history <- list(arm = integer(), reward = numeric())
cum_ucb <- cum_ts <- cum_sw <- 0
regs_ucb <- regs_ts <- regs_sw <- numeric(n_bandit)
for(t in 1:n_bandit){
probs <- c(arm1[t], arm2[t])
# UCB
ucbv <- ifelse(tr_ucb == 0, 1, succ_ucb / tr_ucb + sqrt(2 * log(t) / tr_ucb))
a_ucb <- which.max(ucbv)
r_ucb <- rbinom(1, 1, probs[a_ucb])
succ_ucb[a_ucb] <- succ_ucb[a_ucb] + r_ucb
tr_ucb[a_ucb] <- tr_ucb[a_ucb] + 1
cum_ucb <- cum_ucb + (best_prob[t] - probs[a_ucb])
regs_ucb[t] <- cum_ucb
# Thompson Sampling
draw_ts <- rbeta(2, alpha_ts, beta_ts)
a_ts <- which.max(draw_ts)
r_ts <- rbinom(1, 1, probs[a_ts])
alpha_ts[a_ts] <- alpha_ts[a_ts] + r_ts
beta_ts[a_ts] <- beta_ts[a_ts] + (1 - r_ts)
cum_ts <- cum_ts + (best_prob[t] - probs[a_ts])
regs_ts[t] <- cum_ts
# Sliding Window UCB (W = 50)
W <- 50
swv <- numeric(2)
for(a in 1:2){
idx <- tail(which(history$arm == a), W)
ntr <- length(idx); nsu <- sum(history$reward[idx])
swv[a] <- if(ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a_sw <- which.max(swv)
r_sw <- rbinom(1, 1, probs[a_sw])
history$arm <- c(history$arm, a_sw)
history$reward <- c(history$reward, r_sw)
cum_sw <- cum_sw + (best_prob[t] - probs[a_sw])
regs_sw[t] <- cum_sw
}
reg_ucb_stat[,r] <- regs_ucb
reg_ts_stat[,r]  <- regs_ts
reg_sw_stat[,r]  <- regs_sw
}
df_stat <- tibble(
t = 1:n_bandit,
UCB = rowMeans(reg_ucb_stat),
Thompson = rowMeans(reg_ts_stat),
SW_UCB = rowMeans(reg_sw_stat)
) %>% pivot_longer(-t, names_to = "Method", values_to = "CumulativeRegret")
ggplot(df_stat, aes(t, CumulativeRegret, color = Method)) +
geom_line(size = 1) +
labs(
title = "Cumulative Regret: Discrete Time-Varying Arm Rewards (Markov Switching)",
x = "Time Step", y = "Cumulative Regret"
) +
theme_minimal()
acc_ucb <- cumsum(ucb_choice==best)/(1:n)
acc_ts  <- cumsum(ts_choice==best)/(1:n)
acc_sw  <- cumsum(sw_choice==best)/(1:n)
df_acc  <- data.frame(t=1:n, UCB=acc_ucb, TS=acc_ts, SW=acc_sw)
df_acc_long <- reshape2::melt(df_acc, "t")
ggplot(df_acc_long, aes(t, value, color=variable)) +
geom_line() +
labs(y="Cumulative Accuracy", title="How Often Each Algorithm Picks the Best Arm") +
theme_minimal()
set.seed(224)
trans_mat <- matrix(c(0.6, 0.4, 0.4, 0.6), 2, 2)
# state-dependent probabilities for each arm
arm1_probs <- c(0.8, 0.3)
arm2_probs <- c(0.4, 0.7)
reg_ucb_stat <- reg_ts_stat <- reg_sw_stat <- array(0, c(n_bandit, n_runs))
for(r in 1:n_runs){
# atent states
states <- numeric(n_bandit)
states[1] <- sample(1:2, 1)
for(t in 2:n_bandit) {
states[t] <- sample(1:2, 1, prob = trans_mat[states[t - 1], ])
}
# probs based on state
arm1 <- arm1_probs[states]
arm2 <- arm2_probs[states]
best <- ifelse(arm1 > arm2, 1, 2)
best_prob <- pmax(arm1, arm2)
#initialization
succ_ucb <- tr_ucb <- c(0, 0)
alpha_ts <- beta_ts <- c(1, 1)
history <- list(arm = integer(), reward = numeric())
cum_ucb <- cum_ts <- cum_sw <- 0
regs_ucb <- regs_ts <- regs_sw <- numeric(n_bandit)
for(t in 1:n_bandit){
probs <- c(arm1[t], arm2[t])
# UCB
ucbv <- ifelse(tr_ucb == 0, 1, succ_ucb / tr_ucb + sqrt(2 * log(t) / tr_ucb))
a_ucb <- which.max(ucbv)
r_ucb <- rbinom(1, 1, probs[a_ucb])
succ_ucb[a_ucb] <- succ_ucb[a_ucb] + r_ucb
tr_ucb[a_ucb] <- tr_ucb[a_ucb] + 1
cum_ucb <- cum_ucb + (best_prob[t] - probs[a_ucb])
regs_ucb[t] <- cum_ucb
# Thompson Sampling
draw_ts <- rbeta(2, alpha_ts, beta_ts)
a_ts <- which.max(draw_ts)
r_ts <- rbinom(1, 1, probs[a_ts])
alpha_ts[a_ts] <- alpha_ts[a_ts] + r_ts
beta_ts[a_ts] <- beta_ts[a_ts] + (1 - r_ts)
cum_ts <- cum_ts + (best_prob[t] - probs[a_ts])
regs_ts[t] <- cum_ts
# Sliding Window UCB (W = 50)
W <- 50
swv <- numeric(2)
for(a in 1:2){
idx <- tail(which(history$arm == a), W)
ntr <- length(idx); nsu <- sum(history$reward[idx])
swv[a] <- if(ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a_sw <- which.max(swv)
r_sw <- rbinom(1, 1, probs[a_sw])
history$arm <- c(history$arm, a_sw)
history$reward <- c(history$reward, r_sw)
cum_sw <- cum_sw + (best_prob[t] - probs[a_sw])
regs_sw[t] <- cum_sw
}
reg_ucb_stat[,r] <- regs_ucb
reg_ts_stat[,r]  <- regs_ts
reg_sw_stat[,r]  <- regs_sw
}
df_stat <- tibble(
t = 1:n_bandit,
UCB = rowMeans(reg_ucb_stat),
Thompson = rowMeans(reg_ts_stat),
SW_UCB = rowMeans(reg_sw_stat)
) %>% pivot_longer(-t, names_to = "Method", values_to = "CumulativeRegret")
ggplot(df_stat, aes(t, CumulativeRegret, color = Method)) +
geom_line(size = 1) +
labs(
title = "Cumulative Regret: Discrete Time-Varying Arm Rewards (Markov Switching)",
x = "Time Step", y = "Cumulative Regret"
) +
theme_minimal()
acc_ucb <- cumsum(ucb_choice==best)/(1:n)
acc_ts  <- cumsum(ts_choice==best)/(1:n)
acc_sw  <- cumsum(sw_choice==best)/(1:n)
df_acc  <- data.frame(t=1:n, UCB=acc_ucb, TS=acc_ts, SW=acc_sw)
df_acc_long <- reshape2::melt(df_acc, "t")
ggplot(df_acc_long, aes(t, value, color=variable)) +
geom_line() +
labs(y="Cumulative Accuracy", title="How Often Each Algorithm Picks the Best Arm") +
theme_minimal()
# discrete markov-switching (2 states) – 5-arm bandit
set.seed(2050)
n_runs   <- 1000   # repetitions for MC
n_bandit <- 500
# hidden-state transition matrix
P_mat <- matrix(c(0.6, 0.4,
0.4, 0.6), nrow = 2, byrow = TRUE)
bandit_probs <- matrix(
c(0.80, 0.55, 0.35, 0.25, 0.60,
0.30, 0.45, 0.75, 0.65, 0.40),
nrow = 2, byrow = TRUE)
run_sim <- function(method = c("ucb", "ts", "sw"), window = 20) {
method <- match.arg(method)
cum_reg <- matrix(0, nrow = n_bandit, ncol = n_runs)
for (r in seq_len(n_runs)) {
#hidden states
state <- numeric(n_bandit)
state[1] <- sample(1:2, 1)
for (t in 2:n_bandit)
state[t] <- sample(1:2, 1, prob = P_mat[state[t-1], ])
best_prob <- sapply(state, function(s) max(bandit_probs[s, ]))
if (method == "ucb") {
succ <- trial <- numeric(5)
} else if (method == "ts") {
alpha <- beta <- rep(1, 5)
} else {                       # sliding window
hist <- list(arm = integer(), rwd = numeric())
}
for (t in seq_len(n_bandit)) {
# choose arm
if (method == "ucb") {
vals <- ifelse(trial == 0, 1,
succ / trial + sqrt(2 * log(t) / trial))
a <- which.max(vals)
} else if (method == "ts") {
a <- which.max(rbeta(5, alpha, beta))
} else {  # SW-UCB
vals <- numeric(5)
for (arm in 1:5) {
idx <- tail(which(hist$arm == arm), window)
ntr <- length(idx); nsu <- sum(hist$rwd[idx])
vals[arm] <- if (ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a <- which.max(vals)
}
# pull arm
rwd <- rbinom(1, 1, bandit_probs[state[t], a])
# update stats
if (method == "ucb") {
succ[a]  <- succ[a]  + rwd
trial[a] <- trial[a] + 1
} else if (method == "ts") {
alpha[a] <- alpha[a] + rwd
beta[a]  <- beta[a]  + (1 - rwd)
} else {
hist$arm <- c(hist$arm, a)
hist$rwd <- c(hist$rwd, rwd)
}
cum_reg[t, r] <- (if (t == 1) 0 else cum_reg[t-1, r]) +
(best_prob[t] - rwd)
}
}
cum_reg
}
## Run the three algs
reg_ucb <- run_sim("ucb")
reg_ts  <- run_sim("ts")
reg_sw  <- run_sim("sw", window = 20)
t <- 1:n_bandit
df5 <- tibble(
t      = rep(t, 3),
regret = c(rowMeans(reg_ucb),
rowMeans(reg_ts),
rowMeans(reg_sw)),
sd     = c(apply(reg_ucb, 1, sd),
apply(reg_ts, 1, sd),
apply(reg_sw, 1, sd)),
Method = rep(c("UCB", "Thompson", "SW-UCB (20)"), each = n_bandit)
) %>%
mutate(lower = regret - 1.96*sd/sqrt(n_runs),
upper = regret + 1.96*sd/sqrt(n_runs))
ggplot(df5, aes(t, regret, colour = Method, fill = Method)) +
geom_line(size = 1) +
geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.20, colour = NA) +
labs(title = "Cumulative Regret: Discrete Markov Switching with 5 Arms",
x = "Time Step", y = "Average Cumulative Regret") +
theme_minimal()
acc_ucb <- acc_ts <- acc_sw <- matrix(0, nrow = n_bandit, ncol = n_runs)
for (r in seq_len(n_runs)) {
## latent states
s <- numeric(n_bandit)
s[1] <- sample(1:2, 1)
for (t in 2:n_bandit)
s[t] <- sample(1:2, 1, prob = P_mat[s[t-1], ])
best_arm <- apply(bandit_probs[s, ], 1, which.max)
succ  <- trial <- numeric(5)
alpha <- beta  <- rep(1, 5)
hist  <- list(arm = integer(), rwd = numeric())
for (t in seq_len(n_bandit)) {
## UCB
vals <- ifelse(trial == 0, 1,
succ / trial + sqrt(2 * log(t) / trial))
a_ucb <- which.max(vals)
acc_ucb[t, r] <- (a_ucb == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_ucb])
succ[a_ucb]  <- succ[a_ucb]  + rwd
trial[a_ucb] <- trial[a_ucb] + 1
##  Thompson
a_ts <- which.max(rbeta(5, alpha, beta))
acc_ts[t, r] <- (a_ts == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_ts])
alpha[a_ts] <- alpha[a_ts] + rwd
beta[a_ts]  <- beta[a_ts]  + (1 - rwd)
## Sliding-Window UCB (W = 20)
W <- 20
vals <- numeric(5)
for (arm in 1:5) {
idx <- tail(which(hist$arm == arm), W)
ntr <- length(idx); nsu <- sum(hist$rwd[idx])
vals[arm] <- if (ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a_sw <- which.max(vals)
acc_sw[t, r] <- (a_sw == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_sw])
hist$arm <- c(hist$arm, a_sw)
hist$rwd <- c(hist$rwd, rwd)
}
}
for (r in seq_len(n_runs)) {
## latent states
s <- numeric(n_bandit)
s[1] <- sample(1:2, 1)
for (t in 2:n_bandit)
s[t] <- sample(1:2, 1, prob = P_mat[s[t-1], ])
best_arm <- apply(bandit_probs[s, ], 1, which.max)
succ  <- trial <- numeric(5)
alpha <- beta  <- rep(1, 5)
hist  <- list(arm = integer(), rwd = numeric())
for (t in seq_len(n_bandit)) {
## UCB
vals <- ifelse(trial == 0, 1,
succ / trial + sqrt(2 * log(t) / trial))
a_ucb <- which.max(vals)
acc_ucb[t, r] <- (a_ucb == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_ucb])
succ[a_ucb]  <- succ[a_ucb]  + rwd
trial[a_ucb] <- trial[a_ucb] + 1
##  Thompson
a_ts <- which.max(rbeta(5, alpha, beta))
acc_ts[t, r] <- (a_ts == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_ts])
alpha[a_ts] <- alpha[a_ts] + rwd
beta[a_ts]  <- beta[a_ts]  + (1 - rwd)
## Sliding-Window UCB (W = 20)
W <- 20
vals <- numeric(5)
for (arm in 1:5) {
idx <- tail(which(hist$arm == arm), W)
ntr <- length(idx); nsu <- sum(hist$rwd[idx])
vals[arm] <- if (ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a_sw <- which.max(vals)
acc_sw[t, r] <- (a_sw == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_sw])
hist$arm <- c(hist$arm, a_sw)
hist$rwd <- c(hist$rwd, rwd)
}
}
# average over runs
acc_df <- tibble(
t        = rep(1:n_bandit, 3),
accuracy = c(rowMeans(acc_ucb),
rowMeans(acc_ts),
rowMeans(acc_sw)),
Method   = rep(c("UCB", "Thompson", "SW-UCB (20)"), each = n_bandit)
)
ggplot(acc_df, aes(t, accuracy, colour = Method)) +
geom_line(size = 1) +
labs(title = "Per-Step Optimal-Arm Accuracy (5 Arms, Discrete Switching)",
x = "Time Step", y = "Accuracy") +
theme_minimal()
