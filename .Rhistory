best_prob <- pmax(arm1, arm2)
#initialization
succ_ucb <- tr_ucb <- c(0, 0)
alpha_ts <- beta_ts <- c(1, 1)
history <- list(arm = integer(), reward = numeric())
cum_ucb <- cum_ts <- cum_sw <- 0
regs_ucb <- regs_ts <- regs_sw <- numeric(n_bandit)
for(t in 1:n_bandit){
probs <- c(arm1[t], arm2[t])
# UCB
ucbv <- ifelse(tr_ucb == 0, 1, succ_ucb / tr_ucb + sqrt(2 * log(t) / tr_ucb))
a_ucb <- which.max(ucbv)
r_ucb <- rbinom(1, 1, probs[a_ucb])
succ_ucb[a_ucb] <- succ_ucb[a_ucb] + r_ucb
tr_ucb[a_ucb] <- tr_ucb[a_ucb] + 1
cum_ucb <- cum_ucb + (best_prob[t] - probs[a_ucb])
regs_ucb[t] <- cum_ucb
# Thompson Sampling
draw_ts <- rbeta(2, alpha_ts, beta_ts)
a_ts <- which.max(draw_ts)
r_ts <- rbinom(1, 1, probs[a_ts])
alpha_ts[a_ts] <- alpha_ts[a_ts] + r_ts
beta_ts[a_ts] <- beta_ts[a_ts] + (1 - r_ts)
cum_ts <- cum_ts + (best_prob[t] - probs[a_ts])
regs_ts[t] <- cum_ts
# Sliding Window UCB (W = 50)
W <- 50
swv <- numeric(2)
for(a in 1:2){
idx <- tail(which(history$arm == a), W)
ntr <- length(idx); nsu <- sum(history$reward[idx])
swv[a] <- if(ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a_sw <- which.max(swv)
r_sw <- rbinom(1, 1, probs[a_sw])
history$arm <- c(history$arm, a_sw)
history$reward <- c(history$reward, r_sw)
cum_sw <- cum_sw + (best_prob[t] - probs[a_sw])
regs_sw[t] <- cum_sw
}
reg_ucb_stat[,r] <- regs_ucb
reg_ts_stat[,r]  <- regs_ts
reg_sw_stat[,r]  <- regs_sw
}
df_stat <- tibble(
t = 1:n_bandit,
UCB = rowMeans(reg_ucb_stat),
Thompson = rowMeans(reg_ts_stat),
SW_UCB = rowMeans(reg_sw_stat)
) %>% pivot_longer(-t, names_to = "Method", values_to = "CumulativeRegret")
ggplot(df_stat, aes(t, CumulativeRegret, color = Method)) +
geom_line(size = 1) +
labs(
title = "Cumulative Regret: Discrete Time-Varying Arm Rewards (Markov Switching)",
x = "Time Step", y = "Cumulative Regret"
) +
theme_minimal()
acc_ucb <- cumsum(ucb_choice==best)/(1:n)
acc_ts  <- cumsum(ts_choice==best)/(1:n)
acc_sw  <- cumsum(sw_choice==best)/(1:n)
df_acc  <- data.frame(t=1:n, UCB=acc_ucb, TS=acc_ts, SW=acc_sw)
df_acc_long <- reshape2::melt(df_acc, "t")
ggplot(df_acc_long, aes(t, value, color=variable)) +
geom_line() +
labs(y="Cumulative Accuracy", title="How Often Each Algorithm Picks the Best Arm") +
theme_minimal()
# discrete markov-switching (2 states) â€“ 5-arm bandit
set.seed(2050)
n_runs   <- 1000   # repetitions for MC
n_bandit <- 500
# hidden-state transition matrix
P_mat <- matrix(c(0.6, 0.4,
0.4, 0.6), nrow = 2, byrow = TRUE)
bandit_probs <- matrix(
c(0.80, 0.55, 0.35, 0.25, 0.60,
0.30, 0.45, 0.75, 0.65, 0.40),
nrow = 2, byrow = TRUE)
run_sim <- function(method = c("ucb", "ts", "sw"), window = 20) {
method <- match.arg(method)
cum_reg <- matrix(0, nrow = n_bandit, ncol = n_runs)
for (r in seq_len(n_runs)) {
#hidden states
state <- numeric(n_bandit)
state[1] <- sample(1:2, 1)
for (t in 2:n_bandit)
state[t] <- sample(1:2, 1, prob = P_mat[state[t-1], ])
best_prob <- sapply(state, function(s) max(bandit_probs[s, ]))
if (method == "ucb") {
succ <- trial <- numeric(5)
} else if (method == "ts") {
alpha <- beta <- rep(1, 5)
} else {                       # sliding window
hist <- list(arm = integer(), rwd = numeric())
}
for (t in seq_len(n_bandit)) {
# choose arm
if (method == "ucb") {
vals <- ifelse(trial == 0, 1,
succ / trial + sqrt(2 * log(t) / trial))
a <- which.max(vals)
} else if (method == "ts") {
a <- which.max(rbeta(5, alpha, beta))
} else {  # SW-UCB
vals <- numeric(5)
for (arm in 1:5) {
idx <- tail(which(hist$arm == arm), window)
ntr <- length(idx); nsu <- sum(hist$rwd[idx])
vals[arm] <- if (ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a <- which.max(vals)
}
# pull arm
rwd <- rbinom(1, 1, bandit_probs[state[t], a])
# update stats
if (method == "ucb") {
succ[a]  <- succ[a]  + rwd
trial[a] <- trial[a] + 1
} else if (method == "ts") {
alpha[a] <- alpha[a] + rwd
beta[a]  <- beta[a]  + (1 - rwd)
} else {
hist$arm <- c(hist$arm, a)
hist$rwd <- c(hist$rwd, rwd)
}
cum_reg[t, r] <- (if (t == 1) 0 else cum_reg[t-1, r]) +
(best_prob[t] - rwd)
}
}
cum_reg
}
## Run the three algs
reg_ucb <- run_sim("ucb")
reg_ts  <- run_sim("ts")
reg_sw  <- run_sim("sw", window = 20)
t <- 1:n_bandit
df5 <- tibble(
t      = rep(t, 3),
regret = c(rowMeans(reg_ucb),
rowMeans(reg_ts),
rowMeans(reg_sw)),
sd     = c(apply(reg_ucb, 1, sd),
apply(reg_ts, 1, sd),
apply(reg_sw, 1, sd)),
Method = rep(c("UCB", "Thompson", "SW-UCB (20)"), each = n_bandit)
) %>%
mutate(lower = regret - 1.96*sd/sqrt(n_runs),
upper = regret + 1.96*sd/sqrt(n_runs))
ggplot(df5, aes(t, regret, colour = Method, fill = Method)) +
geom_line(size = 1) +
geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.20, colour = NA) +
labs(title = "Cumulative Regret: Discrete Markov Switching with 5 Arms",
x = "Time Step", y = "Average Cumulative Regret") +
theme_minimal()
acc_ucb <- acc_ts <- acc_sw <- matrix(0, nrow = n_bandit, ncol = n_runs)
for (r in seq_len(n_runs)) {
## latent states
s <- numeric(n_bandit)
s[1] <- sample(1:2, 1)
for (t in 2:n_bandit)
s[t] <- sample(1:2, 1, prob = P_mat[s[t-1], ])
best_arm <- apply(bandit_probs[s, ], 1, which.max)
succ  <- trial <- numeric(5)
alpha <- beta  <- rep(1, 5)
hist  <- list(arm = integer(), rwd = numeric())
for (t in seq_len(n_bandit)) {
## UCB
vals <- ifelse(trial == 0, 1,
succ / trial + sqrt(2 * log(t) / trial))
a_ucb <- which.max(vals)
acc_ucb[t, r] <- (a_ucb == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_ucb])
succ[a_ucb]  <- succ[a_ucb]  + rwd
trial[a_ucb] <- trial[a_ucb] + 1
##  Thompson
a_ts <- which.max(rbeta(5, alpha, beta))
acc_ts[t, r] <- (a_ts == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_ts])
alpha[a_ts] <- alpha[a_ts] + rwd
beta[a_ts]  <- beta[a_ts]  + (1 - rwd)
## Sliding-Window UCB (W = 20)
W <- 20
vals <- numeric(5)
for (arm in 1:5) {
idx <- tail(which(hist$arm == arm), W)
ntr <- length(idx); nsu <- sum(hist$rwd[idx])
vals[arm] <- if (ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a_sw <- which.max(vals)
acc_sw[t, r] <- (a_sw == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_sw])
hist$arm <- c(hist$arm, a_sw)
hist$rwd <- c(hist$rwd, rwd)
}
}
for (r in seq_len(n_runs)) {
## latent states
s <- numeric(n_bandit)
s[1] <- sample(1:2, 1)
for (t in 2:n_bandit)
s[t] <- sample(1:2, 1, prob = P_mat[s[t-1], ])
best_arm <- apply(bandit_probs[s, ], 1, which.max)
succ  <- trial <- numeric(5)
alpha <- beta  <- rep(1, 5)
hist  <- list(arm = integer(), rwd = numeric())
for (t in seq_len(n_bandit)) {
## UCB
vals <- ifelse(trial == 0, 1,
succ / trial + sqrt(2 * log(t) / trial))
a_ucb <- which.max(vals)
acc_ucb[t, r] <- (a_ucb == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_ucb])
succ[a_ucb]  <- succ[a_ucb]  + rwd
trial[a_ucb] <- trial[a_ucb] + 1
##  Thompson
a_ts <- which.max(rbeta(5, alpha, beta))
acc_ts[t, r] <- (a_ts == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_ts])
alpha[a_ts] <- alpha[a_ts] + rwd
beta[a_ts]  <- beta[a_ts]  + (1 - rwd)
## Sliding-Window UCB (W = 20)
W <- 20
vals <- numeric(5)
for (arm in 1:5) {
idx <- tail(which(hist$arm == arm), W)
ntr <- length(idx); nsu <- sum(hist$rwd[idx])
vals[arm] <- if (ntr == 0) 1 else nsu / ntr + sqrt(2 * log(t) / ntr)
}
a_sw <- which.max(vals)
acc_sw[t, r] <- (a_sw == best_arm[t])
rwd <- rbinom(1, 1, bandit_probs[s[t], a_sw])
hist$arm <- c(hist$arm, a_sw)
hist$rwd <- c(hist$rwd, rwd)
}
}
# average over runs
acc_df <- tibble(
t        = rep(1:n_bandit, 3),
accuracy = c(rowMeans(acc_ucb),
rowMeans(acc_ts),
rowMeans(acc_sw)),
Method   = rep(c("UCB", "Thompson", "SW-UCB (20)"), each = n_bandit)
)
ggplot(acc_df, aes(t, accuracy, colour = Method)) +
geom_line(size = 1) +
labs(title = "Per-Step Optimal-Arm Accuracy (5 Arms, Discrete Switching)",
x = "Time Step", y = "Accuracy") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
set.seed(456)
# here we are setting the parameters:
K <- 3 #for 3 arms
N <- 500 # for 500 time steps
# this is our matrix of true thetas and filled by row for the 3 arms.
theta <- matrix(c(0.2, 0.8,
0.5, 0.7,
0.3, 0.9),
nrow = K, byrow = TRUE)
# these are our matrices of transition probabilities for each arm.
pi_indiv <- list(
matrix(c(0.9, 0.1,
0.1, 0.9), 2, 2, byrow = TRUE),
matrix(c(0.85, 0.15,
0.2, 0.8), 2, 2, byrow = TRUE),
matrix(c(0.95, 0.05,
0.3, 0.7), 2, 2, byrow = TRUE)
)
# simulate latent states z_true and observed rewards y_true
# we initialize our matrices of K by N dimensions of as matrices of 0s.
# zs are our states while ys are our observed rewards.
z_true <- matrix(0, K, N)
y_true <- matrix(0, K, N)
#for each arm we sample z from a Bernoulli and also y forma Bernoulli dependent on theta
for(i in 1:K){
z_true[i,1] <- rbinom(1,1,0.5) # the structure of this is nr of draws,bin,success probability.
y_true[i,1] <- rbinom(1,1, theta[i, z_true[i,1]+1]) # i do it this way so that when its z is 0 it picks column 1 but for 1 picks col 2
for(t in 2:N){ #then per iteration, I'm sampling
z_true[i,t] <- rbinom(1,1, pi_indiv[[i]][ z_true[i,t-1]+1 , 2 ]) # the probability of transitioning into state 1 (column 2) from whatever your last state was. its coompl
y_true[i,t] <- rbinom(1,1, theta[i, z_true[i,t]+1 ])
}
}
# FIX the var?
theta_selected <- matrix(0, nrow = K, ncol = N)
for(i in 1:K){
for(t in 1:N){
theta_selected[i, t] <- theta[i, z_true[i, t] + 1]
}
}
# for each arm select the index of the arm where theta selected is the largest.
oracle_arm <- apply(theta_selected, 2, which.max)
# Initializing algorithm structure
counts       <- rep(0, K)   # storing number of times each arm has been pulled
sum_rewards  <- rep(0, K)   # cumulative reward per arm
selected_arms_ucb    <- integer(N) # which arm is picked at time t
received_rewards_ucb <- numeric(N) # reward at time t
regret_ucb           <- numeric(N) # regret at time t
# Initialization: play each arm once
for(t in 1:K){
arm <- t
r   <- y_true[arm, t]
counts[arm]      <- 1
sum_rewards[arm] <- r
selected_arms_ucb[t]    <- arm
received_rewards_ucb[t] <- r
regret_ucb[t] <- theta_selected[oracle_arm[t], t] -
theta_selected[arm, t] # best arm - selected arm
}
# to be repeaated 500 tims
for(t in (K+1):N){
ucb_values <- numeric(K)
for(i in 1:K){
mu_hat <- sum_rewards[i] / counts[i] #nvr /0 bcs we initialized as 1
bonus  <- sqrt(2 * log(t) / counts[i])
ucb_values[i] <- mu_hat + bonus
}
arm <- which.max(ucb_values)
r   <- y_true[arm, t]
counts[arm]      <- counts[arm] + 1 #adding count after the arm chosen again
sum_rewards[arm] <- sum_rewards[arm] + r #ading reward after arm chosen again
selected_arms_ucb[t]    <- arm
received_rewards_ucb[t] <- r
regret_ucb[t] <- theta_selected[oracle_arm[t], t] -
theta_selected[arm, t]
}
#Evaluate
cumu_reward_ucb <- cumsum(received_rewards_ucb)
cumu_regret_ucb <- cumsum(regret_ucb)
par(mfrow=c(1,2))
plot(cumu_reward_ucb, type="l", lwd=2,
xlab="Time", ylab="Cumulative Reward",
main="UCB: Cumulative Reward")
plot(cumu_regret_ucb, type="l", lwd=2, col="red",
xlab="Time", ylab="Cumulative Regret",
main="UCB: Regret vs Oracle")
set.seed(2025)
N_long <- 5000
theta_true <- c(0.3, 0.9)       # theta0, theta1
pi_true    <- c(0.10, 0.95)     # pi[1]=P(0 to1), pi[2]=P(1 to 1)
#init
z_long <- numeric(N_long)
y_long <- numeric(N_long)
# initial draw:
rho0 <- pi_true[2] / sum(pi_true)   # P(z=0)
z_long[1] <- rbinom(1,1, 1 - rho0)   # P(z=1)=rho1
y_long[1] <- rbinom(1,1, theta_true[z_long[1]+1])
# simulate
for(t in 2:N_long) {
# Choose the correct transition probability into state 1, depending on whether z_{t-1} was 0 or 1
p_z1 <- ifelse(z_long[t-1]==0,
pi_true[1],        # 0->1
pi_true[2])        # 1->1
z_long[t] <- rbinom(1,1, p_z1)
y_long[t] <- rbinom(1,1, theta_true[z_long[t]+1])
}
# FIT IN JAGS USING poor_model.jags
data_long <- list(
y = y_long,
N = N_long
)
model_long <- jags.model(
file = "poor_model.jags",
data = data_long,
n.chains = 2,
quiet = TRUE
)
update(model_long, 1000)
post_long <- coda.samples(
model = model_long,
variable.names = c("theta0","theta1","pi[1]","pi[2]"),
n.iter = 1000
)
print( summary(post_long) )
install.packages("glue")
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(tidyverse)
library(here)
library(glue)
library(future.apply)
plan(multisession)
source(here("src/load_all.R"))
set.seed(123)
K <- 2
N <- 5000
pi_global <- matrix(c(0.9, 0.1,
0.1, 0.9),
nrow = 2, byrow = TRUE)
mu <- matrix(c(0.1, 0.95,
0.95, 0.1),
nrow = K, byrow = TRUE)
generate_global_datasets(
K = K,
N = N,
mu = mu,
pi_global = pi_global,
n_runs = 1,
scenario_name = "single_run",
root_path = "data_global"
)
truth <- readRDS("data_global/single_run/global_truth_1.rds")
y_global <- truth$y
z_global <- truth$z
qplot(1:N, z_global, geom = "line") +
labs(title = "Latent Global State Over Time",
x = "Time", y = "State")
y_df <- as.data.frame(y_global)
y_df <- y_df |> mutate(arm = factor(1:K)) |>
pivot_longer(cols = starts_with("V"), names_to = "time", values_to = "reward") |>
mutate(time = as.numeric(gsub("V", "", time)))
ggplot(y_df, aes(x = time, y = reward, color = arm)) +
geom_line() +
facet_wrap(~ arm, ncol = 1) +
labs(title = "Reward Streams Per Arm",
x = "Time", y = "Reward")
baseline_ts_results <- bandit_baselines("ts",
K, N, y_global, z_global, mu,
dynamics = "common",
batch_size = 100)
baseline_ts_results_df <- data.frame(
time = seq_along(baseline_ts_results$cumulative_reward),
cumulative_reward = baseline_ts_results$cumulative_reward,
cumulative_regret = baseline_ts_results$cumulative_regret,
model = "Baseline TS"
)
system.time({
res_adv_ts_common <- thompson_advanced(K, N, mu, y_global, z_global,
batch_size = 100,
burn = 500, n_iter = 100,
dynamics = "common")
})
install.packages("glue")
adv_ts_results_df <- data.frame(
time = seq_along(res_adv_ts_common$cumulative_reward),
cumulative_reward = res_adv_ts_common$cumulative_reward,
cumulative_regret = res_adv_ts_common$cumulative_regret,
model = "Advanced TS"
)
ts_compare_df_s1 <- bind_rows(baseline_ts_results_df, adv_ts_results_df)
ggplot(ts_compare_df_s1, aes(x = time, y = cumulative_reward, color = model)) +
geom_line(size = 1) +
labs(title = "Cumulative Reward: Baseline vs Advanced Thompson Sampling",
x = "Time",
y = "Cumulative Reward") +
theme_minimal()
ggplot(ts_compare_df_s1, aes(x = time, y = cumulative_regret, color = model)) +
geom_line(size = 1) +
labs(title = "Cumulative Regret: Baseline vs Advanced Thompson Sampling",
x = "Time",
y = "Cumulative Regret") +
theme_minimal()
baseline_ucb_results <- bandit_baselines(
algorithm='ucb-tuned',
K, N, y_global, z_global, mu,
dynamics = "common",
batch_size = 100
)
baseline_ucb_results_df <- data.frame(
time = seq_along(baseline_ucb_results$cumulative_reward),
cumulative_reward = baseline_ucb_results$cumulative_reward,
cumulative_regret = baseline_ucb_results$cumulative_regret,
model = "Baseline UCB"
)
system.time({
res_adv_ucb_common <- ucb_advanced(
K, N, mu, y_global, z_global,
batch_size = 100,
dynamics = "common"
)
})
adv_ucb_results_df <- data.frame(
time = seq_along(res_adv_ucb_common$cumulative_reward),
cumulative_reward = res_adv_ucb_common$cumulative_reward,
cumulative_regret = res_adv_ucb_common$cumulative_regret,
model = "Advanced UCB"
)
ucb_compare_df_s1 <- bind_rows(baseline_ucb_results_df, adv_ucb_results_df)
ggplot(ucb_compare_df_s1, aes(x = time, y = cumulative_reward, color = model)) +
geom_line(size = 1) +
labs(title = "Cumulative Reward: Baseline vs Advanced UCB",
x = "Time",
y = "Cumulative Reward") +
theme_minimal()
ggplot(ucb_compare_df_s1, aes(x = time, y = cumulative_regret, color = model)) +
geom_line(size = 1) +
labs(title = "Cumulative Regret: Baseline vs Advanced UCB",
x = "Time",
y = "Cumulative Regret") +
theme_minimal()
generate_global_datasets(K = 2, N = 5000, mu = mu, pi_global = pi_global, scenario_name = "A", n_runs = 25)
ts_runs_df <- future_lapply(1:25, function(i) {
bind_rows(
simulate_model_on_run(
run_id = i, N = 5000, K = 2,
algorithm = "ts",
complexity = "baseline",
dynamics = "common",
data_path = "data_global/A",
setting = "global"
),
simulate_model_on_run(
run_id = i, N = 5000, K = 2,
algorithm = "ts",
complexity = "advanced",
dynamics = "common",
data_path = "data_global/A",
setting = "global"
)
)
}) |> bind_rows()
